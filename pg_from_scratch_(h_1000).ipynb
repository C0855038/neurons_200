{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l9hHvTk6ec8"
      },
      "source": [
        "# Policy Gradient\n",
        "\n",
        "* http://karpathy.github.io/2016/05/31/rl/\n",
        "* https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
        "* https://github.com/gameofdimension/policy-gradient-pong\n",
        "* https://www.youtube.com/watch?v=tqrcjHuNdmQ\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqkOdLyN9Ylm"
      },
      "source": [
        "## Step 1: Installation for Colab - just execute these cells and do not worry too much\n",
        "\n",
        "* http://nbviewer.jupyter.org/github/patrickmineault/xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb \n",
        "* https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi\n",
        "* https://nyu-cds.github.io/python-mpi/setup/\n",
        "* https://medium.com/@kaleajit27/reinforcement-learning-on-google-colab-9cb2e1ef51e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF9MAVI16huj",
        "outputId": "fca8a42a-5692-4022-85bc-e161b0dbc380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install python-opengl -y  >/dev/null\n",
        "!apt install xvfb -y >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fSC11TfN6p69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "770a66b6-50f7-4edb-99a1-241d4b310dd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 69, in main\n",
            "    command = create_command(cmd_name, isolated=(\"--isolated\" in cmd_args))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/__init__.py\", line 91, in create_command\n",
            "    module = importlib.import_module(module_path)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 15, in <module>\n",
            "    from pip._internal.cli.req_command import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 963, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 909, in _find_spec\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!pip install pyvirtualdisplay >/dev/null\n",
        "!pip install piglet >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "caiHE2hy6xrf"
      },
      "outputs": [],
      "source": [
        "# from pyvirtualdisplay import Display\n",
        "# display = Display(visible=0, size=(1400, 900))\n",
        "# display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "19a559ad-b457-4d7f-cf99-177a2f433c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.4 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=e2c0c3926727ce3bca1bfb546ee6b048e72abd8cb5494527ccae5d44ec537e6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "ab5befdb-5c8d-4bba-8e5b-eba4fe7fc741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "9b2ef9a4-6d72-4c6c-affa-ff925398602f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "5b54a943-d67e-42fa-9c3d-9579de22b36d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "4dd027a6-0c5f-4b26-ae05-878b608df086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -16.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  env.close()\n",
        "  display_frames_as_gif(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 1000 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 3 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "2f27755d-8b03-4376-e78b-b4db9ca234bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.019900\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.019701\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.019504\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.019309\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.019116\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.028925\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.028635\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.038349\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.047966\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.057486\n",
            "resetting env. episode 13.000000, reward total was -18.000000. running mean: -20.036911\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.046542\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.046077\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.055616\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.065060\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.074409\n",
            "resetting env. episode 19.000000, reward total was -19.000000. running mean: -20.063665\n",
            "resetting env. episode 20.000000, reward total was -18.000000. running mean: -20.043028\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.042598\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.042172\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.051750\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.051233\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.050720\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.050213\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.059711\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.069114\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.078423\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.087639\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.086762\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.095895\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.104936\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.103886\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.102847\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.111819\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.110701\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.109594\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.118498\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.117313\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.116140\n",
            "resetting env. episode 42.000000, reward total was -18.000000. running mean: -20.094978\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.104029\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.102988\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.111958\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.120839\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.129630\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.128334\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.137051\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.145680\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.144223\n",
            "resetting env. episode 52.000000, reward total was -18.000000. running mean: -20.122781\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.121553\n",
            "resetting env. episode 54.000000, reward total was -19.000000. running mean: -20.110338\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.119235\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.118042\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.126862\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.125593\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.134337\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.142994\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.151564\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.160048\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.158448\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.166863\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.175195\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.183443\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.191608\n",
            "resetting env. episode 68.000000, reward total was -19.000000. running mean: -20.179692\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.187895\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.186016\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.194156\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.202215\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.210192\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.208091\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.216010\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.223850\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.221611\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.229395\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.237101\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.234730\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.232383\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.240059\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.247658\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.255182\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.252630\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.250104\n",
            "resetting env. episode 87.000000, reward total was -17.000000. running mean: -20.217603\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.225427\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.233172\n",
            "resetting env. episode 90.000000, reward total was -19.000000. running mean: -20.220841\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.218632\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.226446\n",
            "resetting env. episode 93.000000, reward total was -19.000000. running mean: -20.214181\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.222040\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -20.209819\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.217721\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.205544\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.203488\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.201453\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.209439\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.217344\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.225171\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.212919\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.220790\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.228582\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.226296\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.224033\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.231793\n",
            "resetting env. episode 109.000000, reward total was -19.000000. running mean: -20.219475\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.217280\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -20.205108\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.213057\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.220926\n",
            "resetting env. episode 114.000000, reward total was -19.000000. running mean: -20.208717\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.206630\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.204563\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.212518\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.220392\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.228189\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.235907\n",
            "resetting env. episode 121.000000, reward total was -18.000000. running mean: -20.213548\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.221412\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.229198\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.236906\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.244537\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.252092\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.259571\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.266975\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.274305\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -20.261562\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.258947\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.256357\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -20.243793\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.251356\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.248842\n",
            "resetting env. episode 136.000000, reward total was -19.000000. running mean: -20.236354\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.233990\n",
            "resetting env. episode 138.000000, reward total was -19.000000. running mean: -20.221650\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.219434\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.227239\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.234967\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.232617\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.230291\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.227988\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.225708\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.223451\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.221217\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.229005\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.236714\n",
            "resetting env. episode 150.000000, reward total was -18.000000. running mean: -20.214347\n",
            "resetting env. episode 151.000000, reward total was -19.000000. running mean: -20.202204\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.200182\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.208180\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.206098\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.214037\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.221897\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.229678\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.237381\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.245007\n",
            "resetting env. episode 160.000000, reward total was -19.000000. running mean: -20.232557\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.240232\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.247829\n",
            "resetting env. episode 163.000000, reward total was -19.000000. running mean: -20.235351\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.242998\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.250568\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.248062\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.255581\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.263025\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.270395\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.277691\n",
            "resetting env. episode 171.000000, reward total was -19.000000. running mean: -20.264914\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.272265\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.279543\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.276747\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.283980\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.291140\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -20.278228\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.285446\n",
            "resetting env. episode 179.000000, reward total was -19.000000. running mean: -20.272592\n",
            "resetting env. episode 180.000000, reward total was -18.000000. running mean: -20.249866\n",
            "resetting env. episode 181.000000, reward total was -19.000000. running mean: -20.237367\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.244993\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.242544\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.250118\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.247617\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.255141\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.262589\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.259963\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.267364\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.264690\n",
            "resetting env. episode 191.000000, reward total was -19.000000. running mean: -20.252043\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.259523\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.256928\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.264358\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.261715\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.259098\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.256507\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.263942\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.261302\n",
            "resetting env. episode 200.000000, reward total was -19.000000. running mean: -20.248689\n",
            "resetting env. episode 201.000000, reward total was -19.000000. running mean: -20.236202\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.243840\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.241402\n",
            "resetting env. episode 204.000000, reward total was -19.000000. running mean: -20.228988\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.236698\n",
            "resetting env. episode 206.000000, reward total was -19.000000. running mean: -20.224331\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.222088\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.219867\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.217668\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.215491\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.213336\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.221203\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.228991\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.236701\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.234334\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.241991\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.249571\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.247075\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.254604\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.252058\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.249538\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.257042\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -20.244472\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.252027\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.259507\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.266912\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.274243\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.271500\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.268785\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.276098\n",
            "resetting env. episode 231.000000, reward total was -19.000000. running mean: -20.263337\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.270703\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.277996\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.285216\n",
            "resetting env. episode 235.000000, reward total was -18.000000. running mean: -20.262364\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.259740\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.267143\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.264472\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.271827\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.269109\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.276417\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -20.263653\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.271017\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.278307\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.285524\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.272668\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.269942\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.277242\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.284470\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.291625\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.298709\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.295722\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.292765\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.289837\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.286939\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.284069\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.291228\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.288316\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.285433\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.282579\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.289753\n",
            "resetting env. episode 262.000000, reward total was -18.000000. running mean: -20.266855\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.264187\n",
            "resetting env. episode 264.000000, reward total was -19.000000. running mean: -20.251545\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.259029\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.266439\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.263775\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.271137\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.268426\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -20.255741\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.263184\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.260552\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.267947\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.265267\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.272615\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.279888\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.287089\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.294219\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.301276\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.308264\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.315181\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.322029\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.328809\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.335521\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.342166\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.348744\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.345257\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.351804\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.358286\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.354703\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.361156\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.367544\n",
            "resetting env. episode 293.000000, reward total was -19.000000. running mean: -20.353869\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.360330\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.356727\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.363160\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -20.349528\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.356033\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.362473\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.368848\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.365159\n",
            "resetting env. episode 302.000000, reward total was -19.000000. running mean: -20.351508\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.357993\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.364413\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.360769\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.367161\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.373489\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.379754\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.385957\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.392097\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.398176\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.404195\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.400153\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.406151\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.412090\n",
            "resetting env. episode 316.000000, reward total was -18.000000. running mean: -20.387969\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.384089\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.390248\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.396346\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.392382\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -20.378458\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.384674\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.380827\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.377019\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.373249\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.379516\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.385721\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.381864\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -20.368045\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.374365\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.370621\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.376915\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.373146\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.369414\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.375720\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.371963\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.378243\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.374461\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.360716\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.357109\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.363538\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.369903\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.376204\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.382441\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.388617\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.384731\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.370884\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.377175\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.383403\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.389569\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.395673\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.401717\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.397699\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.403722\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.409685\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.415588\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.411432\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.407318\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.403245\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.409212\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.415120\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.410969\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.416859\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.412691\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -20.398564\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.394578\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.400633\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.406626\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.402560\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.408534\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.404449\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.400405\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.406400\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.402336\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.408313\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.414230\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.420088\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.415887\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.421728\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.417511\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.423336\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.429102\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -20.414811\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.420663\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.426456\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.422192\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.417970\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.423790\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.419552\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.425357\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.431103\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.436792\n",
            "resetting env. episode 393.000000, reward total was -18.000000. running mean: -20.412424\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.398300\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.384317\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.380474\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.386669\n",
            "resetting env. episode 398.000000, reward total was -19.000000. running mean: -20.372802\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.379074\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.375284\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.371531\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.377816\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.384037\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.380197\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.386395\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.382531\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.388706\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.394819\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.400871\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.406862\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.412793\n",
            "resetting env. episode 412.000000, reward total was -19.000000. running mean: -20.398665\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.404679\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.390632\n",
            "resetting env. episode 415.000000, reward total was -19.000000. running mean: -20.376726\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.372958\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.379229\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.385436\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.391582\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.397666\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -20.383690\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.389853\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.395954\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.391995\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.388075\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.384194\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.390352\n",
            "resetting env. episode 428.000000, reward total was -18.000000. running mean: -20.366448\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.372784\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.379056\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.385266\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.391413\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.387499\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.393624\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.399688\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.405691\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.411634\n",
            "resetting env. episode 438.000000, reward total was -19.000000. running mean: -20.397517\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.393542\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.389607\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.395711\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.391754\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.397836\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.403858\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.409819\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.415721\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.421564\n",
            "resetting env. episode 448.000000, reward total was -18.000000. running mean: -20.397348\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.393375\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.379441\n",
            "resetting env. episode 451.000000, reward total was -18.000000. running mean: -20.355646\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.352090\n",
            "resetting env. episode 453.000000, reward total was -19.000000. running mean: -20.338569\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.325183\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.321932\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.308712\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.315625\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.322469\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.319244\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.316052\n",
            "resetting env. episode 461.000000, reward total was -18.000000. running mean: -20.292891\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.289962\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.287063\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.294192\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.301250\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.298238\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.295255\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.302303\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.299280\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.296287\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.303324\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.310291\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.307188\n",
            "resetting env. episode 474.000000, reward total was -19.000000. running mean: -20.294116\n",
            "resetting env. episode 475.000000, reward total was -19.000000. running mean: -20.281175\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.288363\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.285479\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.282625\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.289798\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.286900\n",
            "resetting env. episode 481.000000, reward total was -18.000000. running mean: -20.264031\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.271391\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.268677\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.265990\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.273331\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.270597\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.277891\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.275112\n",
            "resetting env. episode 489.000000, reward total was -18.000000. running mean: -20.252361\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.259838\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.267239\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.254567\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.262021\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.269401\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.276707\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.273940\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.281200\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.268388\n",
            "resetting env. episode 499.000000, reward total was -18.000000. running mean: -20.245705\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.243248\n",
            "CPU times: user 2h 18min 11s, sys: 18min, total: 2h 36min 12s\n",
            "Wall time: 1h 20min 45s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCZR5OV-z-YJ",
        "outputId": "2333a9d6-ecad-4b4f-c66e-a82911bd84d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -19.000000. running mean: -19.990000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -19.990100\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.000199\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.000197\n",
            "resetting env. episode 6.000000, reward total was -19.000000. running mean: -19.990195\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.000293\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.010290\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -20.000187\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.010185\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.020084\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.029883\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.029584\n",
            "resetting env. episode 14.000000, reward total was -19.000000. running mean: -20.019288\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.029095\n",
            "resetting env. episode 16.000000, reward total was -19.000000. running mean: -20.018804\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.028616\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.028330\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.028047\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.037766\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.047389\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.056915\n",
            "resetting env. episode 23.000000, reward total was -19.000000. running mean: -20.046346\n",
            "resetting env. episode 24.000000, reward total was -17.000000. running mean: -20.015882\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.025723\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.025466\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.035211\n",
            "resetting env. episode 28.000000, reward total was -19.000000. running mean: -20.024859\n",
            "resetting env. episode 29.000000, reward total was -19.000000. running mean: -20.014611\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.024465\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.034220\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.033878\n",
            "resetting env. episode 33.000000, reward total was -19.000000. running mean: -20.023539\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.023304\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.023071\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.032840\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.032511\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.032186\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.031864\n",
            "resetting env. episode 40.000000, reward total was -19.000000. running mean: -20.021546\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.031330\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.041017\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.050607\n",
            "resetting env. episode 44.000000, reward total was -18.000000. running mean: -20.030101\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.039800\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.049402\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.058908\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.068319\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.067635\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.066959\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.076290\n",
            "resetting env. episode 52.000000, reward total was -19.000000. running mean: -20.065527\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.064871\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.064223\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.073580\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.082845\n",
            "resetting env. episode 57.000000, reward total was -19.000000. running mean: -20.072016\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.071296\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.070583\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.069877\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.069178\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.078487\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -20.067702\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.067025\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.076355\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.085591\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.094735\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.103788\n",
            "resetting env. episode 69.000000, reward total was -18.000000. running mean: -20.082750\n",
            "resetting env. episode 70.000000, reward total was -18.000000. running mean: -20.061922\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.071303\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.080590\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.079784\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.078986\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.078197\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.087415\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.096540\n",
            "resetting env. episode 78.000000, reward total was -19.000000. running mean: -20.085575\n",
            "resetting env. episode 79.000000, reward total was -18.000000. running mean: -20.064719\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.074072\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.083331\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.092498\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.101573\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.100557\n",
            "resetting env. episode 85.000000, reward total was -19.000000. running mean: -20.089552\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.088656\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.087770\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.086892\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.096023\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.105063\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.114012\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.122872\n",
            "resetting env. episode 93.000000, reward total was -17.000000. running mean: -20.091643\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.100727\n",
            "resetting env. episode 95.000000, reward total was -18.000000. running mean: -20.079720\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.088922\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.088033\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.097153\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.096181\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.095220\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -20.084267\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.083425\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.092590\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.101665\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.110648\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.119541\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.118346\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.127163\n",
            "resetting env. episode 109.000000, reward total was -19.000000. running mean: -20.115891\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.114732\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.113585\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.122449\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.131224\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.139912\n",
            "resetting env. episode 115.000000, reward total was -19.000000. running mean: -20.128513\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.137228\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.135856\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -20.124497\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -20.123252\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.132020\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.140699\n",
            "resetting env. episode 122.000000, reward total was -19.000000. running mean: -20.129292\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.137999\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.136619\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.145253\n",
            "resetting env. episode 126.000000, reward total was -19.000000. running mean: -20.133801\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.142463\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.141038\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.149628\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.158131\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.156550\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.164985\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -20.153335\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.161801\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.170183\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.178482\n",
            "resetting env. episode 137.000000, reward total was -19.000000. running mean: -20.166697\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.165030\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.173379\n",
            "resetting env. episode 140.000000, reward total was -18.000000. running mean: -20.151646\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.150129\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.158628\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.167042\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.175371\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.173618\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.181881\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.180063\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.178262\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.186479\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.194614\n",
            "resetting env. episode 151.000000, reward total was -18.000000. running mean: -20.172668\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.180942\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.179132\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.187341\n",
            "resetting env. episode 155.000000, reward total was -19.000000. running mean: -20.175468\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.183713\n",
            "resetting env. episode 157.000000, reward total was -19.000000. running mean: -20.171876\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.180157\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.188355\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.196472\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.204507\n",
            "resetting env. episode 162.000000, reward total was -19.000000. running mean: -20.192462\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.200537\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.208532\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.216447\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.214282\n",
            "resetting env. episode 167.000000, reward total was -19.000000. running mean: -20.202139\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.190118\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.198217\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.186235\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.194372\n",
            "resetting env. episode 172.000000, reward total was -19.000000. running mean: -20.182429\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.180604\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.188798\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.196910\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.194941\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.192992\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.201062\n",
            "resetting env. episode 179.000000, reward total was -19.000000. running mean: -20.189051\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.187161\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.195289\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.193336\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.191403\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.189489\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -20.177594\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.175818\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.174060\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.182319\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.180496\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.188691\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.196804\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.194836\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.202888\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.200859\n",
            "resetting env. episode 195.000000, reward total was -19.000000. running mean: -20.188850\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.196962\n",
            "resetting env. episode 197.000000, reward total was -19.000000. running mean: -20.184992\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.193142\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.191211\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.199299\n",
            "resetting env. episode 201.000000, reward total was -19.000000. running mean: -20.187306\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.195433\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.203478\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.211444\n",
            "resetting env. episode 205.000000, reward total was -17.000000. running mean: -20.179329\n",
            "resetting env. episode 206.000000, reward total was -18.000000. running mean: -20.157536\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.155960\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.164401\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.152757\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.151229\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.159717\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.168120\n",
            "resetting env. episode 213.000000, reward total was -18.000000. running mean: -20.146439\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.144974\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.143525\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.152089\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.160568\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.158963\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.167373\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.175699\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.173942\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.172203\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.180481\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -20.168676\n",
            "resetting env. episode 225.000000, reward total was -19.000000. running mean: -20.156989\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.165419\n",
            "resetting env. episode 227.000000, reward total was -18.000000. running mean: -20.143765\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.152328\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.150804\n",
            "resetting env. episode 230.000000, reward total was -19.000000. running mean: -20.139296\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.147903\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.156424\n",
            "resetting env. episode 233.000000, reward total was -18.000000. running mean: -20.134860\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.143511\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -20.132076\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.130756\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.139448\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.148054\n",
            "resetting env. episode 239.000000, reward total was -19.000000. running mean: -20.136573\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.125207\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.133955\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.132616\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.141289\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.149877\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.158378\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.166794\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.165126\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.173475\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.181740\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.179923\n",
            "resetting env. episode 251.000000, reward total was -19.000000. running mean: -20.168123\n",
            "resetting env. episode 252.000000, reward total was -19.000000. running mean: -20.156442\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.154878\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.163329\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.161696\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.170079\n",
            "resetting env. episode 257.000000, reward total was -18.000000. running mean: -20.148378\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.156894\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.165325\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.163672\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.162035\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.160415\n",
            "resetting env. episode 263.000000, reward total was -19.000000. running mean: -20.148811\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.157323\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.165749\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.164092\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.172451\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.170727\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.169019\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.167329\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.175656\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.173899\n",
            "resetting env. episode 273.000000, reward total was -17.000000. running mean: -20.142160\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.150739\n",
            "resetting env. episode 275.000000, reward total was -19.000000. running mean: -20.139231\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.137839\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.146461\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.154996\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.163446\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.171812\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.180093\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.178292\n",
            "resetting env. episode 283.000000, reward total was -18.000000. running mean: -20.156510\n",
            "resetting env. episode 284.000000, reward total was -18.000000. running mean: -20.134944\n",
            "resetting env. episode 285.000000, reward total was -19.000000. running mean: -20.123595\n",
            "resetting env. episode 286.000000, reward total was -19.000000. running mean: -20.112359\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.101235\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.100223\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.099221\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.098229\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.097246\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.106274\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.105211\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -20.094159\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.103217\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.112185\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.121063\n",
            "resetting env. episode 298.000000, reward total was -18.000000. running mean: -20.099853\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.108854\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.117766\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.126588\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.125322\n",
            "resetting env. episode 303.000000, reward total was -17.000000. running mean: -20.094069\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.083128\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.092297\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.091374\n",
            "resetting env. episode 307.000000, reward total was -19.000000. running mean: -20.080460\n",
            "resetting env. episode 308.000000, reward total was -19.000000. running mean: -20.069656\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.078959\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.078170\n",
            "resetting env. episode 311.000000, reward total was -19.000000. running mean: -20.067388\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.076714\n",
            "resetting env. episode 313.000000, reward total was -19.000000. running mean: -20.065947\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.065287\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.064634\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.063988\n",
            "resetting env. episode 317.000000, reward total was -18.000000. running mean: -20.043348\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.052915\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.062386\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.071762\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.081044\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.080234\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.079431\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.078637\n",
            "resetting env. episode 325.000000, reward total was -18.000000. running mean: -20.057851\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.047272\n",
            "resetting env. episode 327.000000, reward total was -19.000000. running mean: -20.036799\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.036431\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.046067\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.045606\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.055150\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.064599\n",
            "resetting env. episode 333.000000, reward total was -19.000000. running mean: -20.053953\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.053413\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.052879\n",
            "resetting env. episode 336.000000, reward total was -19.000000. running mean: -20.042350\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.041927\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.051508\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.060993\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.070383\n",
            "resetting env. episode 341.000000, reward total was -18.000000. running mean: -20.049679\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.049182\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.048690\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.048203\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.047721\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.057244\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.066672\n",
            "resetting env. episode 348.000000, reward total was -18.000000. running mean: -20.046005\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.045545\n",
            "resetting env. episode 350.000000, reward total was -18.000000. running mean: -20.025089\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.024839\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.024590\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -20.014344\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.014201\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.014059\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -20.003918\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.003879\n",
            "resetting env. episode 358.000000, reward total was -19.000000. running mean: -19.993840\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -19.993902\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -19.993963\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -19.994023\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -19.984083\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -19.984242\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -19.984400\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -19.974556\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -19.984810\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -19.994962\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.005012\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.014962\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.024813\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.034565\n",
            "resetting env. episode 372.000000, reward total was -15.000000. running mean: -19.984219\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -19.984377\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -19.994533\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -19.994588\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -19.994642\n",
            "resetting env. episode 377.000000, reward total was -19.000000. running mean: -19.984695\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -19.974848\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -19.985100\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -19.995249\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.005296\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.005243\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -19.995191\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.005239\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.005187\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.005135\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.005084\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.015033\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.024882\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.014634\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.014487\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.024342\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.024099\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.033858\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.023519\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.023284\n",
            "resetting env. episode 397.000000, reward total was -18.000000. running mean: -20.003051\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.003021\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.012991\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.022861\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.022632\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.012406\n",
            "resetting env. episode 403.000000, reward total was -18.000000. running mean: -19.992282\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.002359\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -19.992335\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -19.992412\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.002488\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.002463\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -19.992438\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -19.992514\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -19.992589\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -19.992663\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -19.982736\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -19.982909\n",
            "resetting env. episode 415.000000, reward total was -19.000000. running mean: -19.973080\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -19.983349\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -19.993516\n",
            "resetting env. episode 418.000000, reward total was -18.000000. running mean: -19.973580\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -19.983845\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -19.994006\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -19.994066\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.004125\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.004084\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.004043\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.014003\n",
            "resetting env. episode 426.000000, reward total was -19.000000. running mean: -20.003863\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.003824\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.003786\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.013748\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.013611\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.023475\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.023240\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -20.013007\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.012877\n",
            "resetting env. episode 435.000000, reward total was -19.000000. running mean: -20.002749\n",
            "resetting env. episode 436.000000, reward total was -19.000000. running mean: -19.992721\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.002794\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.002766\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.012738\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.022611\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.032385\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.042061\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.041640\n",
            "resetting env. episode 444.000000, reward total was -18.000000. running mean: -20.021224\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.021012\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.030802\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.020494\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.020289\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -20.010086\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -19.999985\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.009985\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.019885\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.029686\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.029389\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.029096\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.038805\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.048417\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.047932\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.047453\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.056979\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.056409\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.065845\n",
            "resetting env. episode 463.000000, reward total was -18.000000. running mean: -20.045186\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.044734\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.054287\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.053744\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.063207\n",
            "resetting env. episode 468.000000, reward total was -19.000000. running mean: -20.052575\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.052049\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.051528\n",
            "resetting env. episode 471.000000, reward total was -18.000000. running mean: -20.031013\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.040703\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.050296\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.049793\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.059295\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.058702\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.068115\n",
            "resetting env. episode 478.000000, reward total was -19.000000. running mean: -20.057434\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.056860\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.066291\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.075628\n",
            "resetting env. episode 482.000000, reward total was -18.000000. running mean: -20.054872\n",
            "resetting env. episode 483.000000, reward total was -19.000000. running mean: -20.044323\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.053880\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.053341\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.062808\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.072180\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.071458\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.070743\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.070036\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.079335\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.088542\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.087657\n",
            "resetting env. episode 494.000000, reward total was -18.000000. running mean: -20.066780\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -20.056112\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.055551\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.064996\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.054346\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.053802\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.063264\n",
            "CPU times: user 2h 24min 14s, sys: 19min 3s, total: 2h 43min 18s\n",
            "Wall time: 1h 24min 33s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "96438cf1-75d9-4968-8dab-541a88930660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "55ec770b-f032-4834-a841-01c1212b9acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -19.010000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -19.019900\n",
            "resetting env. episode 4.000000, reward total was -17.000000. running mean: -18.999701\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -19.019704\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -19.039507\n",
            "resetting env. episode 7.000000, reward total was -19.000000. running mean: -19.039112\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -19.058721\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.078134\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -19.097352\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -19.106379\n",
            "resetting env. episode 12.000000, reward total was -17.000000. running mean: -19.085315\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -19.084462\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -19.103617\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -19.102581\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -19.111555\n",
            "resetting env. episode 17.000000, reward total was -19.000000. running mean: -19.110440\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -19.119335\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -19.128142\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -19.146860\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -19.155392\n",
            "resetting env. episode 22.000000, reward total was -19.000000. running mean: -19.153838\n",
            "resetting env. episode 23.000000, reward total was -19.000000. running mean: -19.152300\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -19.160777\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -19.159169\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.177577\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.195801\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -19.213843\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -19.221705\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -19.229488\n",
            "resetting env. episode 31.000000, reward total was -19.000000. running mean: -19.227193\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -19.224921\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.242672\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -19.250245\n",
            "resetting env. episode 35.000000, reward total was -18.000000. running mean: -19.237743\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -19.235365\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -19.253012\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -19.250481\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -19.267977\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.285297\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -19.302444\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -19.309419\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -19.316325\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.333162\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -19.329830\n",
            "resetting env. episode 46.000000, reward total was -17.000000. running mean: -19.306532\n",
            "resetting env. episode 47.000000, reward total was -19.000000. running mean: -19.303467\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.320432\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.337228\n",
            "resetting env. episode 50.000000, reward total was -18.000000. running mean: -19.323855\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.340617\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -19.357211\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -19.373639\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -19.389902\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -19.396003\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.412043\n",
            "resetting env. episode 57.000000, reward total was -19.000000. running mean: -19.407923\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -19.413844\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.429705\n",
            "resetting env. episode 60.000000, reward total was -19.000000. running mean: -19.425408\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -19.431154\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.446842\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -19.452374\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.467850\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -19.473172\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.488440\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -19.503556\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -19.508520\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -19.513435\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.528301\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -19.533018\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -19.537687\n",
            "resetting env. episode 73.000000, reward total was -19.000000. running mean: -19.532311\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -19.526987\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.541718\n",
            "resetting env. episode 76.000000, reward total was -18.000000. running mean: -19.526300\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.541037\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.555627\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -19.560071\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -19.574470\n",
            "resetting env. episode 81.000000, reward total was -19.000000. running mean: -19.568725\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.583038\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.597208\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.611236\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -19.615123\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -19.618972\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -19.612782\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.626654\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.640388\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -19.643984\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -19.647544\n",
            "resetting env. episode 92.000000, reward total was -19.000000. running mean: -19.641069\n",
            "resetting env. episode 93.000000, reward total was -19.000000. running mean: -19.634658\n",
            "resetting env. episode 94.000000, reward total was -19.000000. running mean: -19.628311\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.642028\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.655608\n",
            "resetting env. episode 97.000000, reward total was -18.000000. running mean: -19.639052\n",
            "resetting env. episode 98.000000, reward total was -17.000000. running mean: -19.612661\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -19.606535\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -19.610470\n",
            "resetting env. episode 101.000000, reward total was -18.000000. running mean: -19.594365\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -19.608421\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -19.602337\n",
            "resetting env. episode 104.000000, reward total was -18.000000. running mean: -19.586314\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.600450\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.614446\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.628301\n",
            "resetting env. episode 108.000000, reward total was -19.000000. running mean: -19.622018\n",
            "resetting env. episode 109.000000, reward total was -17.000000. running mean: -19.595798\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.609840\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -19.603742\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.617704\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -19.631527\n",
            "resetting env. episode 114.000000, reward total was -18.000000. running mean: -19.615212\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -19.629060\n",
            "resetting env. episode 116.000000, reward total was -17.000000. running mean: -19.602769\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.616742\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.630574\n",
            "resetting env. episode 119.000000, reward total was -18.000000. running mean: -19.614269\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -19.628126\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.641845\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -19.645426\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -19.658972\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -19.672382\n",
            "resetting env. episode 125.000000, reward total was -19.000000. running mean: -19.665658\n",
            "resetting env. episode 126.000000, reward total was -19.000000. running mean: -19.659002\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -19.672412\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -19.685688\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -19.688831\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -19.681942\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -19.695123\n",
            "resetting env. episode 132.000000, reward total was -18.000000. running mean: -19.678172\n",
            "resetting env. episode 133.000000, reward total was -15.000000. running mean: -19.631390\n",
            "resetting env. episode 134.000000, reward total was -18.000000. running mean: -19.615076\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -19.628925\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -19.642636\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -19.656210\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -19.669648\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -19.672951\n",
            "resetting env. episode 140.000000, reward total was -18.000000. running mean: -19.656222\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -19.669660\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -19.672963\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -19.686233\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -19.699371\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -19.712377\n",
            "resetting env. episode 146.000000, reward total was -18.000000. running mean: -19.695253\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -19.708301\n",
            "resetting env. episode 148.000000, reward total was -17.000000. running mean: -19.681218\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -19.694406\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -19.707462\n",
            "resetting env. episode 151.000000, reward total was -19.000000. running mean: -19.700387\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -19.713383\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -19.716249\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -19.719087\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -19.731896\n",
            "resetting env. episode 156.000000, reward total was -15.000000. running mean: -19.684577\n",
            "resetting env. episode 157.000000, reward total was -19.000000. running mean: -19.677731\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -19.690954\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -19.694044\n",
            "resetting env. episode 160.000000, reward total was -17.000000. running mean: -19.667104\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -19.670433\n",
            "resetting env. episode 162.000000, reward total was -17.000000. running mean: -19.643729\n",
            "resetting env. episode 163.000000, reward total was -19.000000. running mean: -19.637291\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -19.640918\n",
            "resetting env. episode 165.000000, reward total was -15.000000. running mean: -19.594509\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -19.598564\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -19.612579\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -19.616453\n",
            "resetting env. episode 169.000000, reward total was -18.000000. running mean: -19.600288\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -19.604285\n",
            "resetting env. episode 171.000000, reward total was -19.000000. running mean: -19.598242\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -19.612260\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -19.616137\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -19.629976\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -19.643676\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -19.657240\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -19.670667\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -19.683960\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -19.687121\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -19.690250\n",
            "resetting env. episode 181.000000, reward total was -19.000000. running mean: -19.683347\n",
            "resetting env. episode 182.000000, reward total was -18.000000. running mean: -19.666514\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -19.679849\n",
            "resetting env. episode 184.000000, reward total was -18.000000. running mean: -19.663050\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -19.656420\n",
            "resetting env. episode 186.000000, reward total was -17.000000. running mean: -19.629855\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -19.633557\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -19.647221\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -19.660749\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -19.674142\n",
            "resetting env. episode 191.000000, reward total was -18.000000. running mean: -19.657400\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -19.660826\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -19.674218\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -19.667476\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -19.680801\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -19.693993\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -19.707053\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -19.709982\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -19.712883\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -19.725754\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -19.728496\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -19.731211\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -19.743899\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -19.756460\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -19.768896\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -19.781207\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -19.783395\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -19.795561\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -19.797605\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -19.799629\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -19.791633\n",
            "resetting env. episode 212.000000, reward total was -18.000000. running mean: -19.773716\n",
            "resetting env. episode 213.000000, reward total was -18.000000. running mean: -19.755979\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -19.768419\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -19.780735\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -19.782928\n",
            "resetting env. episode 217.000000, reward total was -18.000000. running mean: -19.765099\n",
            "resetting env. episode 218.000000, reward total was -18.000000. running mean: -19.747448\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -19.749973\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -19.742473\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -19.745049\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -19.757598\n",
            "resetting env. episode 223.000000, reward total was -18.000000. running mean: -19.740022\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -19.752622\n",
            "resetting env. episode 225.000000, reward total was -19.000000. running mean: -19.745096\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -19.737645\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -19.750268\n",
            "resetting env. episode 228.000000, reward total was -19.000000. running mean: -19.742766\n",
            "resetting env. episode 229.000000, reward total was -17.000000. running mean: -19.715338\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -19.718185\n",
            "resetting env. episode 231.000000, reward total was -19.000000. running mean: -19.711003\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -19.713893\n",
            "resetting env. episode 233.000000, reward total was -19.000000. running mean: -19.706754\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -19.709686\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -19.702589\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -19.715564\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -19.708408\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -19.721324\n",
            "resetting env. episode 239.000000, reward total was -19.000000. running mean: -19.714111\n",
            "resetting env. episode 240.000000, reward total was -18.000000. running mean: -19.696969\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -19.700000\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -19.713000\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -19.725870\n",
            "resetting env. episode 244.000000, reward total was -17.000000. running mean: -19.698611\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -19.711625\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -19.724509\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -19.727264\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -19.739991\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -19.742591\n",
            "resetting env. episode 250.000000, reward total was -19.000000. running mean: -19.735165\n",
            "resetting env. episode 251.000000, reward total was -17.000000. running mean: -19.707814\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -19.720735\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -19.723528\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -19.726293\n",
            "resetting env. episode 255.000000, reward total was -19.000000. running mean: -19.719030\n",
            "resetting env. episode 256.000000, reward total was -18.000000. running mean: -19.701840\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -19.704821\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -19.717773\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -19.710595\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -19.723489\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -19.726254\n",
            "resetting env. episode 262.000000, reward total was -19.000000. running mean: -19.718992\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -19.731802\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -19.744484\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -19.757039\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -19.759469\n",
            "resetting env. episode 267.000000, reward total was -19.000000. running mean: -19.751874\n",
            "resetting env. episode 268.000000, reward total was -19.000000. running mean: -19.744355\n",
            "resetting env. episode 269.000000, reward total was -19.000000. running mean: -19.736912\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -19.739543\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -19.752147\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -19.754626\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -19.757079\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -19.759509\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -19.761914\n",
            "resetting env. episode 276.000000, reward total was -19.000000. running mean: -19.754294\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -19.766751\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -19.769084\n",
            "resetting env. episode 279.000000, reward total was -19.000000. running mean: -19.761393\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -19.753779\n",
            "resetting env. episode 281.000000, reward total was -17.000000. running mean: -19.726241\n",
            "resetting env. episode 282.000000, reward total was -18.000000. running mean: -19.708979\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -19.711889\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -19.724770\n",
            "resetting env. episode 285.000000, reward total was -19.000000. running mean: -19.717523\n",
            "resetting env. episode 286.000000, reward total was -15.000000. running mean: -19.670347\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -19.673644\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -19.676907\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -19.680138\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -19.693337\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -19.706404\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -19.709340\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -19.722246\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -19.715024\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -19.717873\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -19.730695\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -19.733388\n",
            "resetting env. episode 298.000000, reward total was -19.000000. running mean: -19.726054\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -19.728793\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -19.741505\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -19.744090\n",
            "resetting env. episode 302.000000, reward total was -19.000000. running mean: -19.736649\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -19.739283\n",
            "resetting env. episode 304.000000, reward total was -18.000000. running mean: -19.721890\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -19.714671\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -19.727525\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -19.740249\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -19.742847\n",
            "resetting env. episode 309.000000, reward total was -18.000000. running mean: -19.725418\n",
            "resetting env. episode 310.000000, reward total was -18.000000. running mean: -19.708164\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -19.721083\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -19.713872\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -19.726733\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -19.739466\n",
            "resetting env. episode 315.000000, reward total was -19.000000. running mean: -19.732071\n",
            "resetting env. episode 316.000000, reward total was -18.000000. running mean: -19.714750\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -19.717603\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -19.720427\n",
            "resetting env. episode 319.000000, reward total was -19.000000. running mean: -19.713222\n",
            "resetting env. episode 320.000000, reward total was -18.000000. running mean: -19.696090\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -19.699129\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -19.702138\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -19.715117\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -19.727966\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -19.740686\n",
            "resetting env. episode 326.000000, reward total was -18.000000. running mean: -19.723279\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -19.736046\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -19.728686\n",
            "resetting env. episode 329.000000, reward total was -18.000000. running mean: -19.711399\n",
            "resetting env. episode 330.000000, reward total was -19.000000. running mean: -19.704285\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -19.717242\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -19.720070\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -19.732869\n",
            "resetting env. episode 334.000000, reward total was -19.000000. running mean: -19.725540\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -19.738285\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -19.750902\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -19.753393\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -19.755859\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -19.758300\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -19.770717\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -19.773010\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -19.785280\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -19.787427\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -19.789553\n",
            "resetting env. episode 345.000000, reward total was -19.000000. running mean: -19.781658\n",
            "resetting env. episode 346.000000, reward total was -18.000000. running mean: -19.763841\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -19.766203\n",
            "resetting env. episode 348.000000, reward total was -19.000000. running mean: -19.758541\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -19.770955\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -19.783246\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -19.785413\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -19.797559\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -19.799583\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -19.791588\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -19.793672\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -19.785735\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -19.797878\n",
            "resetting env. episode 358.000000, reward total was -19.000000. running mean: -19.789899\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -19.792000\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -19.794080\n",
            "resetting env. episode 361.000000, reward total was -18.000000. running mean: -19.776139\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -19.768378\n",
            "resetting env. episode 363.000000, reward total was -17.000000. running mean: -19.740694\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -19.743287\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -19.755854\n",
            "resetting env. episode 366.000000, reward total was -18.000000. running mean: -19.738296\n",
            "resetting env. episode 367.000000, reward total was -19.000000. running mean: -19.730913\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -19.723603\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -19.736367\n",
            "resetting env. episode 370.000000, reward total was -18.000000. running mean: -19.719004\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -19.731814\n",
            "resetting env. episode 372.000000, reward total was -18.000000. running mean: -19.714496\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -19.727351\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -19.740077\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -19.742676\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -19.755250\n",
            "resetting env. episode 377.000000, reward total was -18.000000. running mean: -19.737697\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -19.750320\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -19.752817\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -19.765289\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -19.777636\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -19.769860\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -19.772161\n",
            "resetting env. episode 384.000000, reward total was -19.000000. running mean: -19.764439\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -19.766795\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -19.779127\n",
            "resetting env. episode 387.000000, reward total was -18.000000. running mean: -19.761336\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -19.763722\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -19.766085\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -19.768424\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -19.760740\n",
            "resetting env. episode 392.000000, reward total was -18.000000. running mean: -19.743133\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -19.745701\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -19.758244\n",
            "resetting env. episode 395.000000, reward total was -17.000000. running mean: -19.730662\n",
            "resetting env. episode 396.000000, reward total was -18.000000. running mean: -19.713355\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -19.716222\n",
            "resetting env. episode 398.000000, reward total was -19.000000. running mean: -19.709059\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -19.721969\n",
            "resetting env. episode 400.000000, reward total was -18.000000. running mean: -19.704749\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -19.697702\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -19.710725\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -19.723617\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -19.736381\n",
            "resetting env. episode 405.000000, reward total was -17.000000. running mean: -19.709017\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -19.721927\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -19.724708\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -19.737461\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -19.750086\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -19.762585\n",
            "resetting env. episode 411.000000, reward total was -18.000000. running mean: -19.744960\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -19.757510\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -19.749935\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -19.742436\n",
            "resetting env. episode 415.000000, reward total was -18.000000. running mean: -19.725011\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -19.737761\n",
            "resetting env. episode 417.000000, reward total was -18.000000. running mean: -19.720383\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -19.723180\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -19.735948\n",
            "resetting env. episode 420.000000, reward total was -18.000000. running mean: -19.718588\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -19.731402\n",
            "resetting env. episode 422.000000, reward total was -17.000000. running mean: -19.704088\n",
            "resetting env. episode 423.000000, reward total was -16.000000. running mean: -19.667048\n",
            "resetting env. episode 424.000000, reward total was -19.000000. running mean: -19.660377\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -19.673773\n",
            "resetting env. episode 426.000000, reward total was -18.000000. running mean: -19.657036\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -19.660465\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -19.663861\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -19.667222\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -19.680550\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -19.693744\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -19.696807\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -19.699839\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -19.692840\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -19.695912\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -19.708953\n",
            "resetting env. episode 437.000000, reward total was -18.000000. running mean: -19.691863\n",
            "resetting env. episode 438.000000, reward total was -17.000000. running mean: -19.664945\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -19.668295\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -19.671612\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -19.674896\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -19.688147\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -19.691266\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -19.694353\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -19.697410\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -19.690435\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -19.683531\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -19.696696\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -19.699729\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -19.692732\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -19.705804\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -19.708746\n",
            "resetting env. episode 453.000000, reward total was -19.000000. running mean: -19.701659\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -19.714642\n",
            "resetting env. episode 455.000000, reward total was -18.000000. running mean: -19.697496\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -19.710521\n",
            "resetting env. episode 457.000000, reward total was -18.000000. running mean: -19.693416\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -19.696481\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -19.709517\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -19.712421\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -19.705297\n",
            "resetting env. episode 462.000000, reward total was -19.000000. running mean: -19.698244\n",
            "resetting env. episode 463.000000, reward total was -18.000000. running mean: -19.681262\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -19.694449\n",
            "resetting env. episode 465.000000, reward total was -19.000000. running mean: -19.687505\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -19.690630\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -19.683723\n",
            "resetting env. episode 468.000000, reward total was -19.000000. running mean: -19.676886\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -19.680117\n",
            "resetting env. episode 470.000000, reward total was -18.000000. running mean: -19.663316\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -19.666683\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -19.680016\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -19.683216\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -19.696384\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -19.699420\n",
            "resetting env. episode 476.000000, reward total was -12.000000. running mean: -19.622426\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -19.626201\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -19.639939\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -19.653540\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -19.667005\n",
            "resetting env. episode 481.000000, reward total was -19.000000. running mean: -19.660335\n",
            "resetting env. episode 482.000000, reward total was -18.000000. running mean: -19.643731\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -19.647294\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -19.660821\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -19.664213\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -19.677571\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -19.690795\n",
            "resetting env. episode 488.000000, reward total was -18.000000. running mean: -19.673887\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -19.687148\n",
            "resetting env. episode 490.000000, reward total was -19.000000. running mean: -19.680277\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -19.683474\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -19.696639\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -19.709673\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -19.722576\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -19.725350\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -19.718097\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -19.720916\n",
            "resetting env. episode 498.000000, reward total was -18.000000. running mean: -19.703707\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -19.716670\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -19.729503\n",
            "resetting env. episode 501.000000, reward total was -21.000000. running mean: -19.742208\n",
            "resetting env. episode 502.000000, reward total was -17.000000. running mean: -19.714786\n",
            "resetting env. episode 503.000000, reward total was -20.000000. running mean: -19.717638\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -19.720462\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -19.733257\n",
            "resetting env. episode 506.000000, reward total was -20.000000. running mean: -19.735924\n",
            "resetting env. episode 507.000000, reward total was -20.000000. running mean: -19.738565\n",
            "resetting env. episode 508.000000, reward total was -18.000000. running mean: -19.721179\n",
            "resetting env. episode 509.000000, reward total was -17.000000. running mean: -19.693968\n",
            "resetting env. episode 510.000000, reward total was -19.000000. running mean: -19.687028\n",
            "resetting env. episode 511.000000, reward total was -18.000000. running mean: -19.670158\n",
            "resetting env. episode 512.000000, reward total was -18.000000. running mean: -19.653456\n",
            "resetting env. episode 513.000000, reward total was -20.000000. running mean: -19.656922\n",
            "resetting env. episode 514.000000, reward total was -20.000000. running mean: -19.660352\n",
            "resetting env. episode 515.000000, reward total was -19.000000. running mean: -19.653749\n",
            "resetting env. episode 516.000000, reward total was -20.000000. running mean: -19.657211\n",
            "resetting env. episode 517.000000, reward total was -20.000000. running mean: -19.660639\n",
            "resetting env. episode 518.000000, reward total was -18.000000. running mean: -19.644033\n",
            "resetting env. episode 519.000000, reward total was -16.000000. running mean: -19.607592\n",
            "resetting env. episode 520.000000, reward total was -17.000000. running mean: -19.581517\n",
            "resetting env. episode 521.000000, reward total was -18.000000. running mean: -19.565701\n",
            "resetting env. episode 522.000000, reward total was -16.000000. running mean: -19.530044\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -19.534744\n",
            "resetting env. episode 524.000000, reward total was -19.000000. running mean: -19.529397\n",
            "resetting env. episode 525.000000, reward total was -16.000000. running mean: -19.494103\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -19.509162\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -19.524070\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -19.538829\n",
            "resetting env. episode 529.000000, reward total was -20.000000. running mean: -19.543441\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -19.558007\n",
            "resetting env. episode 531.000000, reward total was -19.000000. running mean: -19.552426\n",
            "resetting env. episode 532.000000, reward total was -19.000000. running mean: -19.546902\n",
            "resetting env. episode 533.000000, reward total was -19.000000. running mean: -19.541433\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -19.556019\n",
            "resetting env. episode 535.000000, reward total was -19.000000. running mean: -19.550459\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -19.564954\n",
            "resetting env. episode 537.000000, reward total was -20.000000. running mean: -19.569305\n",
            "resetting env. episode 538.000000, reward total was -19.000000. running mean: -19.563611\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -19.567975\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -19.582296\n",
            "resetting env. episode 541.000000, reward total was -20.000000. running mean: -19.586473\n",
            "resetting env. episode 542.000000, reward total was -20.000000. running mean: -19.590608\n",
            "resetting env. episode 543.000000, reward total was -19.000000. running mean: -19.584702\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -19.598855\n",
            "resetting env. episode 545.000000, reward total was -20.000000. running mean: -19.602866\n",
            "resetting env. episode 546.000000, reward total was -18.000000. running mean: -19.586838\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -19.600969\n",
            "resetting env. episode 548.000000, reward total was -19.000000. running mean: -19.594960\n",
            "resetting env. episode 549.000000, reward total was -20.000000. running mean: -19.599010\n",
            "resetting env. episode 550.000000, reward total was -20.000000. running mean: -19.603020\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -19.616990\n",
            "resetting env. episode 552.000000, reward total was -19.000000. running mean: -19.610820\n",
            "resetting env. episode 553.000000, reward total was -17.000000. running mean: -19.584712\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -19.598864\n",
            "resetting env. episode 555.000000, reward total was -20.000000. running mean: -19.602876\n",
            "resetting env. episode 556.000000, reward total was -18.000000. running mean: -19.586847\n",
            "resetting env. episode 557.000000, reward total was -20.000000. running mean: -19.590979\n",
            "resetting env. episode 558.000000, reward total was -20.000000. running mean: -19.595069\n",
            "resetting env. episode 559.000000, reward total was -19.000000. running mean: -19.589118\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -19.603227\n",
            "resetting env. episode 561.000000, reward total was -19.000000. running mean: -19.597195\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -19.611223\n",
            "resetting env. episode 563.000000, reward total was -19.000000. running mean: -19.605110\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -19.619059\n",
            "resetting env. episode 565.000000, reward total was -20.000000. running mean: -19.622869\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -19.636640\n",
            "resetting env. episode 567.000000, reward total was -19.000000. running mean: -19.630274\n",
            "resetting env. episode 568.000000, reward total was -20.000000. running mean: -19.633971\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -19.647631\n",
            "resetting env. episode 570.000000, reward total was -20.000000. running mean: -19.651155\n",
            "resetting env. episode 571.000000, reward total was -20.000000. running mean: -19.654643\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -19.668097\n",
            "resetting env. episode 573.000000, reward total was -19.000000. running mean: -19.661416\n",
            "resetting env. episode 574.000000, reward total was -20.000000. running mean: -19.664802\n",
            "resetting env. episode 575.000000, reward total was -20.000000. running mean: -19.668154\n",
            "resetting env. episode 576.000000, reward total was -19.000000. running mean: -19.661472\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -19.674858\n",
            "resetting env. episode 578.000000, reward total was -17.000000. running mean: -19.648109\n",
            "resetting env. episode 579.000000, reward total was -20.000000. running mean: -19.651628\n",
            "resetting env. episode 580.000000, reward total was -19.000000. running mean: -19.645112\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -19.658660\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -19.672074\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -19.685353\n",
            "resetting env. episode 584.000000, reward total was -18.000000. running mean: -19.668500\n",
            "resetting env. episode 585.000000, reward total was -20.000000. running mean: -19.671815\n",
            "resetting env. episode 586.000000, reward total was -19.000000. running mean: -19.665096\n",
            "resetting env. episode 587.000000, reward total was -19.000000. running mean: -19.658445\n",
            "resetting env. episode 588.000000, reward total was -17.000000. running mean: -19.631861\n",
            "resetting env. episode 589.000000, reward total was -21.000000. running mean: -19.645542\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -19.659087\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -19.672496\n",
            "resetting env. episode 592.000000, reward total was -18.000000. running mean: -19.655771\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -19.669213\n",
            "resetting env. episode 594.000000, reward total was -19.000000. running mean: -19.662521\n",
            "resetting env. episode 595.000000, reward total was -20.000000. running mean: -19.665896\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -19.669237\n",
            "resetting env. episode 597.000000, reward total was -19.000000. running mean: -19.662545\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -19.675919\n",
            "resetting env. episode 599.000000, reward total was -20.000000. running mean: -19.679160\n",
            "resetting env. episode 600.000000, reward total was -19.000000. running mean: -19.672369\n",
            "resetting env. episode 601.000000, reward total was -18.000000. running mean: -19.655645\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -19.669088\n",
            "resetting env. episode 603.000000, reward total was -19.000000. running mean: -19.662398\n",
            "resetting env. episode 604.000000, reward total was -19.000000. running mean: -19.655774\n",
            "resetting env. episode 605.000000, reward total was -16.000000. running mean: -19.619216\n",
            "resetting env. episode 606.000000, reward total was -19.000000. running mean: -19.613024\n",
            "resetting env. episode 607.000000, reward total was -19.000000. running mean: -19.606893\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -19.620824\n",
            "resetting env. episode 609.000000, reward total was -19.000000. running mean: -19.614616\n",
            "resetting env. episode 610.000000, reward total was -20.000000. running mean: -19.618470\n",
            "resetting env. episode 611.000000, reward total was -20.000000. running mean: -19.622285\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -19.636063\n",
            "resetting env. episode 613.000000, reward total was -18.000000. running mean: -19.619702\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -19.633505\n",
            "resetting env. episode 615.000000, reward total was -18.000000. running mean: -19.617170\n",
            "resetting env. episode 616.000000, reward total was -19.000000. running mean: -19.610998\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -19.624888\n",
            "resetting env. episode 618.000000, reward total was -20.000000. running mean: -19.628639\n",
            "resetting env. episode 619.000000, reward total was -16.000000. running mean: -19.592353\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -19.606429\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -19.610365\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -19.624261\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -19.638019\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -19.651639\n",
            "resetting env. episode 625.000000, reward total was -20.000000. running mean: -19.655122\n",
            "resetting env. episode 626.000000, reward total was -17.000000. running mean: -19.628571\n",
            "resetting env. episode 627.000000, reward total was -20.000000. running mean: -19.632285\n",
            "resetting env. episode 628.000000, reward total was -20.000000. running mean: -19.635962\n",
            "resetting env. episode 629.000000, reward total was -20.000000. running mean: -19.639603\n",
            "resetting env. episode 630.000000, reward total was -19.000000. running mean: -19.633207\n",
            "resetting env. episode 631.000000, reward total was -20.000000. running mean: -19.636875\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -19.650506\n",
            "resetting env. episode 633.000000, reward total was -19.000000. running mean: -19.644001\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -19.657561\n",
            "resetting env. episode 635.000000, reward total was -17.000000. running mean: -19.630985\n",
            "resetting env. episode 636.000000, reward total was -19.000000. running mean: -19.624675\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -19.638429\n",
            "resetting env. episode 638.000000, reward total was -15.000000. running mean: -19.592044\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -19.606124\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -19.620063\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -19.633862\n",
            "resetting env. episode 642.000000, reward total was -16.000000. running mean: -19.597523\n",
            "resetting env. episode 643.000000, reward total was -19.000000. running mean: -19.591548\n",
            "resetting env. episode 644.000000, reward total was -18.000000. running mean: -19.575633\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -19.589876\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -19.603978\n",
            "resetting env. episode 647.000000, reward total was -19.000000. running mean: -19.597938\n",
            "resetting env. episode 648.000000, reward total was -19.000000. running mean: -19.591958\n",
            "resetting env. episode 649.000000, reward total was -19.000000. running mean: -19.586039\n",
            "resetting env. episode 650.000000, reward total was -18.000000. running mean: -19.570179\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -19.584477\n",
            "resetting env. episode 652.000000, reward total was -17.000000. running mean: -19.558632\n",
            "resetting env. episode 653.000000, reward total was -20.000000. running mean: -19.563046\n",
            "resetting env. episode 654.000000, reward total was -18.000000. running mean: -19.547415\n",
            "resetting env. episode 655.000000, reward total was -19.000000. running mean: -19.541941\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -19.546522\n",
            "resetting env. episode 657.000000, reward total was -19.000000. running mean: -19.541056\n",
            "resetting env. episode 658.000000, reward total was -20.000000. running mean: -19.545646\n",
            "resetting env. episode 659.000000, reward total was -17.000000. running mean: -19.520189\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -19.524987\n",
            "resetting env. episode 661.000000, reward total was -20.000000. running mean: -19.529738\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -19.534440\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -19.549096\n",
            "resetting env. episode 664.000000, reward total was -19.000000. running mean: -19.543605\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -19.548169\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -19.562687\n",
            "resetting env. episode 667.000000, reward total was -19.000000. running mean: -19.557060\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -19.571490\n",
            "resetting env. episode 669.000000, reward total was -20.000000. running mean: -19.575775\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -19.590017\n",
            "resetting env. episode 671.000000, reward total was -19.000000. running mean: -19.584117\n",
            "resetting env. episode 672.000000, reward total was -18.000000. running mean: -19.568276\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -19.582593\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -19.596767\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -19.610799\n",
            "resetting env. episode 676.000000, reward total was -21.000000. running mean: -19.624691\n",
            "resetting env. episode 677.000000, reward total was -19.000000. running mean: -19.618444\n",
            "resetting env. episode 678.000000, reward total was -20.000000. running mean: -19.622260\n",
            "resetting env. episode 679.000000, reward total was -18.000000. running mean: -19.606037\n",
            "resetting env. episode 680.000000, reward total was -20.000000. running mean: -19.609977\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -19.623877\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -19.637638\n",
            "resetting env. episode 683.000000, reward total was -18.000000. running mean: -19.621262\n",
            "resetting env. episode 684.000000, reward total was -18.000000. running mean: -19.605049\n",
            "resetting env. episode 685.000000, reward total was -19.000000. running mean: -19.598999\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -19.613009\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -19.616879\n",
            "resetting env. episode 688.000000, reward total was -20.000000. running mean: -19.620710\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -19.634503\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -19.638158\n",
            "resetting env. episode 691.000000, reward total was -21.000000. running mean: -19.651776\n",
            "resetting env. episode 692.000000, reward total was -20.000000. running mean: -19.655259\n",
            "resetting env. episode 693.000000, reward total was -19.000000. running mean: -19.648706\n",
            "resetting env. episode 694.000000, reward total was -19.000000. running mean: -19.642219\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -19.655797\n",
            "resetting env. episode 696.000000, reward total was -20.000000. running mean: -19.659239\n",
            "resetting env. episode 697.000000, reward total was -20.000000. running mean: -19.662646\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -19.676020\n",
            "resetting env. episode 699.000000, reward total was -20.000000. running mean: -19.679260\n",
            "resetting env. episode 700.000000, reward total was -18.000000. running mean: -19.662467\n",
            "resetting env. episode 701.000000, reward total was -18.000000. running mean: -19.645842\n",
            "resetting env. episode 702.000000, reward total was -19.000000. running mean: -19.639384\n",
            "resetting env. episode 703.000000, reward total was -19.000000. running mean: -19.632990\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -19.636660\n",
            "resetting env. episode 705.000000, reward total was -17.000000. running mean: -19.610294\n",
            "resetting env. episode 706.000000, reward total was -20.000000. running mean: -19.614191\n",
            "resetting env. episode 707.000000, reward total was -17.000000. running mean: -19.588049\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -19.592168\n",
            "resetting env. episode 709.000000, reward total was -19.000000. running mean: -19.586247\n",
            "resetting env. episode 710.000000, reward total was -18.000000. running mean: -19.570384\n",
            "resetting env. episode 711.000000, reward total was -19.000000. running mean: -19.564680\n",
            "resetting env. episode 712.000000, reward total was -19.000000. running mean: -19.559034\n",
            "resetting env. episode 713.000000, reward total was -18.000000. running mean: -19.543443\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -19.548009\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -19.562529\n",
            "resetting env. episode 716.000000, reward total was -19.000000. running mean: -19.556903\n",
            "resetting env. episode 717.000000, reward total was -20.000000. running mean: -19.561334\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -19.575721\n",
            "resetting env. episode 719.000000, reward total was -20.000000. running mean: -19.579964\n",
            "resetting env. episode 720.000000, reward total was -20.000000. running mean: -19.584164\n",
            "resetting env. episode 721.000000, reward total was -19.000000. running mean: -19.578323\n",
            "resetting env. episode 722.000000, reward total was -20.000000. running mean: -19.582539\n",
            "resetting env. episode 723.000000, reward total was -18.000000. running mean: -19.566714\n",
            "resetting env. episode 724.000000, reward total was -17.000000. running mean: -19.541047\n",
            "resetting env. episode 725.000000, reward total was -20.000000. running mean: -19.545636\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -19.560180\n",
            "resetting env. episode 727.000000, reward total was -19.000000. running mean: -19.554578\n",
            "resetting env. episode 728.000000, reward total was -20.000000. running mean: -19.559032\n",
            "resetting env. episode 729.000000, reward total was -19.000000. running mean: -19.553442\n",
            "resetting env. episode 730.000000, reward total was -19.000000. running mean: -19.547908\n",
            "resetting env. episode 731.000000, reward total was -20.000000. running mean: -19.552429\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -19.566904\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -19.581235\n",
            "resetting env. episode 734.000000, reward total was -18.000000. running mean: -19.565423\n",
            "resetting env. episode 735.000000, reward total was -20.000000. running mean: -19.569769\n",
            "resetting env. episode 736.000000, reward total was -19.000000. running mean: -19.564071\n",
            "resetting env. episode 737.000000, reward total was -20.000000. running mean: -19.568430\n",
            "resetting env. episode 738.000000, reward total was -20.000000. running mean: -19.572746\n",
            "resetting env. episode 739.000000, reward total was -19.000000. running mean: -19.567019\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -19.581348\n",
            "resetting env. episode 741.000000, reward total was -20.000000. running mean: -19.585535\n",
            "resetting env. episode 742.000000, reward total was -20.000000. running mean: -19.589679\n",
            "resetting env. episode 743.000000, reward total was -19.000000. running mean: -19.583783\n",
            "resetting env. episode 744.000000, reward total was -17.000000. running mean: -19.557945\n",
            "resetting env. episode 745.000000, reward total was -17.000000. running mean: -19.532365\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -19.537042\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -19.551671\n",
            "resetting env. episode 748.000000, reward total was -19.000000. running mean: -19.546155\n",
            "resetting env. episode 749.000000, reward total was -20.000000. running mean: -19.550693\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -19.565186\n",
            "resetting env. episode 751.000000, reward total was -19.000000. running mean: -19.559534\n",
            "resetting env. episode 752.000000, reward total was -18.000000. running mean: -19.543939\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -19.558500\n",
            "resetting env. episode 754.000000, reward total was -20.000000. running mean: -19.562915\n",
            "resetting env. episode 755.000000, reward total was -19.000000. running mean: -19.557285\n",
            "resetting env. episode 756.000000, reward total was -19.000000. running mean: -19.551713\n",
            "resetting env. episode 757.000000, reward total was -19.000000. running mean: -19.546195\n",
            "resetting env. episode 758.000000, reward total was -20.000000. running mean: -19.550733\n",
            "resetting env. episode 759.000000, reward total was -20.000000. running mean: -19.555226\n",
            "resetting env. episode 760.000000, reward total was -19.000000. running mean: -19.549674\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -19.564177\n",
            "resetting env. episode 762.000000, reward total was -20.000000. running mean: -19.568535\n",
            "resetting env. episode 763.000000, reward total was -21.000000. running mean: -19.582850\n",
            "resetting env. episode 764.000000, reward total was -19.000000. running mean: -19.577022\n",
            "resetting env. episode 765.000000, reward total was -16.000000. running mean: -19.541251\n",
            "resetting env. episode 766.000000, reward total was -20.000000. running mean: -19.545839\n",
            "resetting env. episode 767.000000, reward total was -18.000000. running mean: -19.530380\n",
            "resetting env. episode 768.000000, reward total was -20.000000. running mean: -19.535077\n",
            "resetting env. episode 769.000000, reward total was -19.000000. running mean: -19.529726\n",
            "resetting env. episode 770.000000, reward total was -18.000000. running mean: -19.514429\n",
            "resetting env. episode 771.000000, reward total was -19.000000. running mean: -19.509284\n",
            "resetting env. episode 772.000000, reward total was -20.000000. running mean: -19.514191\n",
            "resetting env. episode 773.000000, reward total was -18.000000. running mean: -19.499050\n",
            "resetting env. episode 774.000000, reward total was -20.000000. running mean: -19.504059\n",
            "resetting env. episode 775.000000, reward total was -20.000000. running mean: -19.509018\n",
            "resetting env. episode 776.000000, reward total was -19.000000. running mean: -19.503928\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -19.518889\n",
            "resetting env. episode 778.000000, reward total was -19.000000. running mean: -19.513700\n",
            "resetting env. episode 779.000000, reward total was -19.000000. running mean: -19.508563\n",
            "resetting env. episode 780.000000, reward total was -19.000000. running mean: -19.503477\n",
            "resetting env. episode 781.000000, reward total was -19.000000. running mean: -19.498443\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -19.513458\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -19.528324\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -19.543040\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -19.557610\n",
            "resetting env. episode 786.000000, reward total was -20.000000. running mean: -19.562034\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -19.576414\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -19.590649\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -19.604743\n",
            "resetting env. episode 790.000000, reward total was -20.000000. running mean: -19.608696\n",
            "resetting env. episode 791.000000, reward total was -14.000000. running mean: -19.552609\n",
            "resetting env. episode 792.000000, reward total was -20.000000. running mean: -19.557083\n",
            "resetting env. episode 793.000000, reward total was -19.000000. running mean: -19.551512\n",
            "resetting env. episode 794.000000, reward total was -18.000000. running mean: -19.535997\n",
            "resetting env. episode 795.000000, reward total was -17.000000. running mean: -19.510637\n",
            "resetting env. episode 796.000000, reward total was -18.000000. running mean: -19.495530\n",
            "resetting env. episode 797.000000, reward total was -20.000000. running mean: -19.500575\n",
            "resetting env. episode 798.000000, reward total was -20.000000. running mean: -19.505569\n",
            "resetting env. episode 799.000000, reward total was -19.000000. running mean: -19.500513\n",
            "resetting env. episode 800.000000, reward total was -19.000000. running mean: -19.495508\n",
            "resetting env. episode 801.000000, reward total was -18.000000. running mean: -19.480553\n",
            "resetting env. episode 802.000000, reward total was -18.000000. running mean: -19.465748\n",
            "resetting env. episode 803.000000, reward total was -19.000000. running mean: -19.461090\n",
            "resetting env. episode 804.000000, reward total was -17.000000. running mean: -19.436479\n",
            "resetting env. episode 805.000000, reward total was -19.000000. running mean: -19.432115\n",
            "resetting env. episode 806.000000, reward total was -17.000000. running mean: -19.407793\n",
            "resetting env. episode 807.000000, reward total was -18.000000. running mean: -19.393715\n",
            "resetting env. episode 808.000000, reward total was -19.000000. running mean: -19.389778\n",
            "resetting env. episode 809.000000, reward total was -19.000000. running mean: -19.385881\n",
            "resetting env. episode 810.000000, reward total was -20.000000. running mean: -19.392022\n",
            "resetting env. episode 811.000000, reward total was -21.000000. running mean: -19.408102\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -19.424021\n",
            "resetting env. episode 813.000000, reward total was -18.000000. running mean: -19.409780\n",
            "resetting env. episode 814.000000, reward total was -19.000000. running mean: -19.405682\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -19.421626\n",
            "resetting env. episode 816.000000, reward total was -20.000000. running mean: -19.427409\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -19.443135\n",
            "resetting env. episode 818.000000, reward total was -20.000000. running mean: -19.448704\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -19.464217\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -19.469575\n",
            "resetting env. episode 821.000000, reward total was -19.000000. running mean: -19.464879\n",
            "resetting env. episode 822.000000, reward total was -20.000000. running mean: -19.470230\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -19.485528\n",
            "resetting env. episode 824.000000, reward total was -17.000000. running mean: -19.460673\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -19.476066\n",
            "resetting env. episode 826.000000, reward total was -19.000000. running mean: -19.471305\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -19.476592\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -19.491826\n",
            "resetting env. episode 829.000000, reward total was -20.000000. running mean: -19.496908\n",
            "resetting env. episode 830.000000, reward total was -19.000000. running mean: -19.491939\n",
            "resetting env. episode 831.000000, reward total was -18.000000. running mean: -19.477020\n",
            "resetting env. episode 832.000000, reward total was -20.000000. running mean: -19.482249\n",
            "resetting env. episode 833.000000, reward total was -20.000000. running mean: -19.487427\n",
            "resetting env. episode 834.000000, reward total was -15.000000. running mean: -19.442553\n",
            "resetting env. episode 835.000000, reward total was -20.000000. running mean: -19.448127\n",
            "resetting env. episode 836.000000, reward total was -19.000000. running mean: -19.443646\n",
            "resetting env. episode 837.000000, reward total was -19.000000. running mean: -19.439209\n",
            "resetting env. episode 838.000000, reward total was -17.000000. running mean: -19.414817\n",
            "resetting env. episode 839.000000, reward total was -17.000000. running mean: -19.390669\n",
            "resetting env. episode 840.000000, reward total was -19.000000. running mean: -19.386762\n",
            "resetting env. episode 841.000000, reward total was -20.000000. running mean: -19.392895\n",
            "resetting env. episode 842.000000, reward total was -19.000000. running mean: -19.388966\n",
            "resetting env. episode 843.000000, reward total was -20.000000. running mean: -19.395076\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -19.411125\n",
            "resetting env. episode 845.000000, reward total was -18.000000. running mean: -19.397014\n",
            "resetting env. episode 846.000000, reward total was -19.000000. running mean: -19.393044\n",
            "resetting env. episode 847.000000, reward total was -18.000000. running mean: -19.379114\n",
            "resetting env. episode 848.000000, reward total was -18.000000. running mean: -19.365322\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -19.381669\n",
            "resetting env. episode 850.000000, reward total was -19.000000. running mean: -19.377853\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -19.394074\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -19.410133\n",
            "resetting env. episode 853.000000, reward total was -20.000000. running mean: -19.416032\n",
            "resetting env. episode 854.000000, reward total was -18.000000. running mean: -19.401872\n",
            "resetting env. episode 855.000000, reward total was -19.000000. running mean: -19.397853\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -19.413874\n",
            "resetting env. episode 857.000000, reward total was -18.000000. running mean: -19.399736\n",
            "resetting env. episode 858.000000, reward total was -19.000000. running mean: -19.395738\n",
            "resetting env. episode 859.000000, reward total was -17.000000. running mean: -19.371781\n",
            "resetting env. episode 860.000000, reward total was -20.000000. running mean: -19.378063\n",
            "resetting env. episode 861.000000, reward total was -20.000000. running mean: -19.384282\n",
            "resetting env. episode 862.000000, reward total was -19.000000. running mean: -19.380440\n",
            "resetting env. episode 863.000000, reward total was -18.000000. running mean: -19.366635\n",
            "resetting env. episode 864.000000, reward total was -19.000000. running mean: -19.362969\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -19.379339\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -19.395546\n",
            "resetting env. episode 867.000000, reward total was -20.000000. running mean: -19.401590\n",
            "resetting env. episode 868.000000, reward total was -18.000000. running mean: -19.387574\n",
            "resetting env. episode 869.000000, reward total was -19.000000. running mean: -19.383699\n",
            "resetting env. episode 870.000000, reward total was -20.000000. running mean: -19.389862\n",
            "resetting env. episode 871.000000, reward total was -20.000000. running mean: -19.395963\n",
            "resetting env. episode 872.000000, reward total was -19.000000. running mean: -19.392003\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -19.408083\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -19.424003\n",
            "resetting env. episode 875.000000, reward total was -19.000000. running mean: -19.419763\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -19.435565\n",
            "resetting env. episode 877.000000, reward total was -16.000000. running mean: -19.401209\n",
            "resetting env. episode 878.000000, reward total was -18.000000. running mean: -19.387197\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -19.403325\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -19.419292\n",
            "resetting env. episode 881.000000, reward total was -19.000000. running mean: -19.415099\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -19.420948\n",
            "resetting env. episode 883.000000, reward total was -18.000000. running mean: -19.406739\n",
            "resetting env. episode 884.000000, reward total was -20.000000. running mean: -19.412671\n",
            "resetting env. episode 885.000000, reward total was -18.000000. running mean: -19.398544\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -19.414559\n",
            "resetting env. episode 887.000000, reward total was -17.000000. running mean: -19.390413\n",
            "resetting env. episode 888.000000, reward total was -19.000000. running mean: -19.386509\n",
            "resetting env. episode 889.000000, reward total was -19.000000. running mean: -19.382644\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -19.398818\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -19.414830\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -19.430681\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -19.436374\n",
            "resetting env. episode 894.000000, reward total was -19.000000. running mean: -19.432011\n",
            "resetting env. episode 895.000000, reward total was -21.000000. running mean: -19.447691\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -19.463214\n",
            "resetting env. episode 897.000000, reward total was -18.000000. running mean: -19.448582\n",
            "resetting env. episode 898.000000, reward total was -19.000000. running mean: -19.444096\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -19.459655\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -19.475058\n",
            "resetting env. episode 901.000000, reward total was -19.000000. running mean: -19.470308\n",
            "resetting env. episode 902.000000, reward total was -18.000000. running mean: -19.455605\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -19.471049\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -19.486338\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -19.501475\n",
            "resetting env. episode 906.000000, reward total was -17.000000. running mean: -19.476460\n",
            "resetting env. episode 907.000000, reward total was -17.000000. running mean: -19.451695\n",
            "resetting env. episode 908.000000, reward total was -19.000000. running mean: -19.447178\n",
            "resetting env. episode 909.000000, reward total was -21.000000. running mean: -19.462707\n",
            "resetting env. episode 910.000000, reward total was -18.000000. running mean: -19.448080\n",
            "resetting env. episode 911.000000, reward total was -21.000000. running mean: -19.463599\n",
            "resetting env. episode 912.000000, reward total was -18.000000. running mean: -19.448963\n",
            "resetting env. episode 913.000000, reward total was -20.000000. running mean: -19.454473\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -19.469928\n",
            "resetting env. episode 915.000000, reward total was -19.000000. running mean: -19.465229\n",
            "resetting env. episode 916.000000, reward total was -19.000000. running mean: -19.460577\n",
            "resetting env. episode 917.000000, reward total was -18.000000. running mean: -19.445971\n",
            "resetting env. episode 918.000000, reward total was -19.000000. running mean: -19.441511\n",
            "resetting env. episode 919.000000, reward total was -20.000000. running mean: -19.447096\n",
            "resetting env. episode 920.000000, reward total was -19.000000. running mean: -19.442625\n",
            "resetting env. episode 921.000000, reward total was -18.000000. running mean: -19.428199\n",
            "resetting env. episode 922.000000, reward total was -20.000000. running mean: -19.433917\n",
            "resetting env. episode 923.000000, reward total was -15.000000. running mean: -19.389578\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -19.405682\n",
            "resetting env. episode 925.000000, reward total was -19.000000. running mean: -19.401625\n",
            "resetting env. episode 926.000000, reward total was -19.000000. running mean: -19.397609\n",
            "resetting env. episode 927.000000, reward total was -20.000000. running mean: -19.403633\n",
            "resetting env. episode 928.000000, reward total was -20.000000. running mean: -19.409597\n",
            "resetting env. episode 929.000000, reward total was -19.000000. running mean: -19.405501\n",
            "resetting env. episode 930.000000, reward total was -20.000000. running mean: -19.411446\n",
            "resetting env. episode 931.000000, reward total was -20.000000. running mean: -19.417331\n",
            "resetting env. episode 932.000000, reward total was -19.000000. running mean: -19.413158\n",
            "resetting env. episode 933.000000, reward total was -20.000000. running mean: -19.419026\n",
            "resetting env. episode 934.000000, reward total was -19.000000. running mean: -19.414836\n",
            "resetting env. episode 935.000000, reward total was -17.000000. running mean: -19.390688\n",
            "resetting env. episode 936.000000, reward total was -19.000000. running mean: -19.386781\n",
            "resetting env. episode 937.000000, reward total was -17.000000. running mean: -19.362913\n",
            "resetting env. episode 938.000000, reward total was -19.000000. running mean: -19.359284\n",
            "resetting env. episode 939.000000, reward total was -18.000000. running mean: -19.345691\n",
            "resetting env. episode 940.000000, reward total was -17.000000. running mean: -19.322234\n",
            "resetting env. episode 941.000000, reward total was -18.000000. running mean: -19.309012\n",
            "resetting env. episode 942.000000, reward total was -21.000000. running mean: -19.325922\n",
            "resetting env. episode 943.000000, reward total was -20.000000. running mean: -19.332662\n",
            "resetting env. episode 944.000000, reward total was -17.000000. running mean: -19.309336\n",
            "resetting env. episode 945.000000, reward total was -19.000000. running mean: -19.306242\n",
            "resetting env. episode 946.000000, reward total was -20.000000. running mean: -19.313180\n",
            "resetting env. episode 947.000000, reward total was -19.000000. running mean: -19.310048\n",
            "resetting env. episode 948.000000, reward total was -19.000000. running mean: -19.306948\n",
            "resetting env. episode 949.000000, reward total was -17.000000. running mean: -19.283878\n",
            "resetting env. episode 950.000000, reward total was -19.000000. running mean: -19.281039\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -19.298229\n",
            "resetting env. episode 952.000000, reward total was -19.000000. running mean: -19.295247\n",
            "resetting env. episode 953.000000, reward total was -20.000000. running mean: -19.302294\n",
            "resetting env. episode 954.000000, reward total was -15.000000. running mean: -19.259271\n",
            "resetting env. episode 955.000000, reward total was -20.000000. running mean: -19.266679\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -19.284012\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -19.301172\n",
            "resetting env. episode 958.000000, reward total was -19.000000. running mean: -19.298160\n",
            "resetting env. episode 959.000000, reward total was -15.000000. running mean: -19.255178\n",
            "resetting env. episode 960.000000, reward total was -20.000000. running mean: -19.262627\n",
            "resetting env. episode 961.000000, reward total was -21.000000. running mean: -19.280000\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -19.297200\n",
            "resetting env. episode 963.000000, reward total was -19.000000. running mean: -19.294228\n",
            "resetting env. episode 964.000000, reward total was -19.000000. running mean: -19.291286\n",
            "resetting env. episode 965.000000, reward total was -20.000000. running mean: -19.298373\n",
            "resetting env. episode 966.000000, reward total was -19.000000. running mean: -19.295389\n",
            "resetting env. episode 967.000000, reward total was -19.000000. running mean: -19.292436\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -19.309511\n",
            "resetting env. episode 969.000000, reward total was -19.000000. running mean: -19.306416\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -19.323352\n",
            "resetting env. episode 971.000000, reward total was -20.000000. running mean: -19.330118\n",
            "resetting env. episode 972.000000, reward total was -20.000000. running mean: -19.336817\n",
            "resetting env. episode 973.000000, reward total was -20.000000. running mean: -19.343449\n",
            "resetting env. episode 974.000000, reward total was -19.000000. running mean: -19.340015\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -19.356614\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -19.373048\n",
            "resetting env. episode 977.000000, reward total was -20.000000. running mean: -19.379318\n",
            "resetting env. episode 978.000000, reward total was -19.000000. running mean: -19.375525\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -19.391769\n",
            "resetting env. episode 980.000000, reward total was -17.000000. running mean: -19.367852\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -19.384173\n",
            "resetting env. episode 982.000000, reward total was -20.000000. running mean: -19.390331\n",
            "resetting env. episode 983.000000, reward total was -18.000000. running mean: -19.376428\n",
            "resetting env. episode 984.000000, reward total was -20.000000. running mean: -19.382664\n",
            "resetting env. episode 985.000000, reward total was -17.000000. running mean: -19.358837\n",
            "resetting env. episode 986.000000, reward total was -17.000000. running mean: -19.335249\n",
            "resetting env. episode 987.000000, reward total was -15.000000. running mean: -19.291896\n",
            "resetting env. episode 988.000000, reward total was -19.000000. running mean: -19.288977\n",
            "resetting env. episode 989.000000, reward total was -17.000000. running mean: -19.266088\n",
            "resetting env. episode 990.000000, reward total was -19.000000. running mean: -19.263427\n",
            "resetting env. episode 991.000000, reward total was -21.000000. running mean: -19.280792\n",
            "resetting env. episode 992.000000, reward total was -19.000000. running mean: -19.277985\n",
            "resetting env. episode 993.000000, reward total was -19.000000. running mean: -19.275205\n",
            "resetting env. episode 994.000000, reward total was -19.000000. running mean: -19.272453\n",
            "resetting env. episode 995.000000, reward total was -18.000000. running mean: -19.259728\n",
            "resetting env. episode 996.000000, reward total was -20.000000. running mean: -19.267131\n",
            "resetting env. episode 997.000000, reward total was -17.000000. running mean: -19.244460\n",
            "resetting env. episode 998.000000, reward total was -19.000000. running mean: -19.242015\n",
            "resetting env. episode 999.000000, reward total was -17.000000. running mean: -19.219595\n",
            "resetting env. episode 1000.000000, reward total was -19.000000. running mean: -19.217399\n",
            "resetting env. episode 1001.000000, reward total was -19.000000. running mean: -19.215225\n",
            "resetting env. episode 1002.000000, reward total was -19.000000. running mean: -19.213073\n",
            "resetting env. episode 1003.000000, reward total was -20.000000. running mean: -19.220942\n",
            "resetting env. episode 1004.000000, reward total was -17.000000. running mean: -19.198732\n",
            "resetting env. episode 1005.000000, reward total was -20.000000. running mean: -19.206745\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -19.224678\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -19.242431\n",
            "resetting env. episode 1008.000000, reward total was -16.000000. running mean: -19.210007\n",
            "resetting env. episode 1009.000000, reward total was -19.000000. running mean: -19.207907\n",
            "resetting env. episode 1010.000000, reward total was -17.000000. running mean: -19.185827\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -19.203969\n",
            "resetting env. episode 1012.000000, reward total was -20.000000. running mean: -19.211930\n",
            "resetting env. episode 1013.000000, reward total was -17.000000. running mean: -19.189810\n",
            "resetting env. episode 1014.000000, reward total was -19.000000. running mean: -19.187912\n",
            "resetting env. episode 1015.000000, reward total was -20.000000. running mean: -19.196033\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -19.204073\n",
            "resetting env. episode 1017.000000, reward total was -18.000000. running mean: -19.192032\n",
            "resetting env. episode 1018.000000, reward total was -19.000000. running mean: -19.190112\n",
            "resetting env. episode 1019.000000, reward total was -20.000000. running mean: -19.198210\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -19.216228\n",
            "resetting env. episode 1021.000000, reward total was -21.000000. running mean: -19.234066\n",
            "resetting env. episode 1022.000000, reward total was -18.000000. running mean: -19.221725\n",
            "resetting env. episode 1023.000000, reward total was -20.000000. running mean: -19.229508\n",
            "resetting env. episode 1024.000000, reward total was -20.000000. running mean: -19.237213\n",
            "resetting env. episode 1025.000000, reward total was -19.000000. running mean: -19.234841\n",
            "resetting env. episode 1026.000000, reward total was -16.000000. running mean: -19.202493\n",
            "resetting env. episode 1027.000000, reward total was -20.000000. running mean: -19.210468\n",
            "resetting env. episode 1028.000000, reward total was -19.000000. running mean: -19.208363\n",
            "resetting env. episode 1029.000000, reward total was -20.000000. running mean: -19.216279\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -19.224117\n",
            "resetting env. episode 1031.000000, reward total was -19.000000. running mean: -19.221875\n",
            "resetting env. episode 1032.000000, reward total was -19.000000. running mean: -19.219657\n",
            "resetting env. episode 1033.000000, reward total was -21.000000. running mean: -19.237460\n",
            "resetting env. episode 1034.000000, reward total was -17.000000. running mean: -19.215085\n",
            "resetting env. episode 1035.000000, reward total was -20.000000. running mean: -19.222935\n",
            "resetting env. episode 1036.000000, reward total was -17.000000. running mean: -19.200705\n",
            "resetting env. episode 1037.000000, reward total was -20.000000. running mean: -19.208698\n",
            "resetting env. episode 1038.000000, reward total was -21.000000. running mean: -19.226611\n",
            "resetting env. episode 1039.000000, reward total was -18.000000. running mean: -19.214345\n",
            "resetting env. episode 1040.000000, reward total was -20.000000. running mean: -19.222202\n",
            "resetting env. episode 1041.000000, reward total was -21.000000. running mean: -19.239980\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -19.257580\n",
            "resetting env. episode 1043.000000, reward total was -20.000000. running mean: -19.265004\n",
            "resetting env. episode 1044.000000, reward total was -18.000000. running mean: -19.252354\n",
            "resetting env. episode 1045.000000, reward total was -19.000000. running mean: -19.249830\n",
            "resetting env. episode 1046.000000, reward total was -19.000000. running mean: -19.247332\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -19.264859\n",
            "resetting env. episode 1048.000000, reward total was -19.000000. running mean: -19.262210\n",
            "resetting env. episode 1049.000000, reward total was -17.000000. running mean: -19.239588\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -19.257192\n",
            "resetting env. episode 1051.000000, reward total was -19.000000. running mean: -19.254620\n",
            "resetting env. episode 1052.000000, reward total was -15.000000. running mean: -19.212074\n",
            "resetting env. episode 1053.000000, reward total was -18.000000. running mean: -19.199953\n",
            "resetting env. episode 1054.000000, reward total was -19.000000. running mean: -19.197954\n",
            "resetting env. episode 1055.000000, reward total was -19.000000. running mean: -19.195974\n",
            "resetting env. episode 1056.000000, reward total was -19.000000. running mean: -19.194015\n",
            "resetting env. episode 1057.000000, reward total was -20.000000. running mean: -19.202074\n",
            "resetting env. episode 1058.000000, reward total was -19.000000. running mean: -19.200054\n",
            "resetting env. episode 1059.000000, reward total was -20.000000. running mean: -19.208053\n",
            "resetting env. episode 1060.000000, reward total was -20.000000. running mean: -19.215973\n",
            "resetting env. episode 1061.000000, reward total was -19.000000. running mean: -19.213813\n",
            "resetting env. episode 1062.000000, reward total was -18.000000. running mean: -19.201675\n",
            "resetting env. episode 1063.000000, reward total was -21.000000. running mean: -19.219658\n",
            "resetting env. episode 1064.000000, reward total was -19.000000. running mean: -19.217461\n",
            "resetting env. episode 1065.000000, reward total was -15.000000. running mean: -19.175287\n",
            "resetting env. episode 1066.000000, reward total was -16.000000. running mean: -19.143534\n",
            "resetting env. episode 1067.000000, reward total was -18.000000. running mean: -19.132099\n",
            "resetting env. episode 1068.000000, reward total was -19.000000. running mean: -19.130778\n",
            "resetting env. episode 1069.000000, reward total was -19.000000. running mean: -19.129470\n",
            "resetting env. episode 1070.000000, reward total was -19.000000. running mean: -19.128175\n",
            "resetting env. episode 1071.000000, reward total was -19.000000. running mean: -19.126893\n",
            "resetting env. episode 1072.000000, reward total was -20.000000. running mean: -19.135624\n",
            "resetting env. episode 1073.000000, reward total was -19.000000. running mean: -19.134268\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -19.152926\n",
            "resetting env. episode 1075.000000, reward total was -19.000000. running mean: -19.151396\n",
            "resetting env. episode 1076.000000, reward total was -20.000000. running mean: -19.159882\n",
            "resetting env. episode 1077.000000, reward total was -17.000000. running mean: -19.138284\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -19.156901\n",
            "resetting env. episode 1079.000000, reward total was -15.000000. running mean: -19.115332\n",
            "resetting env. episode 1080.000000, reward total was -19.000000. running mean: -19.114178\n",
            "resetting env. episode 1081.000000, reward total was -19.000000. running mean: -19.113037\n",
            "resetting env. episode 1082.000000, reward total was -20.000000. running mean: -19.121906\n",
            "resetting env. episode 1083.000000, reward total was -19.000000. running mean: -19.120687\n",
            "resetting env. episode 1084.000000, reward total was -20.000000. running mean: -19.129480\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -19.148185\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -19.166704\n",
            "resetting env. episode 1087.000000, reward total was -20.000000. running mean: -19.175037\n",
            "resetting env. episode 1088.000000, reward total was -19.000000. running mean: -19.173286\n",
            "resetting env. episode 1089.000000, reward total was -19.000000. running mean: -19.171553\n",
            "resetting env. episode 1090.000000, reward total was -19.000000. running mean: -19.169838\n",
            "resetting env. episode 1091.000000, reward total was -20.000000. running mean: -19.178139\n",
            "resetting env. episode 1092.000000, reward total was -18.000000. running mean: -19.166358\n",
            "resetting env. episode 1093.000000, reward total was -20.000000. running mean: -19.174694\n",
            "resetting env. episode 1094.000000, reward total was -20.000000. running mean: -19.182948\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -19.201118\n",
            "resetting env. episode 1096.000000, reward total was -19.000000. running mean: -19.199107\n",
            "resetting env. episode 1097.000000, reward total was -15.000000. running mean: -19.157116\n",
            "resetting env. episode 1098.000000, reward total was -19.000000. running mean: -19.155545\n",
            "resetting env. episode 1099.000000, reward total was -19.000000. running mean: -19.153989\n",
            "resetting env. episode 1100.000000, reward total was -19.000000. running mean: -19.152449\n",
            "resetting env. episode 1101.000000, reward total was -18.000000. running mean: -19.140925\n",
            "resetting env. episode 1102.000000, reward total was -19.000000. running mean: -19.139516\n",
            "resetting env. episode 1103.000000, reward total was -20.000000. running mean: -19.148120\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -19.166639\n",
            "resetting env. episode 1105.000000, reward total was -19.000000. running mean: -19.164973\n",
            "resetting env. episode 1106.000000, reward total was -16.000000. running mean: -19.133323\n",
            "resetting env. episode 1107.000000, reward total was -18.000000. running mean: -19.121990\n",
            "resetting env. episode 1108.000000, reward total was -19.000000. running mean: -19.120770\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -19.139562\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -19.158167\n",
            "resetting env. episode 1111.000000, reward total was -19.000000. running mean: -19.156585\n",
            "resetting env. episode 1112.000000, reward total was -18.000000. running mean: -19.145019\n",
            "resetting env. episode 1113.000000, reward total was -18.000000. running mean: -19.133569\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -19.142233\n",
            "resetting env. episode 1115.000000, reward total was -20.000000. running mean: -19.150811\n",
            "resetting env. episode 1116.000000, reward total was -18.000000. running mean: -19.139303\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -19.157910\n",
            "resetting env. episode 1118.000000, reward total was -19.000000. running mean: -19.156331\n",
            "resetting env. episode 1119.000000, reward total was -20.000000. running mean: -19.164767\n",
            "resetting env. episode 1120.000000, reward total was -19.000000. running mean: -19.163120\n",
            "resetting env. episode 1121.000000, reward total was -16.000000. running mean: -19.131488\n",
            "resetting env. episode 1122.000000, reward total was -19.000000. running mean: -19.130174\n",
            "resetting env. episode 1123.000000, reward total was -19.000000. running mean: -19.128872\n",
            "resetting env. episode 1124.000000, reward total was -19.000000. running mean: -19.127583\n",
            "resetting env. episode 1125.000000, reward total was -17.000000. running mean: -19.106307\n",
            "resetting env. episode 1126.000000, reward total was -19.000000. running mean: -19.105244\n",
            "resetting env. episode 1127.000000, reward total was -19.000000. running mean: -19.104192\n",
            "resetting env. episode 1128.000000, reward total was -20.000000. running mean: -19.113150\n",
            "resetting env. episode 1129.000000, reward total was -17.000000. running mean: -19.092018\n",
            "resetting env. episode 1130.000000, reward total was -18.000000. running mean: -19.081098\n",
            "resetting env. episode 1131.000000, reward total was -19.000000. running mean: -19.080287\n",
            "resetting env. episode 1132.000000, reward total was -18.000000. running mean: -19.069484\n",
            "resetting env. episode 1133.000000, reward total was -18.000000. running mean: -19.058789\n",
            "resetting env. episode 1134.000000, reward total was -19.000000. running mean: -19.058202\n",
            "resetting env. episode 1135.000000, reward total was -18.000000. running mean: -19.047620\n",
            "resetting env. episode 1136.000000, reward total was -19.000000. running mean: -19.047143\n",
            "resetting env. episode 1137.000000, reward total was -20.000000. running mean: -19.056672\n",
            "resetting env. episode 1138.000000, reward total was -18.000000. running mean: -19.046105\n",
            "resetting env. episode 1139.000000, reward total was -19.000000. running mean: -19.045644\n",
            "resetting env. episode 1140.000000, reward total was -19.000000. running mean: -19.045188\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -19.064736\n",
            "resetting env. episode 1142.000000, reward total was -20.000000. running mean: -19.074089\n",
            "resetting env. episode 1143.000000, reward total was -20.000000. running mean: -19.083348\n",
            "resetting env. episode 1144.000000, reward total was -16.000000. running mean: -19.052514\n",
            "resetting env. episode 1145.000000, reward total was -18.000000. running mean: -19.041989\n",
            "resetting env. episode 1146.000000, reward total was -16.000000. running mean: -19.011569\n",
            "resetting env. episode 1147.000000, reward total was -19.000000. running mean: -19.011453\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -19.021339\n",
            "resetting env. episode 1149.000000, reward total was -20.000000. running mean: -19.031126\n",
            "resetting env. episode 1150.000000, reward total was -18.000000. running mean: -19.020814\n",
            "resetting env. episode 1151.000000, reward total was -19.000000. running mean: -19.020606\n",
            "resetting env. episode 1152.000000, reward total was -20.000000. running mean: -19.030400\n",
            "resetting env. episode 1153.000000, reward total was -20.000000. running mean: -19.040096\n",
            "resetting env. episode 1154.000000, reward total was -19.000000. running mean: -19.039695\n",
            "resetting env. episode 1155.000000, reward total was -18.000000. running mean: -19.029298\n",
            "resetting env. episode 1156.000000, reward total was -18.000000. running mean: -19.019005\n",
            "resetting env. episode 1157.000000, reward total was -20.000000. running mean: -19.028815\n",
            "resetting env. episode 1158.000000, reward total was -18.000000. running mean: -19.018527\n",
            "resetting env. episode 1159.000000, reward total was -21.000000. running mean: -19.038342\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -19.057958\n",
            "resetting env. episode 1161.000000, reward total was -21.000000. running mean: -19.077379\n",
            "resetting env. episode 1162.000000, reward total was -16.000000. running mean: -19.046605\n",
            "resetting env. episode 1163.000000, reward total was -21.000000. running mean: -19.066139\n",
            "resetting env. episode 1164.000000, reward total was -21.000000. running mean: -19.085477\n",
            "resetting env. episode 1165.000000, reward total was -19.000000. running mean: -19.084623\n",
            "resetting env. episode 1166.000000, reward total was -19.000000. running mean: -19.083776\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -19.102939\n",
            "resetting env. episode 1168.000000, reward total was -19.000000. running mean: -19.101909\n",
            "resetting env. episode 1169.000000, reward total was -19.000000. running mean: -19.100890\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -19.119881\n",
            "resetting env. episode 1171.000000, reward total was -16.000000. running mean: -19.088682\n",
            "resetting env. episode 1172.000000, reward total was -19.000000. running mean: -19.087796\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -19.106918\n",
            "resetting env. episode 1174.000000, reward total was -20.000000. running mean: -19.115849\n",
            "resetting env. episode 1175.000000, reward total was -19.000000. running mean: -19.114690\n",
            "resetting env. episode 1176.000000, reward total was -17.000000. running mean: -19.093543\n",
            "resetting env. episode 1177.000000, reward total was -16.000000. running mean: -19.062608\n",
            "resetting env. episode 1178.000000, reward total was -18.000000. running mean: -19.051982\n",
            "resetting env. episode 1179.000000, reward total was -19.000000. running mean: -19.051462\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -19.070947\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -19.090238\n",
            "resetting env. episode 1182.000000, reward total was -21.000000. running mean: -19.109335\n",
            "resetting env. episode 1183.000000, reward total was -20.000000. running mean: -19.118242\n",
            "resetting env. episode 1184.000000, reward total was -16.000000. running mean: -19.087060\n",
            "resetting env. episode 1185.000000, reward total was -19.000000. running mean: -19.086189\n",
            "resetting env. episode 1186.000000, reward total was -16.000000. running mean: -19.055327\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -19.074774\n",
            "resetting env. episode 1188.000000, reward total was -19.000000. running mean: -19.074026\n",
            "resetting env. episode 1189.000000, reward total was -16.000000. running mean: -19.043286\n",
            "resetting env. episode 1190.000000, reward total was -20.000000. running mean: -19.052853\n",
            "resetting env. episode 1191.000000, reward total was -19.000000. running mean: -19.052324\n",
            "resetting env. episode 1192.000000, reward total was -19.000000. running mean: -19.051801\n",
            "resetting env. episode 1193.000000, reward total was -20.000000. running mean: -19.061283\n",
            "resetting env. episode 1194.000000, reward total was -19.000000. running mean: -19.060670\n",
            "resetting env. episode 1195.000000, reward total was -20.000000. running mean: -19.070064\n",
            "resetting env. episode 1196.000000, reward total was -18.000000. running mean: -19.059363\n",
            "resetting env. episode 1197.000000, reward total was -19.000000. running mean: -19.058769\n",
            "resetting env. episode 1198.000000, reward total was -19.000000. running mean: -19.058182\n",
            "resetting env. episode 1199.000000, reward total was -19.000000. running mean: -19.057600\n",
            "resetting env. episode 1200.000000, reward total was -19.000000. running mean: -19.057024\n",
            "resetting env. episode 1201.000000, reward total was -19.000000. running mean: -19.056454\n",
            "resetting env. episode 1202.000000, reward total was -16.000000. running mean: -19.025889\n",
            "resetting env. episode 1203.000000, reward total was -19.000000. running mean: -19.025630\n",
            "resetting env. episode 1204.000000, reward total was -20.000000. running mean: -19.035374\n",
            "resetting env. episode 1205.000000, reward total was -19.000000. running mean: -19.035020\n",
            "resetting env. episode 1206.000000, reward total was -16.000000. running mean: -19.004670\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -19.004623\n",
            "resetting env. episode 1208.000000, reward total was -17.000000. running mean: -18.984577\n",
            "resetting env. episode 1209.000000, reward total was -20.000000. running mean: -18.994731\n",
            "resetting env. episode 1210.000000, reward total was -20.000000. running mean: -19.004784\n",
            "resetting env. episode 1211.000000, reward total was -19.000000. running mean: -19.004736\n",
            "resetting env. episode 1212.000000, reward total was -19.000000. running mean: -19.004689\n",
            "resetting env. episode 1213.000000, reward total was -19.000000. running mean: -19.004642\n",
            "resetting env. episode 1214.000000, reward total was -19.000000. running mean: -19.004595\n",
            "resetting env. episode 1215.000000, reward total was -18.000000. running mean: -18.994549\n",
            "resetting env. episode 1216.000000, reward total was -19.000000. running mean: -18.994604\n",
            "resetting env. episode 1217.000000, reward total was -17.000000. running mean: -18.974658\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -18.994911\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -19.014962\n",
            "resetting env. episode 1220.000000, reward total was -19.000000. running mean: -19.014813\n",
            "resetting env. episode 1221.000000, reward total was -17.000000. running mean: -18.994665\n",
            "resetting env. episode 1222.000000, reward total was -20.000000. running mean: -19.004718\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -19.024671\n",
            "resetting env. episode 1224.000000, reward total was -19.000000. running mean: -19.024424\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -19.044180\n",
            "resetting env. episode 1226.000000, reward total was -17.000000. running mean: -19.023738\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -19.043501\n",
            "resetting env. episode 1228.000000, reward total was -18.000000. running mean: -19.033066\n",
            "resetting env. episode 1229.000000, reward total was -18.000000. running mean: -19.022735\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -19.042508\n",
            "resetting env. episode 1231.000000, reward total was -16.000000. running mean: -19.012082\n",
            "resetting env. episode 1232.000000, reward total was -19.000000. running mean: -19.011962\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -19.031842\n",
            "resetting env. episode 1234.000000, reward total was -20.000000. running mean: -19.041524\n",
            "resetting env. episode 1235.000000, reward total was -17.000000. running mean: -19.021108\n",
            "resetting env. episode 1236.000000, reward total was -20.000000. running mean: -19.030897\n",
            "resetting env. episode 1237.000000, reward total was -19.000000. running mean: -19.030588\n",
            "resetting env. episode 1238.000000, reward total was -20.000000. running mean: -19.040282\n",
            "resetting env. episode 1239.000000, reward total was -18.000000. running mean: -19.029880\n",
            "resetting env. episode 1240.000000, reward total was -19.000000. running mean: -19.029581\n",
            "resetting env. episode 1241.000000, reward total was -16.000000. running mean: -18.999285\n",
            "resetting env. episode 1242.000000, reward total was -16.000000. running mean: -18.969292\n",
            "resetting env. episode 1243.000000, reward total was -20.000000. running mean: -18.979599\n",
            "resetting env. episode 1244.000000, reward total was -18.000000. running mean: -18.969803\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -18.990105\n",
            "resetting env. episode 1246.000000, reward total was -19.000000. running mean: -18.990204\n",
            "resetting env. episode 1247.000000, reward total was -18.000000. running mean: -18.980302\n",
            "resetting env. episode 1248.000000, reward total was -18.000000. running mean: -18.970499\n",
            "resetting env. episode 1249.000000, reward total was -19.000000. running mean: -18.970794\n",
            "resetting env. episode 1250.000000, reward total was -18.000000. running mean: -18.961086\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -18.971475\n",
            "resetting env. episode 1252.000000, reward total was -19.000000. running mean: -18.971761\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -18.992043\n",
            "resetting env. episode 1254.000000, reward total was -17.000000. running mean: -18.972123\n",
            "resetting env. episode 1255.000000, reward total was -17.000000. running mean: -18.952401\n",
            "resetting env. episode 1256.000000, reward total was -18.000000. running mean: -18.942877\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -18.963449\n",
            "resetting env. episode 1258.000000, reward total was -19.000000. running mean: -18.963814\n",
            "resetting env. episode 1259.000000, reward total was -19.000000. running mean: -18.964176\n",
            "resetting env. episode 1260.000000, reward total was -18.000000. running mean: -18.954534\n",
            "resetting env. episode 1261.000000, reward total was -20.000000. running mean: -18.964989\n",
            "resetting env. episode 1262.000000, reward total was -19.000000. running mean: -18.965339\n",
            "resetting env. episode 1263.000000, reward total was -19.000000. running mean: -18.965686\n",
            "resetting env. episode 1264.000000, reward total was -19.000000. running mean: -18.966029\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -18.986368\n",
            "resetting env. episode 1266.000000, reward total was -19.000000. running mean: -18.986505\n",
            "resetting env. episode 1267.000000, reward total was -20.000000. running mean: -18.996640\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -19.016673\n",
            "resetting env. episode 1269.000000, reward total was -18.000000. running mean: -19.006507\n",
            "resetting env. episode 1270.000000, reward total was -18.000000. running mean: -18.996441\n",
            "resetting env. episode 1271.000000, reward total was -18.000000. running mean: -18.986477\n",
            "resetting env. episode 1272.000000, reward total was -18.000000. running mean: -18.976612\n",
            "resetting env. episode 1273.000000, reward total was -20.000000. running mean: -18.986846\n",
            "resetting env. episode 1274.000000, reward total was -19.000000. running mean: -18.986978\n",
            "resetting env. episode 1275.000000, reward total was -21.000000. running mean: -19.007108\n",
            "resetting env. episode 1276.000000, reward total was -19.000000. running mean: -19.007037\n",
            "resetting env. episode 1277.000000, reward total was -19.000000. running mean: -19.006966\n",
            "resetting env. episode 1278.000000, reward total was -17.000000. running mean: -18.986897\n",
            "resetting env. episode 1279.000000, reward total was -17.000000. running mean: -18.967028\n",
            "resetting env. episode 1280.000000, reward total was -21.000000. running mean: -18.987358\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -19.007484\n",
            "resetting env. episode 1282.000000, reward total was -18.000000. running mean: -18.997409\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -19.017435\n",
            "resetting env. episode 1284.000000, reward total was -20.000000. running mean: -19.027261\n",
            "resetting env. episode 1285.000000, reward total was -19.000000. running mean: -19.026988\n",
            "resetting env. episode 1286.000000, reward total was -18.000000. running mean: -19.016718\n",
            "resetting env. episode 1287.000000, reward total was -18.000000. running mean: -19.006551\n",
            "resetting env. episode 1288.000000, reward total was -19.000000. running mean: -19.006485\n",
            "resetting env. episode 1289.000000, reward total was -15.000000. running mean: -18.966421\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -18.986756\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -19.006889\n",
            "resetting env. episode 1292.000000, reward total was -19.000000. running mean: -19.006820\n",
            "resetting env. episode 1293.000000, reward total was -18.000000. running mean: -18.996752\n",
            "resetting env. episode 1294.000000, reward total was -19.000000. running mean: -18.996784\n",
            "resetting env. episode 1295.000000, reward total was -19.000000. running mean: -18.996816\n",
            "resetting env. episode 1296.000000, reward total was -17.000000. running mean: -18.976848\n",
            "resetting env. episode 1297.000000, reward total was -19.000000. running mean: -18.977080\n",
            "resetting env. episode 1298.000000, reward total was -18.000000. running mean: -18.967309\n",
            "resetting env. episode 1299.000000, reward total was -20.000000. running mean: -18.977636\n",
            "resetting env. episode 1300.000000, reward total was -20.000000. running mean: -18.987860\n",
            "resetting env. episode 1301.000000, reward total was -20.000000. running mean: -18.997981\n",
            "resetting env. episode 1302.000000, reward total was -17.000000. running mean: -18.978001\n",
            "resetting env. episode 1303.000000, reward total was -19.000000. running mean: -18.978221\n",
            "resetting env. episode 1304.000000, reward total was -21.000000. running mean: -18.998439\n",
            "resetting env. episode 1305.000000, reward total was -18.000000. running mean: -18.988455\n",
            "resetting env. episode 1306.000000, reward total was -18.000000. running mean: -18.978570\n",
            "resetting env. episode 1307.000000, reward total was -19.000000. running mean: -18.978784\n",
            "resetting env. episode 1308.000000, reward total was -19.000000. running mean: -18.978996\n",
            "resetting env. episode 1309.000000, reward total was -18.000000. running mean: -18.969206\n",
            "resetting env. episode 1310.000000, reward total was -20.000000. running mean: -18.979514\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -18.999719\n",
            "resetting env. episode 1312.000000, reward total was -17.000000. running mean: -18.979722\n",
            "resetting env. episode 1313.000000, reward total was -19.000000. running mean: -18.979925\n",
            "resetting env. episode 1314.000000, reward total was -15.000000. running mean: -18.940126\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -18.950724\n",
            "resetting env. episode 1316.000000, reward total was -17.000000. running mean: -18.931217\n",
            "resetting env. episode 1317.000000, reward total was -14.000000. running mean: -18.881905\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -18.903086\n",
            "resetting env. episode 1319.000000, reward total was -20.000000. running mean: -18.914055\n",
            "resetting env. episode 1320.000000, reward total was -19.000000. running mean: -18.914914\n",
            "resetting env. episode 1321.000000, reward total was -17.000000. running mean: -18.895765\n",
            "resetting env. episode 1322.000000, reward total was -18.000000. running mean: -18.886808\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -18.907940\n",
            "resetting env. episode 1324.000000, reward total was -19.000000. running mean: -18.908860\n",
            "resetting env. episode 1325.000000, reward total was -18.000000. running mean: -18.899772\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -18.920774\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -18.941566\n",
            "resetting env. episode 1328.000000, reward total was -20.000000. running mean: -18.952150\n",
            "resetting env. episode 1329.000000, reward total was -21.000000. running mean: -18.972629\n",
            "resetting env. episode 1330.000000, reward total was -19.000000. running mean: -18.972903\n",
            "resetting env. episode 1331.000000, reward total was -20.000000. running mean: -18.983174\n",
            "resetting env. episode 1332.000000, reward total was -19.000000. running mean: -18.983342\n",
            "resetting env. episode 1333.000000, reward total was -19.000000. running mean: -18.983508\n",
            "resetting env. episode 1334.000000, reward total was -20.000000. running mean: -18.993673\n",
            "resetting env. episode 1335.000000, reward total was -20.000000. running mean: -19.003737\n",
            "resetting env. episode 1336.000000, reward total was -19.000000. running mean: -19.003699\n",
            "resetting env. episode 1337.000000, reward total was -18.000000. running mean: -18.993662\n",
            "resetting env. episode 1338.000000, reward total was -16.000000. running mean: -18.963726\n",
            "resetting env. episode 1339.000000, reward total was -18.000000. running mean: -18.954088\n",
            "resetting env. episode 1340.000000, reward total was -17.000000. running mean: -18.934548\n",
            "resetting env. episode 1341.000000, reward total was -18.000000. running mean: -18.925202\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -18.935950\n",
            "resetting env. episode 1343.000000, reward total was -16.000000. running mean: -18.906591\n",
            "resetting env. episode 1344.000000, reward total was -21.000000. running mean: -18.927525\n",
            "resetting env. episode 1345.000000, reward total was -19.000000. running mean: -18.928249\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -18.948967\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -18.969477\n",
            "resetting env. episode 1348.000000, reward total was -20.000000. running mean: -18.979782\n",
            "resetting env. episode 1349.000000, reward total was -18.000000. running mean: -18.969985\n",
            "resetting env. episode 1350.000000, reward total was -14.000000. running mean: -18.920285\n",
            "resetting env. episode 1351.000000, reward total was -19.000000. running mean: -18.921082\n",
            "resetting env. episode 1352.000000, reward total was -20.000000. running mean: -18.931871\n",
            "resetting env. episode 1353.000000, reward total was -20.000000. running mean: -18.942552\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -18.963127\n",
            "resetting env. episode 1355.000000, reward total was -20.000000. running mean: -18.973496\n",
            "resetting env. episode 1356.000000, reward total was -17.000000. running mean: -18.953761\n",
            "resetting env. episode 1357.000000, reward total was -18.000000. running mean: -18.944223\n",
            "resetting env. episode 1358.000000, reward total was -17.000000. running mean: -18.924781\n",
            "resetting env. episode 1359.000000, reward total was -19.000000. running mean: -18.925533\n",
            "resetting env. episode 1360.000000, reward total was -20.000000. running mean: -18.936278\n",
            "resetting env. episode 1361.000000, reward total was -21.000000. running mean: -18.956915\n",
            "resetting env. episode 1362.000000, reward total was -19.000000. running mean: -18.957346\n",
            "resetting env. episode 1363.000000, reward total was -17.000000. running mean: -18.937772\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -18.958395\n",
            "resetting env. episode 1365.000000, reward total was -15.000000. running mean: -18.918811\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -18.939623\n",
            "resetting env. episode 1367.000000, reward total was -16.000000. running mean: -18.910226\n",
            "resetting env. episode 1368.000000, reward total was -20.000000. running mean: -18.921124\n",
            "resetting env. episode 1369.000000, reward total was -19.000000. running mean: -18.921913\n",
            "resetting env. episode 1370.000000, reward total was -17.000000. running mean: -18.902694\n",
            "resetting env. episode 1371.000000, reward total was -20.000000. running mean: -18.913667\n",
            "resetting env. episode 1372.000000, reward total was -17.000000. running mean: -18.894530\n",
            "resetting env. episode 1373.000000, reward total was -17.000000. running mean: -18.875585\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -18.896829\n",
            "resetting env. episode 1375.000000, reward total was -19.000000. running mean: -18.897861\n",
            "resetting env. episode 1376.000000, reward total was -18.000000. running mean: -18.888882\n",
            "resetting env. episode 1377.000000, reward total was -20.000000. running mean: -18.899993\n",
            "resetting env. episode 1378.000000, reward total was -18.000000. running mean: -18.890993\n",
            "resetting env. episode 1379.000000, reward total was -19.000000. running mean: -18.892083\n",
            "resetting env. episode 1380.000000, reward total was -20.000000. running mean: -18.903163\n",
            "resetting env. episode 1381.000000, reward total was -15.000000. running mean: -18.864131\n",
            "resetting env. episode 1382.000000, reward total was -19.000000. running mean: -18.865490\n",
            "resetting env. episode 1383.000000, reward total was -19.000000. running mean: -18.866835\n",
            "resetting env. episode 1384.000000, reward total was -19.000000. running mean: -18.868166\n",
            "resetting env. episode 1385.000000, reward total was -19.000000. running mean: -18.869485\n",
            "resetting env. episode 1386.000000, reward total was -18.000000. running mean: -18.860790\n",
            "resetting env. episode 1387.000000, reward total was -20.000000. running mean: -18.872182\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -18.893460\n",
            "resetting env. episode 1389.000000, reward total was -16.000000. running mean: -18.864526\n",
            "resetting env. episode 1390.000000, reward total was -20.000000. running mean: -18.875880\n",
            "resetting env. episode 1391.000000, reward total was -19.000000. running mean: -18.877121\n",
            "resetting env. episode 1392.000000, reward total was -18.000000. running mean: -18.868350\n",
            "resetting env. episode 1393.000000, reward total was -21.000000. running mean: -18.889667\n",
            "resetting env. episode 1394.000000, reward total was -18.000000. running mean: -18.880770\n",
            "resetting env. episode 1395.000000, reward total was -19.000000. running mean: -18.881962\n",
            "resetting env. episode 1396.000000, reward total was -19.000000. running mean: -18.883143\n",
            "resetting env. episode 1397.000000, reward total was -20.000000. running mean: -18.894311\n",
            "resetting env. episode 1398.000000, reward total was -20.000000. running mean: -18.905368\n",
            "resetting env. episode 1399.000000, reward total was -19.000000. running mean: -18.906315\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -18.927251\n",
            "resetting env. episode 1401.000000, reward total was -17.000000. running mean: -18.907979\n",
            "resetting env. episode 1402.000000, reward total was -18.000000. running mean: -18.898899\n",
            "resetting env. episode 1403.000000, reward total was -17.000000. running mean: -18.879910\n",
            "resetting env. episode 1404.000000, reward total was -19.000000. running mean: -18.881111\n",
            "resetting env. episode 1405.000000, reward total was -18.000000. running mean: -18.872300\n",
            "resetting env. episode 1406.000000, reward total was -20.000000. running mean: -18.883577\n",
            "resetting env. episode 1407.000000, reward total was -19.000000. running mean: -18.884741\n",
            "resetting env. episode 1408.000000, reward total was -17.000000. running mean: -18.865894\n",
            "resetting env. episode 1409.000000, reward total was -19.000000. running mean: -18.867235\n",
            "resetting env. episode 1410.000000, reward total was -17.000000. running mean: -18.848562\n",
            "resetting env. episode 1411.000000, reward total was -19.000000. running mean: -18.850077\n",
            "resetting env. episode 1412.000000, reward total was -17.000000. running mean: -18.831576\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -18.853260\n",
            "resetting env. episode 1414.000000, reward total was -20.000000. running mean: -18.864728\n",
            "resetting env. episode 1415.000000, reward total was -17.000000. running mean: -18.846080\n",
            "resetting env. episode 1416.000000, reward total was -19.000000. running mean: -18.847620\n",
            "resetting env. episode 1417.000000, reward total was -18.000000. running mean: -18.839143\n",
            "resetting env. episode 1418.000000, reward total was -19.000000. running mean: -18.840752\n",
            "resetting env. episode 1419.000000, reward total was -17.000000. running mean: -18.822344\n",
            "resetting env. episode 1420.000000, reward total was -20.000000. running mean: -18.834121\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -18.845780\n",
            "resetting env. episode 1422.000000, reward total was -21.000000. running mean: -18.867322\n",
            "resetting env. episode 1423.000000, reward total was -18.000000. running mean: -18.858649\n",
            "resetting env. episode 1424.000000, reward total was -17.000000. running mean: -18.840062\n",
            "resetting env. episode 1425.000000, reward total was -18.000000. running mean: -18.831662\n",
            "resetting env. episode 1426.000000, reward total was -18.000000. running mean: -18.823345\n",
            "resetting env. episode 1427.000000, reward total was -17.000000. running mean: -18.805112\n",
            "resetting env. episode 1428.000000, reward total was -20.000000. running mean: -18.817060\n",
            "resetting env. episode 1429.000000, reward total was -18.000000. running mean: -18.808890\n",
            "resetting env. episode 1430.000000, reward total was -19.000000. running mean: -18.810801\n",
            "resetting env. episode 1431.000000, reward total was -19.000000. running mean: -18.812693\n",
            "resetting env. episode 1432.000000, reward total was -20.000000. running mean: -18.824566\n",
            "resetting env. episode 1433.000000, reward total was -19.000000. running mean: -18.826320\n",
            "resetting env. episode 1434.000000, reward total was -20.000000. running mean: -18.838057\n",
            "resetting env. episode 1435.000000, reward total was -19.000000. running mean: -18.839677\n",
            "resetting env. episode 1436.000000, reward total was -19.000000. running mean: -18.841280\n",
            "resetting env. episode 1437.000000, reward total was -19.000000. running mean: -18.842867\n",
            "resetting env. episode 1438.000000, reward total was -18.000000. running mean: -18.834438\n",
            "resetting env. episode 1439.000000, reward total was -18.000000. running mean: -18.826094\n",
            "resetting env. episode 1440.000000, reward total was -15.000000. running mean: -18.787833\n",
            "resetting env. episode 1441.000000, reward total was -17.000000. running mean: -18.769955\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -18.792255\n",
            "resetting env. episode 1443.000000, reward total was -18.000000. running mean: -18.784333\n",
            "resetting env. episode 1444.000000, reward total was -19.000000. running mean: -18.786489\n",
            "resetting env. episode 1445.000000, reward total was -19.000000. running mean: -18.788624\n",
            "resetting env. episode 1446.000000, reward total was -17.000000. running mean: -18.770738\n",
            "resetting env. episode 1447.000000, reward total was -19.000000. running mean: -18.773031\n",
            "resetting env. episode 1448.000000, reward total was -20.000000. running mean: -18.785300\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -18.797447\n",
            "resetting env. episode 1450.000000, reward total was -19.000000. running mean: -18.799473\n",
            "resetting env. episode 1451.000000, reward total was -18.000000. running mean: -18.791478\n",
            "resetting env. episode 1452.000000, reward total was -19.000000. running mean: -18.793563\n",
            "resetting env. episode 1453.000000, reward total was -20.000000. running mean: -18.805628\n",
            "resetting env. episode 1454.000000, reward total was -15.000000. running mean: -18.767572\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -18.789896\n",
            "resetting env. episode 1456.000000, reward total was -19.000000. running mean: -18.791997\n",
            "resetting env. episode 1457.000000, reward total was -19.000000. running mean: -18.794077\n",
            "resetting env. episode 1458.000000, reward total was -19.000000. running mean: -18.796136\n",
            "resetting env. episode 1459.000000, reward total was -20.000000. running mean: -18.808175\n",
            "resetting env. episode 1460.000000, reward total was -18.000000. running mean: -18.800093\n",
            "resetting env. episode 1461.000000, reward total was -17.000000. running mean: -18.782092\n",
            "resetting env. episode 1462.000000, reward total was -19.000000. running mean: -18.784271\n",
            "resetting env. episode 1463.000000, reward total was -19.000000. running mean: -18.786428\n",
            "resetting env. episode 1464.000000, reward total was -17.000000. running mean: -18.768564\n",
            "resetting env. episode 1465.000000, reward total was -19.000000. running mean: -18.770879\n",
            "resetting env. episode 1466.000000, reward total was -18.000000. running mean: -18.763170\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -18.785538\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -18.807683\n",
            "resetting env. episode 1469.000000, reward total was -17.000000. running mean: -18.789606\n",
            "resetting env. episode 1470.000000, reward total was -19.000000. running mean: -18.791710\n",
            "resetting env. episode 1471.000000, reward total was -18.000000. running mean: -18.783793\n",
            "resetting env. episode 1472.000000, reward total was -15.000000. running mean: -18.745955\n",
            "resetting env. episode 1473.000000, reward total was -19.000000. running mean: -18.748495\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -18.771010\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -18.793300\n",
            "resetting env. episode 1476.000000, reward total was -19.000000. running mean: -18.795367\n",
            "resetting env. episode 1477.000000, reward total was -19.000000. running mean: -18.797413\n",
            "resetting env. episode 1478.000000, reward total was -20.000000. running mean: -18.809439\n",
            "resetting env. episode 1479.000000, reward total was -16.000000. running mean: -18.781345\n",
            "resetting env. episode 1480.000000, reward total was -18.000000. running mean: -18.773532\n",
            "resetting env. episode 1481.000000, reward total was -16.000000. running mean: -18.745796\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -18.768338\n",
            "resetting env. episode 1483.000000, reward total was -19.000000. running mean: -18.770655\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -18.792948\n",
            "resetting env. episode 1485.000000, reward total was -20.000000. running mean: -18.805019\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -18.826969\n",
            "resetting env. episode 1487.000000, reward total was -19.000000. running mean: -18.828699\n",
            "resetting env. episode 1488.000000, reward total was -21.000000. running mean: -18.850412\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -18.861908\n",
            "resetting env. episode 1490.000000, reward total was -20.000000. running mean: -18.873289\n",
            "resetting env. episode 1491.000000, reward total was -20.000000. running mean: -18.884556\n",
            "resetting env. episode 1492.000000, reward total was -19.000000. running mean: -18.885710\n",
            "resetting env. episode 1493.000000, reward total was -18.000000. running mean: -18.876853\n",
            "resetting env. episode 1494.000000, reward total was -19.000000. running mean: -18.878085\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -18.899304\n",
            "resetting env. episode 1496.000000, reward total was -21.000000. running mean: -18.920311\n",
            "resetting env. episode 1497.000000, reward total was -16.000000. running mean: -18.891108\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -18.912197\n",
            "resetting env. episode 1499.000000, reward total was -20.000000. running mean: -18.923075\n",
            "resetting env. episode 1500.000000, reward total was -16.000000. running mean: -18.893844\n",
            "CPU times: user 9h 31min 53s, sys: 1h 15min 18s, total: 10h 47min 11s\n",
            "Wall time: 5h 35min 40s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "CteN7XKMVGqg",
        "outputId": "aa7e33df-2e08-4719-d414-b18729793952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYA0HgMoO77a"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# pickle.dump(model, open('model.pkl', 'wb'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "pg_from_scratch_(h_1000).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}