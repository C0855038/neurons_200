{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l9hHvTk6ec8"
      },
      "source": [
        "# Policy Gradient\n",
        "\n",
        "* http://karpathy.github.io/2016/05/31/rl/\n",
        "* https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
        "* https://github.com/gameofdimension/policy-gradient-pong\n",
        "* https://www.youtube.com/watch?v=tqrcjHuNdmQ\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqkOdLyN9Ylm"
      },
      "source": [
        "## Step 1: Installation for Colab - just execute these cells and do not worry too much\n",
        "\n",
        "* http://nbviewer.jupyter.org/github/patrickmineault/xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb \n",
        "* https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi\n",
        "* https://nyu-cds.github.io/python-mpi/setup/\n",
        "* https://medium.com/@kaleajit27/reinforcement-learning-on-google-colab-9cb2e1ef51e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF9MAVI16huj",
        "outputId": "5fd70b6a-bccf-461d-f3cf-6fe6cdf09f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install python-opengl -y  >/dev/null\n",
        "!apt install xvfb -y >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fSC11TfN6p69"
      },
      "outputs": [],
      "source": [
        "!pip install pyvirtualdisplay >/dev/null\n",
        "!pip install piglet >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "caiHE2hy6xrf"
      },
      "outputs": [],
      "source": [
        "# from pyvirtualdisplay import Display\n",
        "# display = Display(visible=0, size=(1400, 900))\n",
        "# display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "cd9a7d67-1d42-4426-cd24-d9004e190dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.7.5)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "c2553db3-b060-4200-a2a8-21f7afd04e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "26ab2e01-8b74-4f50-808d-1cbd1477d207"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "35d07881-6248-4603-b31e-9014095ea1a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "f376a9ec-169d-45f3-d06a-3751b51baac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -16.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  env.close()\n",
        "  display_frames_as_gif(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 3 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-5\n",
        "learning_rate = 1e-5\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "bb00eadf-caae-4715-e22e-d63efc01c68d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.980297\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.980494\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.980689\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.980882\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.981073\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.981263\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.981450\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.981636\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.981819\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.982001\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.982181\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.972359\n",
            "resetting env. episode 17.000000, reward total was -19.000000. running mean: -20.952636\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.953109\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.953578\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.944042\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.944602\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.945156\n",
            "resetting env. episode 23.000000, reward total was -19.000000. running mean: -20.925704\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.926447\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.927183\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.927911\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.928632\n",
            "resetting env. episode 28.000000, reward total was -19.000000. running mean: -20.909346\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.910252\n",
            "resetting env. episode 30.000000, reward total was -19.000000. running mean: -20.891150\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.882238\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.873416\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.874682\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.875935\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.877175\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.878404\n",
            "resetting env. episode 37.000000, reward total was -19.000000. running mean: -20.859620\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.861023\n",
            "resetting env. episode 39.000000, reward total was -19.000000. running mean: -20.842413\n",
            "resetting env. episode 40.000000, reward total was -19.000000. running mean: -20.823989\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.825749\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.817492\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.819317\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.821124\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.822912\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.824683\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.816436\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.818272\n",
            "resetting env. episode 49.000000, reward total was -19.000000. running mean: -20.800089\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.802088\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.804068\n",
            "resetting env. episode 52.000000, reward total was -19.000000. running mean: -20.786027\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.788167\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.780285\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -20.762482\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.764857\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.767209\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.769537\n",
            "resetting env. episode 59.000000, reward total was -18.000000. running mean: -20.741841\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.744423\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.736979\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.739609\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.732213\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.734891\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -20.717542\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.720366\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.723163\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.725931\n",
            "resetting env. episode 69.000000, reward total was -18.000000. running mean: -20.698672\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.701685\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.694668\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.697721\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.700744\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.703737\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.696699\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.699732\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.702735\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.695708\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.698751\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.701763\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.704746\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.707698\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.710621\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.713515\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.716380\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.719216\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.712024\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.714904\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.717754\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.710577\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.713471\n",
            "resetting env. episode 92.000000, reward total was -18.000000. running mean: -20.686336\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.689473\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.692578\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.695653\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.698696\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.691709\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -20.674792\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.678044\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.681264\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -20.664451\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.667806\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.671128\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.674417\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.677673\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.680896\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.674087\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.667346\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.660673\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.654066\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -20.637526\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.641150\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.644739\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.648291\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.641808\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.635390\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.639036\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.642646\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.646220\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -20.629757\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.623460\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.627225\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.620953\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.624744\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.618496\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.622311\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.626088\n",
            "resetting env. episode 128.000000, reward total was -18.000000. running mean: -20.599827\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.603829\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.607791\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.611713\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.615596\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.619440\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.613245\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.617113\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.620942\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.624732\n",
            "resetting env. episode 138.000000, reward total was -17.000000. running mean: -20.588485\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.592600\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.596674\n",
            "resetting env. episode 141.000000, reward total was -19.000000. running mean: -20.580707\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.584900\n",
            "resetting env. episode 143.000000, reward total was -19.000000. running mean: -20.569051\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.573361\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.567627\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.561951\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.566331\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.570668\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.574961\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.579212\n",
            "resetting env. episode 151.000000, reward total was -19.000000. running mean: -20.563420\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.567785\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.562108\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.566486\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.570822\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.575113\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.569362\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.573669\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.567932\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.572253\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.576530\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.570765\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.575057\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.569307\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.573614\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.577877\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.582099\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.586278\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.590415\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.594511\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.598566\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.602580\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.606554\n",
            "resetting env. episode 174.000000, reward total was -19.000000. running mean: -20.590489\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.584584\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.588738\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.582850\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.587022\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.591152\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.595240\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.599288\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.593295\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.597362\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -20.581388\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.575575\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.569819\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.564121\n",
            "resetting env. episode 188.000000, reward total was -19.000000. running mean: -20.548479\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.552995\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.557465\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.561890\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.566271\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.570608\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.574902\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.579153\n",
            "resetting env. episode 196.000000, reward total was -19.000000. running mean: -20.563362\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.557728\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.562151\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.556529\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.560964\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.555354\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.549801\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.554303\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.558760\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.553172\n",
            "resetting env. episode 206.000000, reward total was -19.000000. running mean: -20.537640\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.542264\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.546841\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.551373\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.545859\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.550401\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.544897\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.549448\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.543953\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.548514\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.543029\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.537598\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.542222\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.546800\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.541332\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.545919\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.540460\n",
            "resetting env. episode 223.000000, reward total was -18.000000. running mean: -20.515055\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.519904\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.524705\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.519458\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.524264\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.529021\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.533731\n",
            "resetting env. episode 230.000000, reward total was -18.000000. running mean: -20.508394\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.503310\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.498277\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.503294\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.508261\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.513178\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.508046\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.512966\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.517836\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.512658\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.507531\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.502456\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.507432\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.502357\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.507334\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.502260\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.497238\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.502265\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.507243\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.512170\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.507049\n",
            "resetting env. episode 251.000000, reward total was -19.000000. running mean: -20.491978\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.497058\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.502088\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.507067\n",
            "resetting env. episode 255.000000, reward total was -17.000000. running mean: -20.471996\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.477276\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.482503\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.487678\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.492802\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.497874\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.502895\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.497866\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.502887\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.507858\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.512780\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.517652\n",
            "resetting env. episode 267.000000, reward total was -18.000000. running mean: -20.492475\n",
            "resetting env. episode 268.000000, reward total was -19.000000. running mean: -20.477551\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.482775\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.477947\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.483168\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.478336\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.483553\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.488717\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.493830\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.488892\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.474003\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.479263\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.484470\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.479626\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.484829\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.479981\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.475181\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.480429\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.475625\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.470869\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.476160\n",
            "resetting env. episode 288.000000, reward total was -18.000000. running mean: -20.451399\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.446885\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.452416\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.457892\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.453313\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.448780\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.454292\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.449749\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.435251\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.440899\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.446490\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.452025\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.457505\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.452930\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.448400\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.443916\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.449477\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.454982\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.460433\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.455828\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.461270\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.466657\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.461991\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.467371\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.472697\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.477970\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.473190\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.478459\n",
            "resetting env. episode 316.000000, reward total was -19.000000. running mean: -20.463674\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.469037\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.474347\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.469603\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.464907\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.470258\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.475556\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.480800\n",
            "resetting env. episode 324.000000, reward total was -19.000000. running mean: -20.465992\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.461332\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.466719\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.462052\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.467431\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -20.452757\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.448229\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.453747\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.449210\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.454717\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.460170\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.465569\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.460913\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.456304\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.441741\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.447323\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.452850\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.448322\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.443838\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.449400\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.454906\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.450357\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.455853\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.441295\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.446882\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.442413\n",
            "resetting env. episode 350.000000, reward total was -19.000000. running mean: -20.427989\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.433709\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.439372\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.434978\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.440628\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.436222\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.441860\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.447441\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.452967\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.448437\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.453953\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.459413\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.454819\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.460271\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.465668\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.471012\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -20.456302\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.461739\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.467121\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.472450\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.467725\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.473048\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.478318\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.483534\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.488699\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.483812\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.488974\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.494084\n",
            "resetting env. episode 378.000000, reward total was -18.000000. running mean: -20.469143\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.474452\n",
            "resetting env. episode 380.000000, reward total was -19.000000. running mean: -20.459708\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.455110\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.450559\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.456054\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.461493\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.466878\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.472209\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.477487\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.482713\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.487885\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.493007\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.488076\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.483196\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.488364\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.493480\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.478545\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.483760\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.488922\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.484033\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.479193\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.484401\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.489557\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.494661\n",
            "resetting env. episode 403.000000, reward total was -16.000000. running mean: -20.449715\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.445217\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.440765\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.446358\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.441894\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.437475\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.443100\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.448669\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.454183\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.459641\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.455044\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.450494\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.445989\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.451529\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.447014\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.452544\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.458018\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.463438\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.458804\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.464216\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.469574\n",
            "resetting env. episode 424.000000, reward total was -19.000000. running mean: -20.454878\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.460329\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.465726\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -20.451068\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.456558\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.461992\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.467372\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.452699\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.448172\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.453690\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.459153\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.464561\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.469916\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.475217\n",
            "resetting env. episode 438.000000, reward total was -19.000000. running mean: -20.460464\n",
            "resetting env. episode 439.000000, reward total was -19.000000. running mean: -20.445860\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.451401\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.456887\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.462318\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.467695\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.473018\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.468288\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.463605\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.468969\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.474279\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.479537\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.484741\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.489894\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.494995\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.500045\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.495045\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.490094\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.485193\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.490341\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.485438\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.480583\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -20.465778\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.471120\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.476409\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.471645\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.476928\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.482159\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.477337\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.472564\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.467838\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.473160\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.468428\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.473744\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -20.459006\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.444416\n",
            "resetting env. episode 474.000000, reward total was -19.000000. running mean: -20.429972\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.425673\n",
            "resetting env. episode 476.000000, reward total was -19.000000. running mean: -20.411416\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.407302\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.413229\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.419096\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.424905\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.430656\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.436350\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.441986\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.437566\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.443191\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.448759\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.454271\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.459729\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.465131\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.470480\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.465775\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.471117\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.476406\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.481642\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.486826\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.491957\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.487038\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.482168\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.477346\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.482572\n",
            "CPU times: user 23min 46s, sys: 10min 40s, total: 34min 27s\n",
            "Wall time: 17min 49s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "NuSP4FE8QTA-",
        "outputId": "29253889-6541-4b9d-faa7-55128c42f632"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ4AAAFZCAYAAABpOsHqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAH30lEQVR4nO3dv2/cdx3H8c+d7TjOJbYTO0Zx2rqFlg4RExVbF1joysbKxoA68Q8gsTAgwc7SDf6BioWBBRYkBGoH0jaqsMiP+vz7x/nnlwGQml6s+vW9K987+/GQIkUf3feTd6Lkqft+LnfXqqqqACTaTQ8AjB/hAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQm6x74fdfn7nw22rbrVLeXpkuN6YG79S9u3fLzPT0wPt8trFedvb2+9YX5ufK3M1bA++/vbdb1jY2B96H4dtaWSx7924PvM+Np1tl/tGzIUzUnHffX2/Vua52ON55Y6bupQNZXrpb7szNDbxP7+jonHDMl5Xl5YH3X33yVDhG1NarS+XZt18beJ/Fv3069uGoy60KEBMOICYcQEw4gFjtw9GmPOuul90XHGqeZ+H2fOnMDH6Qu7mzU7Z3dvvWb93slNuzswPvT/M6jzdK53H/gfb+1+bK7v07DUw0usYuHP988iR6/LeuvTGUcHQ3Nssnq6t96yvLy8JxScw9+qws//lh3/qTt74uHF/gVgWICQcQEw4gJhxAbOwORxfm58q1qWsXfvww3tcCPG/swvHq/ftDea8KUJ9bFSAmHEBMOICYcACxsTscPc/m9k45PD668ON7h4df4TRwuV2acDxaXS1rmz5xC/4f3KoAMeEAYsIBxIQDiF2aw9FbnU45qy78VS9lb3+/HB4fX/jxM9enX/hf3W9cv37hPRhth3M3yvYrC33rvflOA9OMtksTjtdXXoke/8FHH5d/Pbv4d2IsLy2V5aWldCzGSPfBS6X74KWmxxgLblWAmHAAMeEAYsIBxMbucHTv4KBMtAfv3fE5r6j0Do/K1s7OwPsfHPYG3oOvxvTOwQu/PyXeZ+tgCNOMp1YVvIT5eb965069C6Fhw/yL2xriXk149/31Wr+FsXvGAYMa93/so8AZBxATDiBW+1bl7Z/8ephzAGOk9uFot9t1OApjbmFhodaRj1sVICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBitd9W/9ff/XKYcwAN+N6Pf17rOp85CldY3c8cdasCxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQm2x6ALjqziba5bgz3bfeOj0rU3uHpdXATF9GOKBhu/fmy8MffKdvvfN0q7z52z81MNGXEw5oWqtVqol2Ka3nn1tU7dE9SRjdyYCRJRxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiPuUcRkFV/efHF9dGlHBAwzqPN8uD9/7Yt94+OWtgmosRDmjYxMlpmVnfa3qMiDMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcQmmx7gPIu358vU5FTfendzsxwdHzcwEfA/IxuOb7z8cpm9efO5taqqyl8++FA4qK3Vnvjvz6pSnZ01Oss4G9lwwLBdn1ss3/3pb8rE1HTp7ayXP/ziR+X0qNf0WGNJOLgy2u3JMnvvtTI5PVOubc6WVssRX13+5ICYcAAx4QBiwsEVUpVSqlJVZ6WqqqaHGWsOR7kyetvd8vuf/bC02u1ydnpSTryiUptwcGWcnZ6UjU8/bHqMS8GtChATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYiP7tvr9Xq+02/1dOz09bWAa4PNGNhx//8fDpkcAzuFWBYgJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEJuse+Hdb741zDmAMdKqqqrWhWtra/UuBEbG4uJiq851tZ9xtFq1fj3gEnDGAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gFjt71UBri7POICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiP0b9JUIDmZyTsQAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCZR5OV-z-YJ",
        "outputId": "ddc8f5d5-2a24-4c38-edeb-50809f2a8694"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -19.020000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.039800\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.059402\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -19.078808\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -19.098020\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.117040\n",
            "resetting env. episode 8.000000, reward total was -17.000000. running mean: -19.095869\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -19.094911\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -19.113962\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -19.132822\n",
            "resetting env. episode 12.000000, reward total was -19.000000. running mean: -19.131494\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -19.130179\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -19.148877\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -19.147388\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -19.165914\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -19.184255\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -19.202413\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -19.220388\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -19.238185\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -19.245803\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -19.253345\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.270811\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -19.278103\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.295322\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -19.302369\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.319345\n",
            "resetting env. episode 28.000000, reward total was -18.000000. running mean: -19.306152\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -19.313090\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.329959\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -19.346660\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -19.353193\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.369661\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -19.375965\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -19.392205\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -19.408283\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -19.414200\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.430058\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -19.445758\n",
            "resetting env. episode 40.000000, reward total was -18.000000. running mean: -19.431300\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -19.436987\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -19.442617\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -19.458191\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.473609\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -19.488873\n",
            "resetting env. episode 46.000000, reward total was -19.000000. running mean: -19.483984\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -19.499144\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.514153\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.529011\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.543721\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.558284\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -19.562701\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -19.577074\n",
            "resetting env. episode 54.000000, reward total was -19.000000. running mean: -19.571303\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.585590\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.599734\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -19.613737\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -19.617600\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.631424\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.645110\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.658658\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.672072\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.685351\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -19.678498\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -19.681713\n",
            "resetting env. episode 66.000000, reward total was -19.000000. running mean: -19.674896\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -19.688147\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.701265\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.714252\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.727110\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -19.729839\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.742540\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.755115\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -19.757564\n",
            "resetting env. episode 75.000000, reward total was -19.000000. running mean: -19.749988\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.762488\n",
            "resetting env. episode 77.000000, reward total was -18.000000. running mean: -19.744863\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.757415\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -19.759841\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -19.772242\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -19.774520\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -19.776775\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.789007\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.801117\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.813106\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -19.814975\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.826825\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.838557\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.850171\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -19.851669\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.863153\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -19.864521\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -19.865876\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -19.877217\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.888445\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -19.889561\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -19.890665\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -19.901758\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -19.902741\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.913713\n",
            "resetting env. episode 101.000000, reward total was -18.000000. running mean: -19.894576\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -19.895630\n",
            "resetting env. episode 103.000000, reward total was -18.000000. running mean: -19.876674\n",
            "resetting env. episode 104.000000, reward total was -19.000000. running mean: -19.867907\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.879228\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.890436\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -19.891532\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -19.892616\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.903690\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.914653\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -19.915507\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.926352\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -19.937088\n",
            "resetting env. episode 114.000000, reward total was -19.000000. running mean: -19.927717\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -19.938440\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.949056\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.959565\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.969969\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.970270\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -19.980567\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.990761\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.000854\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.000845\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.000837\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.000828\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.010820\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.020712\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.020505\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -20.010300\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.010197\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.020095\n",
            "resetting env. episode 132.000000, reward total was -19.000000. running mean: -20.009894\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -19.999795\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.009797\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.019699\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.019502\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.029307\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.039014\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.048624\n",
            "resetting env. episode 140.000000, reward total was -19.000000. running mean: -20.038138\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.047756\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.057279\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.056706\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.066139\n",
            "resetting env. episode 145.000000, reward total was -19.000000. running mean: -20.055477\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.054923\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.064373\n",
            "resetting env. episode 148.000000, reward total was -19.000000. running mean: -20.053730\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.063192\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.062560\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.071935\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.081215\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.090403\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.099499\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.108504\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.117419\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.126245\n",
            "resetting env. episode 158.000000, reward total was -17.000000. running mean: -20.094983\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.104033\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.112992\n",
            "resetting env. episode 161.000000, reward total was -19.000000. running mean: -20.101863\n",
            "resetting env. episode 162.000000, reward total was -19.000000. running mean: -20.090844\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.089935\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.099036\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.108046\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.116965\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.115796\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.114638\n",
            "resetting env. episode 169.000000, reward total was -19.000000. running mean: -20.103491\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.112456\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.121332\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.120119\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.118917\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.127728\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.126451\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.115186\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.124035\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.132794\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.131466\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.140152\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.148750\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.157263\n",
            "resetting env. episode 183.000000, reward total was -19.000000. running mean: -20.145690\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.154233\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.162691\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.171064\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.159353\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.167760\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.166082\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.164421\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.172777\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.181049\n",
            "resetting env. episode 193.000000, reward total was -16.000000. running mean: -20.139239\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.147846\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.146368\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.154904\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.163355\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.171722\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.180004\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.188204\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.186322\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.194459\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.192514\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.200589\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.208583\n",
            "resetting env. episode 206.000000, reward total was -18.000000. running mean: -20.186498\n",
            "resetting env. episode 207.000000, reward total was -19.000000. running mean: -20.174633\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.162886\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.161257\n",
            "resetting env. episode 210.000000, reward total was -18.000000. running mean: -20.139645\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.148248\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.156766\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.165198\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.173546\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.181811\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.189993\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.198093\n",
            "resetting env. episode 218.000000, reward total was -17.000000. running mean: -20.166112\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.174451\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.182706\n",
            "resetting env. episode 221.000000, reward total was -16.000000. running mean: -20.140879\n",
            "resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.129470\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.128176\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.136894\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.135525\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.134170\n",
            "resetting env. episode 227.000000, reward total was -19.000000. running mean: -20.122828\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.131600\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.140284\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.148881\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.157392\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.155818\n",
            "resetting env. episode 233.000000, reward total was -19.000000. running mean: -20.144260\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.152817\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.161289\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.149676\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.158180\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.156598\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.165032\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.173381\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.181648\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.189831\n",
            "resetting env. episode 243.000000, reward total was -19.000000. running mean: -20.177933\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.176154\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.174392\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.172648\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.180922\n",
            "resetting env. episode 248.000000, reward total was -19.000000. running mean: -20.169112\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.177421\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.175647\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.183891\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.182052\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.180231\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.188429\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.196545\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.204579\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.212533\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.220408\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.228204\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.235922\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.233563\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.241227\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.248815\n",
            "resetting env. episode 264.000000, reward total was -19.000000. running mean: -20.236327\n",
            "resetting env. episode 265.000000, reward total was -19.000000. running mean: -20.223963\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.231724\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.229406\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.237112\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.244741\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -20.232294\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.239971\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.237571\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.235196\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.242844\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.250415\n",
            "resetting env. episode 276.000000, reward total was -19.000000. running mean: -20.237911\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.225532\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.223277\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.231044\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.228733\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.236446\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.234082\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.231741\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.239423\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.237029\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.234659\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.242312\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.249889\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.247390\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.254916\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.252367\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.259843\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.267245\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.274573\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.281827\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.279009\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.286219\n",
            "resetting env. episode 298.000000, reward total was -19.000000. running mean: -20.273356\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.280623\n",
            "resetting env. episode 300.000000, reward total was -18.000000. running mean: -20.257817\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.255238\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.252686\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.260159\n",
            "resetting env. episode 304.000000, reward total was -18.000000. running mean: -20.237558\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.245182\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.252730\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.250203\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.257701\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.255124\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.262573\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.269947\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.267247\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.264575\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.261929\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.269310\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.276617\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.283851\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.271012\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.268302\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.275619\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.282863\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.290034\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.297134\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.294162\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.301221\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.298209\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.305227\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.312174\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.309053\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.315962\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.312802\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.319674\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.316478\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.313313\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.320180\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.316978\n",
            "resetting env. episode 337.000000, reward total was -19.000000. running mean: -20.303808\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.290770\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.297862\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.304884\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.311835\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.318717\n",
            "resetting env. episode 343.000000, reward total was -19.000000. running mean: -20.305529\n",
            "resetting env. episode 344.000000, reward total was -19.000000. running mean: -20.292474\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.299549\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.306554\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.303488\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.310453\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.317349\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.324175\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.320934\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.327724\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.324447\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -20.311203\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.318091\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.314910\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.311761\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.318643\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.325457\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.322202\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.328980\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.335690\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.332333\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.339010\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.345620\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.352164\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.358642\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -20.345056\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.351605\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.348089\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -20.334608\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.331262\n",
            "resetting env. episode 373.000000, reward total was -19.000000. running mean: -20.317949\n",
            "resetting env. episode 374.000000, reward total was -18.000000. running mean: -20.294770\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.301822\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.308804\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.305716\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -20.292659\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.299732\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.306735\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.313667\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.320531\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.327326\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.334052\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.340712\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.347305\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.353832\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.360293\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.356690\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.343123\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.349692\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.336195\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.342833\n",
            "resetting env. episode 394.000000, reward total was -17.000000. running mean: -20.309405\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.306311\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.303248\n",
            "resetting env. episode 397.000000, reward total was -17.000000. running mean: -20.270215\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.267513\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.264838\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.262190\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.259568\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.266972\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.274302\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.281559\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.288744\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.285856\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.292998\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.300068\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.307067\n",
            "resetting env. episode 410.000000, reward total was -18.000000. running mean: -20.283996\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.281156\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.278345\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.275561\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.282806\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.289978\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.297078\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.304107\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.311066\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.307955\n",
            "resetting env. episode 420.000000, reward total was -19.000000. running mean: -20.294876\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.301927\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.308908\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.305819\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.312761\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.319633\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.326437\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.333172\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.339841\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.346442\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.352978\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.349448\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.355954\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.362394\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.368770\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.375082\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.371332\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.367618\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.373942\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.370203\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.366501\n",
            "resetting env. episode 441.000000, reward total was -19.000000. running mean: -20.352836\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.359307\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.365714\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.372057\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -20.358336\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.354753\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.361206\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.367593\n",
            "resetting env. episode 449.000000, reward total was -18.000000. running mean: -20.343918\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.340478\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.347074\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.343603\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.350167\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.356665\n",
            "resetting env. episode 455.000000, reward total was -19.000000. running mean: -20.343099\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.329668\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.336371\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.343007\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.349577\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.346081\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.352620\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.359094\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.355503\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.361948\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.368329\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.374646\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.370899\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.377190\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.383418\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.389584\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.395688\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.401731\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.397714\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.393737\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.399799\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.405801\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -20.391743\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.397826\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.403848\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.409809\n",
            "resetting env. episode 481.000000, reward total was -17.000000. running mean: -20.375711\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.381954\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.388135\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.394253\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.390311\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.396408\n",
            "resetting env. episode 487.000000, reward total was -18.000000. running mean: -20.372443\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.368719\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.355032\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.361482\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.367867\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.374188\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.370446\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.366742\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.363074\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.369444\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.375749\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.381992\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.388172\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.394290\n",
            "CPU times: user 24min 14s, sys: 10min 52s, total: 35min 6s\n",
            "Wall time: 18min 11s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "ffcb3d6d-44ac-4c6b-e127-bf48a00c00c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "d2a7c4cf-eac0-47b6-d862-4f1f54ff593b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 6.000000, reward total was -18.000000. running mean: -20.970000\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.960300\n",
            "resetting env. episode 8.000000, reward total was -18.000000. running mean: -20.930697\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.921390\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.912176\n",
            "resetting env. episode 11.000000, reward total was -18.000000. running mean: -20.883054\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.884224\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.885382\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.886528\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.887662\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.888786\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.879898\n",
            "resetting env. episode 18.000000, reward total was -19.000000. running mean: -20.861099\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.862488\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.863863\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.855225\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.856672\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.858106\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.849525\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.851029\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.852519\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.853994\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.855454\n",
            "resetting env. episode 29.000000, reward total was -19.000000. running mean: -20.836899\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.828530\n",
            "resetting env. episode 31.000000, reward total was -18.000000. running mean: -20.800245\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.792243\n",
            "resetting env. episode 33.000000, reward total was -19.000000. running mean: -20.774320\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.766577\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.758911\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.761322\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.763709\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.766072\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.758411\n",
            "resetting env. episode 40.000000, reward total was -18.000000. running mean: -20.730827\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.733519\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.736183\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.738822\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.741433\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.744019\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.746579\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.749113\n",
            "resetting env. episode 48.000000, reward total was -19.000000. running mean: -20.731622\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.734306\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.736963\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.739593\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.742197\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.734775\n",
            "resetting env. episode 54.000000, reward total was -19.000000. running mean: -20.717427\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.720253\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.723051\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.715820\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.718662\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.711475\n",
            "resetting env. episode 60.000000, reward total was -18.000000. running mean: -20.684361\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.677517\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.670742\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.674034\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.677294\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.680521\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.683716\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.686879\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.690010\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.693110\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.696179\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.699217\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.692225\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.695302\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.698349\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.691366\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.694452\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.687508\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.690633\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.683726\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -20.666889\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.670220\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.673518\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.676783\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.680015\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.683215\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.686383\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.679519\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.682724\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.675896\n",
            "resetting env. episode 90.000000, reward total was -19.000000. running mean: -20.659137\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.662546\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.665921\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.659261\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.652669\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.656142\n",
            "resetting env. episode 96.000000, reward total was -19.000000. running mean: -20.639581\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.643185\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -20.626753\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.630486\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.624181\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.627939\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.631659\n",
            "resetting env. episode 103.000000, reward total was -18.000000. running mean: -20.605343\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.609289\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.613197\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.617065\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.620894\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.624685\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.628438\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.622154\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.625932\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.619673\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.613476\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.617341\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.621168\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.624956\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.618707\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -20.602520\n",
            "resetting env. episode 119.000000, reward total was -19.000000. running mean: -20.586495\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.590630\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.584723\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.588876\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.592987\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.587057\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.591187\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.585275\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.589422\n",
            "resetting env. episode 128.000000, reward total was -17.000000. running mean: -20.553528\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.547993\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.552513\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.556988\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.561418\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.565804\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.570146\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.574444\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.568700\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.563013\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.567383\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.571709\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.575992\n",
            "resetting env. episode 141.000000, reward total was -19.000000. running mean: -20.560232\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.564629\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.568983\n",
            "resetting env. episode 144.000000, reward total was -19.000000. running mean: -20.553293\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.557760\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.562183\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.556561\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.560995\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.565385\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.569731\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.574034\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.578294\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.582511\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.576686\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.580919\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.585110\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.589259\n",
            "resetting env. episode 158.000000, reward total was -19.000000. running mean: -20.573366\n",
            "resetting env. episode 159.000000, reward total was -19.000000. running mean: -20.557632\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.562056\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.556436\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.560871\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.565262\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.569610\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.573914\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.578175\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.582393\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.586569\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.590703\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.594796\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.598848\n",
            "resetting env. episode 172.000000, reward total was -19.000000. running mean: -20.582860\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.567031\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.561361\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.565747\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.570090\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.574389\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.568645\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.572959\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.577229\n",
            "resetting env. episode 181.000000, reward total was -19.000000. running mean: -20.561457\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.545842\n",
            "resetting env. episode 183.000000, reward total was -19.000000. running mean: -20.530384\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.535080\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.529729\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.534432\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.539087\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.543697\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.548260\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.542777\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.537349\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.541976\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.536556\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.541190\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.545779\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.550321\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.544818\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.539369\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.543976\n",
            "resetting env. episode 200.000000, reward total was -19.000000. running mean: -20.528536\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.523251\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.518018\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.512838\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.517709\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.522532\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.517307\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.522134\n",
            "resetting env. episode 208.000000, reward total was -18.000000. running mean: -20.496913\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.501944\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.496924\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.501955\n",
            "resetting env. episode 212.000000, reward total was -19.000000. running mean: -20.486935\n",
            "resetting env. episode 213.000000, reward total was -19.000000. running mean: -20.472066\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.477345\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.482572\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.487746\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.492869\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.497940\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.502961\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.497931\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.502952\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.507922\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.512843\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.517714\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.522537\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -20.507312\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.512239\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.517116\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.511945\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.516826\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.521658\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.516441\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.521277\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.526064\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.520803\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.525595\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -20.510339\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.515236\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.520083\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.524883\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.529634\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.534337\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.538994\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.543604\n",
            "resetting env. episode 245.000000, reward total was -19.000000. running mean: -20.528168\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.532886\n",
            "resetting env. episode 247.000000, reward total was -19.000000. running mean: -20.517558\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.522382\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.527158\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.521887\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.516668\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.511501\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.516386\n",
            "resetting env. episode 254.000000, reward total was -19.000000. running mean: -20.501222\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.506210\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.511148\n",
            "resetting env. episode 257.000000, reward total was -17.000000. running mean: -20.476036\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.471276\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.466563\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.451898\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.457379\n",
            "resetting env. episode 262.000000, reward total was -19.000000. running mean: -20.442805\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.438377\n",
            "resetting env. episode 264.000000, reward total was -19.000000. running mean: -20.423993\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.429753\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.435456\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.441101\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.436690\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.442323\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.447900\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.453421\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.458887\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.464298\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.449655\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.455158\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.460607\n",
            "resetting env. episode 277.000000, reward total was -18.000000. running mean: -20.436001\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.431641\n",
            "resetting env. episode 279.000000, reward total was -19.000000. running mean: -20.417324\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.423151\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.428919\n",
            "resetting env. episode 282.000000, reward total was -19.000000. running mean: -20.414630\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.420484\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.416279\n",
            "resetting env. episode 285.000000, reward total was -19.000000. running mean: -20.402116\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.408095\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.414014\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.409874\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.415775\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.411618\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.407501\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.413426\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.409292\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.405199\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.411147\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.417036\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.422865\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.428637\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.434350\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.440007\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.445607\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.451151\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.446639\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.432173\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.417851\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.413673\n",
            "resetting env. episode 307.000000, reward total was -18.000000. running mean: -20.389536\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.395640\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.391684\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.397767\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.403790\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.409752\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.415654\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.411498\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.417383\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.423209\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.428977\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.424687\n",
            "resetting env. episode 319.000000, reward total was -19.000000. running mean: -20.410440\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.416336\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.422172\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.417951\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.423771\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.429533\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.435238\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.440886\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.446477\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.452012\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.457492\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.462917\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.468288\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.473605\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.478869\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.484080\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.489239\n",
            "resetting env. episode 336.000000, reward total was -19.000000. running mean: -20.474347\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.479604\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.474808\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.480059\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.485259\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.490406\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.495502\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.500547\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.505542\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.510486\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.505381\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.510328\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.515224\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.520072\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.514871\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.499723\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.504725\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.499678\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -20.484681\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.489835\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.494936\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.499987\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.504987\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.509937\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.514838\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.519689\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.524492\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.529248\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.533955\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -20.518616\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.513429\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.508295\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.513212\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.518080\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.522899\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -20.507670\n",
            "resetting env. episode 372.000000, reward total was -19.000000. running mean: -20.492594\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.497668\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.502691\n",
            "resetting env. episode 375.000000, reward total was -19.000000. running mean: -20.487664\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.492787\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.497859\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.502881\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.507852\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.512774\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -20.497646\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.502669\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.497643\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.502666\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.507640\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.512563\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.517438\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.522263\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.527041\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.521770\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.526552\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.511287\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.506174\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.491112\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.496201\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.501239\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.506227\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.511165\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.516053\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.500892\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.505883\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.510825\n",
            "resetting env. episode 403.000000, reward total was -19.000000. running mean: -20.495716\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.490759\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.495852\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.490893\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.495984\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -20.481024\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.486214\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.481352\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.486538\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.491673\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.476756\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.481989\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.487169\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.492297\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.497374\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.492400\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.487476\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.492602\n",
            "resetting env. episode 421.000000, reward total was -18.000000. running mean: -20.467676\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.462999\n",
            "resetting env. episode 423.000000, reward total was -19.000000. running mean: -20.448369\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.453885\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.459346\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.464753\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.470105\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.475404\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.480650\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.485844\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.480985\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.476175\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.481414\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.486600\n",
            "resetting env. episode 435.000000, reward total was -19.000000. running mean: -20.471734\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.477016\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.482246\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.487424\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.492549\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.487624\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.492748\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.487820\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.492942\n",
            "resetting env. episode 444.000000, reward total was -19.000000. running mean: -20.478013\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.473232\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.468500\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.473815\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.479077\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -20.464286\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.469643\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.474947\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.480197\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.485395\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.470541\n",
            "resetting env. episode 455.000000, reward total was -19.000000. running mean: -20.455836\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.461278\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.466665\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -20.451998\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.447478\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.453004\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.458473\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.453889\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.459350\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.464756\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.470109\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.465408\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.470754\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.466046\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.471386\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.466672\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.462005\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -20.447385\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.442911\n",
            "resetting env. episode 474.000000, reward total was -18.000000. running mean: -20.418482\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.424297\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.430054\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.435754\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.441396\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.436982\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.442612\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.448186\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.443704\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.449267\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.454775\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.460227\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.455625\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.461068\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.456458\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.461893\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.457274\n",
            "resetting env. episode 491.000000, reward total was -19.000000. running mean: -20.442701\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.438274\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.433892\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.429553\n",
            "resetting env. episode 495.000000, reward total was -18.000000. running mean: -20.405257\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.411205\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.417093\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.422922\n",
            "resetting env. episode 499.000000, reward total was -18.000000. running mean: -20.398693\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.394706\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -20.390759\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.396851\n",
            "resetting env. episode 503.000000, reward total was -19.000000. running mean: -20.382882\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.389054\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.395163\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.401211\n",
            "resetting env. episode 507.000000, reward total was -18.000000. running mean: -20.377199\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -20.373427\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.379693\n",
            "resetting env. episode 510.000000, reward total was -21.000000. running mean: -20.385896\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.392037\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.398117\n",
            "resetting env. episode 513.000000, reward total was -20.000000. running mean: -20.394136\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -20.400194\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.406192\n",
            "resetting env. episode 516.000000, reward total was -21.000000. running mean: -20.412130\n",
            "resetting env. episode 517.000000, reward total was -20.000000. running mean: -20.408009\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.413929\n",
            "resetting env. episode 519.000000, reward total was -20.000000. running mean: -20.409790\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.415692\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.421535\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.427320\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -20.423046\n",
            "resetting env. episode 524.000000, reward total was -21.000000. running mean: -20.428816\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.424528\n",
            "resetting env. episode 526.000000, reward total was -20.000000. running mean: -20.420282\n",
            "resetting env. episode 527.000000, reward total was -19.000000. running mean: -20.406080\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.412019\n",
            "resetting env. episode 529.000000, reward total was -20.000000. running mean: -20.407899\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -20.413820\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.419681\n",
            "resetting env. episode 532.000000, reward total was -20.000000. running mean: -20.415485\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.421330\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.427117\n",
            "resetting env. episode 535.000000, reward total was -21.000000. running mean: -20.432845\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.438517\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.444132\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.449690\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -20.455194\n",
            "resetting env. episode 540.000000, reward total was -20.000000. running mean: -20.450642\n",
            "resetting env. episode 541.000000, reward total was -20.000000. running mean: -20.446135\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.451674\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.457157\n",
            "resetting env. episode 544.000000, reward total was -20.000000. running mean: -20.452586\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -20.458060\n",
            "resetting env. episode 546.000000, reward total was -19.000000. running mean: -20.443479\n",
            "resetting env. episode 547.000000, reward total was -20.000000. running mean: -20.439044\n",
            "resetting env. episode 548.000000, reward total was -20.000000. running mean: -20.434654\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.440307\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.445904\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.451445\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.456931\n",
            "resetting env. episode 553.000000, reward total was -21.000000. running mean: -20.462361\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -20.467738\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.473060\n",
            "resetting env. episode 556.000000, reward total was -21.000000. running mean: -20.478330\n",
            "resetting env. episode 557.000000, reward total was -20.000000. running mean: -20.473547\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.478811\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.484023\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -20.489183\n",
            "resetting env. episode 561.000000, reward total was -21.000000. running mean: -20.494291\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.499348\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.504354\n",
            "resetting env. episode 564.000000, reward total was -19.000000. running mean: -20.489311\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.494418\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.499474\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.504479\n",
            "resetting env. episode 568.000000, reward total was -21.000000. running mean: -20.509434\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -20.514340\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.519196\n",
            "resetting env. episode 571.000000, reward total was -20.000000. running mean: -20.514004\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -20.518864\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -20.513676\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.518539\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.523354\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.528120\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -20.522839\n",
            "resetting env. episode 578.000000, reward total was -20.000000. running mean: -20.517610\n",
            "resetting env. episode 579.000000, reward total was -19.000000. running mean: -20.502434\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -20.497410\n",
            "resetting env. episode 581.000000, reward total was -20.000000. running mean: -20.492436\n",
            "resetting env. episode 582.000000, reward total was -20.000000. running mean: -20.487512\n",
            "resetting env. episode 583.000000, reward total was -19.000000. running mean: -20.472636\n",
            "resetting env. episode 584.000000, reward total was -20.000000. running mean: -20.467910\n",
            "resetting env. episode 585.000000, reward total was -18.000000. running mean: -20.443231\n",
            "resetting env. episode 586.000000, reward total was -20.000000. running mean: -20.438799\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.444411\n",
            "resetting env. episode 588.000000, reward total was -19.000000. running mean: -20.429967\n",
            "resetting env. episode 589.000000, reward total was -18.000000. running mean: -20.405667\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.411610\n",
            "resetting env. episode 591.000000, reward total was -19.000000. running mean: -20.397494\n",
            "resetting env. episode 592.000000, reward total was -19.000000. running mean: -20.383519\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.389684\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -20.385787\n",
            "resetting env. episode 595.000000, reward total was -19.000000. running mean: -20.371929\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -20.378210\n",
            "resetting env. episode 597.000000, reward total was -18.000000. running mean: -20.354428\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -20.350884\n",
            "resetting env. episode 599.000000, reward total was -20.000000. running mean: -20.347375\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.353901\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -20.360362\n",
            "resetting env. episode 602.000000, reward total was -20.000000. running mean: -20.356758\n",
            "resetting env. episode 603.000000, reward total was -19.000000. running mean: -20.343191\n",
            "resetting env. episode 604.000000, reward total was -20.000000. running mean: -20.339759\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.346361\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -20.342898\n",
            "resetting env. episode 607.000000, reward total was -18.000000. running mean: -20.319469\n",
            "resetting env. episode 608.000000, reward total was -20.000000. running mean: -20.316274\n",
            "resetting env. episode 609.000000, reward total was -21.000000. running mean: -20.323111\n",
            "resetting env. episode 610.000000, reward total was -19.000000. running mean: -20.309880\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.316781\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.323614\n",
            "resetting env. episode 613.000000, reward total was -19.000000. running mean: -20.310377\n",
            "resetting env. episode 614.000000, reward total was -20.000000. running mean: -20.307274\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.314201\n",
            "resetting env. episode 616.000000, reward total was -21.000000. running mean: -20.321059\n",
            "resetting env. episode 617.000000, reward total was -19.000000. running mean: -20.307848\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.314770\n",
            "resetting env. episode 619.000000, reward total was -18.000000. running mean: -20.291622\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.298706\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.305719\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.312662\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.319535\n",
            "resetting env. episode 624.000000, reward total was -19.000000. running mean: -20.306340\n",
            "resetting env. episode 625.000000, reward total was -20.000000. running mean: -20.303276\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.310244\n",
            "resetting env. episode 627.000000, reward total was -19.000000. running mean: -20.297141\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.304170\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.311128\n",
            "resetting env. episode 630.000000, reward total was -20.000000. running mean: -20.308017\n",
            "resetting env. episode 631.000000, reward total was -20.000000. running mean: -20.304937\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.311887\n",
            "resetting env. episode 633.000000, reward total was -20.000000. running mean: -20.308768\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.315681\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.322524\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.329299\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -20.336006\n",
            "resetting env. episode 638.000000, reward total was -20.000000. running mean: -20.332646\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.339319\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.345926\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.352467\n",
            "resetting env. episode 642.000000, reward total was -19.000000. running mean: -20.338942\n",
            "resetting env. episode 643.000000, reward total was -21.000000. running mean: -20.345553\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.352097\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.358576\n",
            "resetting env. episode 646.000000, reward total was -20.000000. running mean: -20.354990\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.361440\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.367826\n",
            "resetting env. episode 649.000000, reward total was -20.000000. running mean: -20.364148\n",
            "resetting env. episode 650.000000, reward total was -20.000000. running mean: -20.360506\n",
            "resetting env. episode 651.000000, reward total was -17.000000. running mean: -20.326901\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.333632\n",
            "resetting env. episode 653.000000, reward total was -19.000000. running mean: -20.320296\n",
            "resetting env. episode 654.000000, reward total was -20.000000. running mean: -20.317093\n",
            "resetting env. episode 655.000000, reward total was -20.000000. running mean: -20.313922\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -20.310783\n",
            "resetting env. episode 657.000000, reward total was -21.000000. running mean: -20.317675\n",
            "resetting env. episode 658.000000, reward total was -21.000000. running mean: -20.324498\n",
            "resetting env. episode 659.000000, reward total was -18.000000. running mean: -20.301253\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.308241\n",
            "resetting env. episode 661.000000, reward total was -20.000000. running mean: -20.305158\n",
            "resetting env. episode 662.000000, reward total was -19.000000. running mean: -20.292107\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -20.299186\n",
            "resetting env. episode 664.000000, reward total was -17.000000. running mean: -20.266194\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -20.263532\n",
            "resetting env. episode 666.000000, reward total was -18.000000. running mean: -20.240896\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -20.248488\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.256003\n",
            "resetting env. episode 669.000000, reward total was -20.000000. running mean: -20.253443\n",
            "resetting env. episode 670.000000, reward total was -20.000000. running mean: -20.250908\n",
            "resetting env. episode 671.000000, reward total was -20.000000. running mean: -20.248399\n",
            "resetting env. episode 672.000000, reward total was -21.000000. running mean: -20.255915\n",
            "resetting env. episode 673.000000, reward total was -20.000000. running mean: -20.253356\n",
            "resetting env. episode 674.000000, reward total was -19.000000. running mean: -20.240822\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -20.248414\n",
            "resetting env. episode 676.000000, reward total was -19.000000. running mean: -20.235930\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.233571\n",
            "resetting env. episode 678.000000, reward total was -20.000000. running mean: -20.231235\n",
            "resetting env. episode 679.000000, reward total was -21.000000. running mean: -20.238923\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -20.246533\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.254068\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -20.251527\n",
            "resetting env. episode 683.000000, reward total was -19.000000. running mean: -20.239012\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -20.236622\n",
            "resetting env. episode 685.000000, reward total was -20.000000. running mean: -20.234256\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -20.241913\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -20.239494\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.247099\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.254628\n",
            "resetting env. episode 690.000000, reward total was -21.000000. running mean: -20.262082\n",
            "resetting env. episode 691.000000, reward total was -21.000000. running mean: -20.269461\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.276767\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.283999\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.291159\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.298247\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.305265\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.312212\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -20.319090\n",
            "resetting env. episode 699.000000, reward total was -19.000000. running mean: -20.305899\n",
            "resetting env. episode 700.000000, reward total was -19.000000. running mean: -20.292840\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.299912\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -20.306913\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -20.313843\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -20.320705\n",
            "resetting env. episode 705.000000, reward total was -21.000000. running mean: -20.327498\n",
            "resetting env. episode 706.000000, reward total was -20.000000. running mean: -20.324223\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -20.330981\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -20.327671\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.334394\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.341050\n",
            "resetting env. episode 711.000000, reward total was -21.000000. running mean: -20.347640\n",
            "resetting env. episode 712.000000, reward total was -21.000000. running mean: -20.354163\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.360622\n",
            "resetting env. episode 714.000000, reward total was -18.000000. running mean: -20.337016\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.343645\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.350209\n",
            "resetting env. episode 717.000000, reward total was -19.000000. running mean: -20.336707\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -20.333340\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -20.340006\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.346606\n",
            "resetting env. episode 721.000000, reward total was -18.000000. running mean: -20.323140\n",
            "resetting env. episode 722.000000, reward total was -20.000000. running mean: -20.319909\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.326710\n",
            "resetting env. episode 724.000000, reward total was -19.000000. running mean: -20.313443\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.320308\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -20.317105\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.323934\n",
            "resetting env. episode 728.000000, reward total was -18.000000. running mean: -20.300695\n",
            "resetting env. episode 729.000000, reward total was -18.000000. running mean: -20.277688\n",
            "resetting env. episode 730.000000, reward total was -19.000000. running mean: -20.264911\n",
            "resetting env. episode 731.000000, reward total was -19.000000. running mean: -20.252262\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.259739\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.267142\n",
            "resetting env. episode 734.000000, reward total was -21.000000. running mean: -20.274470\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.281726\n",
            "resetting env. episode 736.000000, reward total was -19.000000. running mean: -20.268908\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.276219\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.283457\n",
            "resetting env. episode 739.000000, reward total was -21.000000. running mean: -20.290623\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.297716\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.304739\n",
            "resetting env. episode 742.000000, reward total was -21.000000. running mean: -20.311692\n",
            "resetting env. episode 743.000000, reward total was -21.000000. running mean: -20.318575\n",
            "resetting env. episode 744.000000, reward total was -20.000000. running mean: -20.315389\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -20.322235\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -20.319013\n",
            "resetting env. episode 747.000000, reward total was -20.000000. running mean: -20.315823\n",
            "resetting env. episode 748.000000, reward total was -19.000000. running mean: -20.302665\n",
            "resetting env. episode 749.000000, reward total was -20.000000. running mean: -20.299638\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.306642\n",
            "resetting env. episode 751.000000, reward total was -20.000000. running mean: -20.303575\n",
            "resetting env. episode 752.000000, reward total was -20.000000. running mean: -20.300539\n",
            "resetting env. episode 753.000000, reward total was -20.000000. running mean: -20.297534\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -20.304559\n",
            "resetting env. episode 755.000000, reward total was -21.000000. running mean: -20.311513\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -20.318398\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.325214\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.331962\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.338642\n",
            "resetting env. episode 760.000000, reward total was -20.000000. running mean: -20.335256\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.341903\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.348484\n",
            "resetting env. episode 763.000000, reward total was -19.000000. running mean: -20.334999\n",
            "resetting env. episode 764.000000, reward total was -21.000000. running mean: -20.341649\n",
            "resetting env. episode 765.000000, reward total was -19.000000. running mean: -20.328233\n",
            "resetting env. episode 766.000000, reward total was -19.000000. running mean: -20.314951\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -20.321801\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.328583\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.335297\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.341944\n",
            "resetting env. episode 771.000000, reward total was -20.000000. running mean: -20.338525\n",
            "resetting env. episode 772.000000, reward total was -20.000000. running mean: -20.335140\n",
            "resetting env. episode 773.000000, reward total was -20.000000. running mean: -20.331788\n",
            "resetting env. episode 774.000000, reward total was -20.000000. running mean: -20.328470\n",
            "resetting env. episode 775.000000, reward total was -20.000000. running mean: -20.325186\n",
            "resetting env. episode 776.000000, reward total was -21.000000. running mean: -20.331934\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -20.338614\n",
            "resetting env. episode 778.000000, reward total was -21.000000. running mean: -20.345228\n",
            "resetting env. episode 779.000000, reward total was -17.000000. running mean: -20.311776\n",
            "resetting env. episode 780.000000, reward total was -18.000000. running mean: -20.288658\n",
            "resetting env. episode 781.000000, reward total was -19.000000. running mean: -20.275772\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -20.273014\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -20.280284\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -20.287481\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.294606\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -20.301660\n",
            "resetting env. episode 787.000000, reward total was -20.000000. running mean: -20.298643\n",
            "resetting env. episode 788.000000, reward total was -19.000000. running mean: -20.285657\n",
            "resetting env. episode 789.000000, reward total was -20.000000. running mean: -20.282800\n",
            "resetting env. episode 790.000000, reward total was -19.000000. running mean: -20.269972\n",
            "resetting env. episode 791.000000, reward total was -20.000000. running mean: -20.267273\n",
            "resetting env. episode 792.000000, reward total was -19.000000. running mean: -20.254600\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.262054\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.269433\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.276739\n",
            "resetting env. episode 796.000000, reward total was -20.000000. running mean: -20.273972\n",
            "resetting env. episode 797.000000, reward total was -19.000000. running mean: -20.261232\n",
            "resetting env. episode 798.000000, reward total was -20.000000. running mean: -20.258620\n",
            "resetting env. episode 799.000000, reward total was -19.000000. running mean: -20.246033\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.253573\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -20.261037\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.268427\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.275743\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -20.282985\n",
            "resetting env. episode 805.000000, reward total was -20.000000. running mean: -20.280155\n",
            "resetting env. episode 806.000000, reward total was -19.000000. running mean: -20.267354\n",
            "resetting env. episode 807.000000, reward total was -21.000000. running mean: -20.274680\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.281934\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.289114\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.296223\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -20.293261\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -20.300328\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -20.307325\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -20.314252\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -20.311109\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.317998\n",
            "resetting env. episode 817.000000, reward total was -20.000000. running mean: -20.314818\n",
            "resetting env. episode 818.000000, reward total was -20.000000. running mean: -20.311670\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -20.318553\n",
            "resetting env. episode 820.000000, reward total was -21.000000. running mean: -20.325368\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.332114\n",
            "resetting env. episode 822.000000, reward total was -20.000000. running mean: -20.328793\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.335505\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -20.342150\n",
            "resetting env. episode 825.000000, reward total was -19.000000. running mean: -20.328728\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.335441\n",
            "resetting env. episode 827.000000, reward total was -19.000000. running mean: -20.322087\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.328866\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.335577\n",
            "resetting env. episode 830.000000, reward total was -20.000000. running mean: -20.332221\n",
            "resetting env. episode 831.000000, reward total was -20.000000. running mean: -20.328899\n",
            "resetting env. episode 832.000000, reward total was -20.000000. running mean: -20.325610\n",
            "resetting env. episode 833.000000, reward total was -20.000000. running mean: -20.322354\n",
            "resetting env. episode 834.000000, reward total was -21.000000. running mean: -20.329131\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.335839\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -20.342481\n",
            "resetting env. episode 837.000000, reward total was -20.000000. running mean: -20.339056\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.345666\n",
            "resetting env. episode 839.000000, reward total was -20.000000. running mean: -20.342209\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -20.348787\n",
            "resetting env. episode 841.000000, reward total was -20.000000. running mean: -20.345299\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.351846\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -20.358327\n",
            "resetting env. episode 844.000000, reward total was -18.000000. running mean: -20.334744\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -20.341397\n",
            "resetting env. episode 846.000000, reward total was -19.000000. running mean: -20.327983\n",
            "resetting env. episode 847.000000, reward total was -21.000000. running mean: -20.334703\n",
            "resetting env. episode 848.000000, reward total was -19.000000. running mean: -20.321356\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -20.328142\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.334861\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -20.331512\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.338197\n",
            "resetting env. episode 853.000000, reward total was -18.000000. running mean: -20.314815\n",
            "resetting env. episode 854.000000, reward total was -20.000000. running mean: -20.311667\n",
            "resetting env. episode 855.000000, reward total was -20.000000. running mean: -20.308550\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.315465\n",
            "resetting env. episode 857.000000, reward total was -19.000000. running mean: -20.302310\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -20.309287\n",
            "resetting env. episode 859.000000, reward total was -21.000000. running mean: -20.316194\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.323032\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -20.329802\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -20.336504\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -20.343139\n",
            "resetting env. episode 864.000000, reward total was -19.000000. running mean: -20.329708\n",
            "resetting env. episode 865.000000, reward total was -19.000000. running mean: -20.316410\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.323246\n",
            "resetting env. episode 867.000000, reward total was -20.000000. running mean: -20.320014\n",
            "resetting env. episode 868.000000, reward total was -21.000000. running mean: -20.326814\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.333546\n",
            "resetting env. episode 870.000000, reward total was -21.000000. running mean: -20.340210\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -20.346808\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -20.353340\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.359807\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.366209\n",
            "resetting env. episode 875.000000, reward total was -17.000000. running mean: -20.332546\n",
            "resetting env. episode 876.000000, reward total was -17.000000. running mean: -20.299221\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.306229\n",
            "resetting env. episode 878.000000, reward total was -20.000000. running mean: -20.303166\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -20.310135\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -20.317033\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.323863\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -20.320625\n",
            "resetting env. episode 883.000000, reward total was -20.000000. running mean: -20.317418\n",
            "resetting env. episode 884.000000, reward total was -20.000000. running mean: -20.314244\n",
            "resetting env. episode 885.000000, reward total was -21.000000. running mean: -20.321102\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -20.327891\n",
            "resetting env. episode 887.000000, reward total was -19.000000. running mean: -20.314612\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.321466\n",
            "resetting env. episode 889.000000, reward total was -20.000000. running mean: -20.318251\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.325068\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.331818\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -20.338500\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -20.335115\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -20.341763\n",
            "resetting env. episode 895.000000, reward total was -21.000000. running mean: -20.348346\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.354862\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.361314\n",
            "resetting env. episode 898.000000, reward total was -20.000000. running mean: -20.357701\n",
            "resetting env. episode 899.000000, reward total was -20.000000. running mean: -20.354124\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.360582\n",
            "resetting env. episode 901.000000, reward total was -20.000000. running mean: -20.356977\n",
            "resetting env. episode 902.000000, reward total was -20.000000. running mean: -20.353407\n",
            "resetting env. episode 903.000000, reward total was -20.000000. running mean: -20.349873\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -20.356374\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.362810\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.369182\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -20.375490\n",
            "resetting env. episode 908.000000, reward total was -19.000000. running mean: -20.361735\n",
            "resetting env. episode 909.000000, reward total was -19.000000. running mean: -20.348118\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -20.354637\n",
            "resetting env. episode 911.000000, reward total was -20.000000. running mean: -20.351090\n",
            "resetting env. episode 912.000000, reward total was -20.000000. running mean: -20.347580\n",
            "resetting env. episode 913.000000, reward total was -19.000000. running mean: -20.334104\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -20.340763\n",
            "resetting env. episode 915.000000, reward total was -21.000000. running mean: -20.347355\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.353882\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.360343\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -20.366739\n",
            "resetting env. episode 919.000000, reward total was -21.000000. running mean: -20.373072\n",
            "resetting env. episode 920.000000, reward total was -20.000000. running mean: -20.369341\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -20.375648\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -20.381891\n",
            "resetting env. episode 923.000000, reward total was -20.000000. running mean: -20.378072\n",
            "resetting env. episode 924.000000, reward total was -20.000000. running mean: -20.374292\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -20.380549\n",
            "resetting env. episode 926.000000, reward total was -20.000000. running mean: -20.376743\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -20.382976\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.389146\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -20.395255\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.401302\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -20.407289\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.413216\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -20.419084\n",
            "resetting env. episode 934.000000, reward total was -21.000000. running mean: -20.424893\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -20.430644\n",
            "resetting env. episode 936.000000, reward total was -20.000000. running mean: -20.426338\n",
            "resetting env. episode 937.000000, reward total was -21.000000. running mean: -20.432074\n",
            "resetting env. episode 938.000000, reward total was -19.000000. running mean: -20.417754\n",
            "resetting env. episode 939.000000, reward total was -19.000000. running mean: -20.403576\n",
            "resetting env. episode 940.000000, reward total was -20.000000. running mean: -20.399540\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -20.405545\n",
            "resetting env. episode 942.000000, reward total was -21.000000. running mean: -20.411490\n",
            "resetting env. episode 943.000000, reward total was -19.000000. running mean: -20.397375\n",
            "resetting env. episode 944.000000, reward total was -21.000000. running mean: -20.403401\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -20.409367\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -20.415273\n",
            "resetting env. episode 947.000000, reward total was -21.000000. running mean: -20.421120\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -20.426909\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -20.432640\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.438314\n",
            "resetting env. episode 951.000000, reward total was -20.000000. running mean: -20.433931\n",
            "resetting env. episode 952.000000, reward total was -20.000000. running mean: -20.429591\n",
            "resetting env. episode 953.000000, reward total was -19.000000. running mean: -20.415295\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.421142\n",
            "resetting env. episode 955.000000, reward total was -20.000000. running mean: -20.416931\n",
            "resetting env. episode 956.000000, reward total was -19.000000. running mean: -20.402762\n",
            "resetting env. episode 957.000000, reward total was -19.000000. running mean: -20.388734\n",
            "resetting env. episode 958.000000, reward total was -19.000000. running mean: -20.374847\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.381098\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -20.387287\n",
            "resetting env. episode 961.000000, reward total was -20.000000. running mean: -20.383414\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -20.389580\n",
            "resetting env. episode 963.000000, reward total was -20.000000. running mean: -20.385684\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -20.391828\n",
            "resetting env. episode 965.000000, reward total was -21.000000. running mean: -20.397909\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.403930\n",
            "resetting env. episode 967.000000, reward total was -21.000000. running mean: -20.409891\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -20.415792\n",
            "resetting env. episode 969.000000, reward total was -18.000000. running mean: -20.391634\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.397718\n",
            "resetting env. episode 971.000000, reward total was -20.000000. running mean: -20.393741\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.399803\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.405805\n",
            "resetting env. episode 974.000000, reward total was -21.000000. running mean: -20.411747\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.417630\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.423453\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -20.429219\n",
            "resetting env. episode 978.000000, reward total was -19.000000. running mean: -20.414927\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.420777\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -20.426570\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.432304\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.437981\n",
            "resetting env. episode 983.000000, reward total was -20.000000. running mean: -20.433601\n",
            "resetting env. episode 984.000000, reward total was -18.000000. running mean: -20.409265\n",
            "resetting env. episode 985.000000, reward total was -20.000000. running mean: -20.405172\n",
            "resetting env. episode 986.000000, reward total was -20.000000. running mean: -20.401121\n",
            "resetting env. episode 987.000000, reward total was -19.000000. running mean: -20.387109\n",
            "resetting env. episode 988.000000, reward total was -20.000000. running mean: -20.383238\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -20.389406\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.395512\n",
            "resetting env. episode 991.000000, reward total was -20.000000. running mean: -20.391557\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -20.397641\n",
            "resetting env. episode 993.000000, reward total was -20.000000. running mean: -20.393665\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.399728\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.405731\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -20.411674\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.417557\n",
            "resetting env. episode 998.000000, reward total was -21.000000. running mean: -20.423381\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -20.429147\n",
            "resetting env. episode 1000.000000, reward total was -21.000000. running mean: -20.434856\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -20.440507\n",
            "resetting env. episode 1002.000000, reward total was -18.000000. running mean: -20.416102\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.421941\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -20.427722\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.433445\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.439110\n",
            "resetting env. episode 1007.000000, reward total was -19.000000. running mean: -20.424719\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -20.430472\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.436167\n",
            "resetting env. episode 1010.000000, reward total was -17.000000. running mean: -20.401806\n",
            "resetting env. episode 1011.000000, reward total was -20.000000. running mean: -20.397788\n",
            "resetting env. episode 1012.000000, reward total was -20.000000. running mean: -20.393810\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.399872\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.405873\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.411814\n",
            "resetting env. episode 1016.000000, reward total was -21.000000. running mean: -20.417696\n",
            "resetting env. episode 1017.000000, reward total was -21.000000. running mean: -20.423519\n",
            "resetting env. episode 1018.000000, reward total was -20.000000. running mean: -20.419284\n",
            "resetting env. episode 1019.000000, reward total was -20.000000. running mean: -20.415091\n",
            "resetting env. episode 1020.000000, reward total was -18.000000. running mean: -20.390940\n",
            "resetting env. episode 1021.000000, reward total was -20.000000. running mean: -20.387031\n",
            "resetting env. episode 1022.000000, reward total was -20.000000. running mean: -20.383160\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -20.389329\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.395435\n",
            "resetting env. episode 1025.000000, reward total was -20.000000. running mean: -20.391481\n",
            "resetting env. episode 1026.000000, reward total was -20.000000. running mean: -20.387566\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -20.393691\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.399754\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.405756\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -20.411699\n",
            "resetting env. episode 1031.000000, reward total was -20.000000. running mean: -20.407582\n",
            "resetting env. episode 1032.000000, reward total was -21.000000. running mean: -20.413506\n",
            "resetting env. episode 1033.000000, reward total was -21.000000. running mean: -20.419371\n",
            "resetting env. episode 1034.000000, reward total was -20.000000. running mean: -20.415177\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -20.421025\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -20.426815\n",
            "resetting env. episode 1037.000000, reward total was -19.000000. running mean: -20.412547\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -20.408421\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.414337\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.420194\n",
            "resetting env. episode 1041.000000, reward total was -21.000000. running mean: -20.425992\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.431732\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.437415\n",
            "resetting env. episode 1044.000000, reward total was -21.000000. running mean: -20.443041\n",
            "resetting env. episode 1045.000000, reward total was -19.000000. running mean: -20.428610\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -20.434324\n",
            "resetting env. episode 1047.000000, reward total was -20.000000. running mean: -20.429981\n",
            "resetting env. episode 1048.000000, reward total was -18.000000. running mean: -20.405681\n",
            "resetting env. episode 1049.000000, reward total was -21.000000. running mean: -20.411624\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -20.417508\n",
            "resetting env. episode 1051.000000, reward total was -20.000000. running mean: -20.413333\n",
            "resetting env. episode 1052.000000, reward total was -21.000000. running mean: -20.419199\n",
            "resetting env. episode 1053.000000, reward total was -19.000000. running mean: -20.405008\n",
            "resetting env. episode 1054.000000, reward total was -21.000000. running mean: -20.410957\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -20.416848\n",
            "resetting env. episode 1056.000000, reward total was -19.000000. running mean: -20.402679\n",
            "resetting env. episode 1057.000000, reward total was -20.000000. running mean: -20.398653\n",
            "resetting env. episode 1058.000000, reward total was -20.000000. running mean: -20.394666\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -20.400719\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -20.406712\n",
            "resetting env. episode 1061.000000, reward total was -20.000000. running mean: -20.402645\n",
            "resetting env. episode 1062.000000, reward total was -20.000000. running mean: -20.398619\n",
            "resetting env. episode 1063.000000, reward total was -21.000000. running mean: -20.404632\n",
            "resetting env. episode 1064.000000, reward total was -20.000000. running mean: -20.400586\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.406580\n",
            "resetting env. episode 1066.000000, reward total was -21.000000. running mean: -20.412514\n",
            "resetting env. episode 1067.000000, reward total was -20.000000. running mean: -20.408389\n",
            "resetting env. episode 1068.000000, reward total was -21.000000. running mean: -20.414305\n",
            "resetting env. episode 1069.000000, reward total was -20.000000. running mean: -20.410162\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -20.416061\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -20.421900\n",
            "resetting env. episode 1072.000000, reward total was -19.000000. running mean: -20.407681\n",
            "resetting env. episode 1073.000000, reward total was -20.000000. running mean: -20.403604\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.409568\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -20.415473\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.421318\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.427105\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -20.432834\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.438505\n",
            "resetting env. episode 1080.000000, reward total was -20.000000. running mean: -20.434120\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -20.439779\n",
            "resetting env. episode 1082.000000, reward total was -19.000000. running mean: -20.425381\n",
            "resetting env. episode 1083.000000, reward total was -21.000000. running mean: -20.431127\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.436816\n",
            "resetting env. episode 1085.000000, reward total was -20.000000. running mean: -20.432448\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.438124\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -20.443742\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.449305\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -20.444812\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.450364\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.455860\n",
            "resetting env. episode 1092.000000, reward total was -19.000000. running mean: -20.441301\n",
            "resetting env. episode 1093.000000, reward total was -20.000000. running mean: -20.436888\n",
            "resetting env. episode 1094.000000, reward total was -21.000000. running mean: -20.442520\n",
            "resetting env. episode 1095.000000, reward total was -20.000000. running mean: -20.438094\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -20.443713\n",
            "resetting env. episode 1097.000000, reward total was -19.000000. running mean: -20.429276\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -20.434984\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.440634\n",
            "resetting env. episode 1100.000000, reward total was -21.000000. running mean: -20.446227\n",
            "resetting env. episode 1101.000000, reward total was -19.000000. running mean: -20.431765\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.437447\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -20.443073\n",
            "resetting env. episode 1104.000000, reward total was -20.000000. running mean: -20.438642\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.444256\n",
            "resetting env. episode 1106.000000, reward total was -20.000000. running mean: -20.439813\n",
            "resetting env. episode 1107.000000, reward total was -20.000000. running mean: -20.435415\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -20.441061\n",
            "resetting env. episode 1109.000000, reward total was -20.000000. running mean: -20.436650\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -20.442284\n",
            "resetting env. episode 1111.000000, reward total was -21.000000. running mean: -20.447861\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -20.453382\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -20.458849\n",
            "resetting env. episode 1114.000000, reward total was -21.000000. running mean: -20.464260\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.469617\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.474921\n",
            "resetting env. episode 1117.000000, reward total was -20.000000. running mean: -20.470172\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -20.475470\n",
            "resetting env. episode 1119.000000, reward total was -20.000000. running mean: -20.470716\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -20.476009\n",
            "resetting env. episode 1121.000000, reward total was -21.000000. running mean: -20.481248\n",
            "resetting env. episode 1122.000000, reward total was -20.000000. running mean: -20.476436\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.481672\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.486855\n",
            "resetting env. episode 1125.000000, reward total was -18.000000. running mean: -20.461986\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -20.467366\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.472693\n",
            "resetting env. episode 1128.000000, reward total was -21.000000. running mean: -20.477966\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.483186\n",
            "resetting env. episode 1130.000000, reward total was -20.000000. running mean: -20.478354\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -20.483571\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -20.488735\n",
            "resetting env. episode 1133.000000, reward total was -19.000000. running mean: -20.473848\n",
            "resetting env. episode 1134.000000, reward total was -19.000000. running mean: -20.459109\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -20.454518\n",
            "resetting env. episode 1136.000000, reward total was -21.000000. running mean: -20.459973\n",
            "resetting env. episode 1137.000000, reward total was -20.000000. running mean: -20.455373\n",
            "resetting env. episode 1138.000000, reward total was -21.000000. running mean: -20.460820\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -20.466211\n",
            "resetting env. episode 1140.000000, reward total was -20.000000. running mean: -20.461549\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -20.466934\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -20.472264\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -20.477542\n",
            "resetting env. episode 1144.000000, reward total was -18.000000. running mean: -20.452766\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -20.458239\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -20.463656\n",
            "resetting env. episode 1147.000000, reward total was -21.000000. running mean: -20.469020\n",
            "resetting env. episode 1148.000000, reward total was -19.000000. running mean: -20.454330\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.459786\n",
            "resetting env. episode 1150.000000, reward total was -21.000000. running mean: -20.465188\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -20.470536\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -20.475831\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -20.481073\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.486262\n",
            "resetting env. episode 1155.000000, reward total was -19.000000. running mean: -20.471399\n",
            "resetting env. episode 1156.000000, reward total was -20.000000. running mean: -20.466685\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -20.472019\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.477298\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -20.472525\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -20.477800\n",
            "resetting env. episode 1161.000000, reward total was -20.000000. running mean: -20.473022\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -20.478292\n",
            "resetting env. episode 1163.000000, reward total was -21.000000. running mean: -20.483509\n",
            "resetting env. episode 1164.000000, reward total was -21.000000. running mean: -20.488674\n",
            "resetting env. episode 1165.000000, reward total was -20.000000. running mean: -20.483787\n",
            "resetting env. episode 1166.000000, reward total was -20.000000. running mean: -20.478949\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -20.484160\n",
            "resetting env. episode 1168.000000, reward total was -20.000000. running mean: -20.479318\n",
            "resetting env. episode 1169.000000, reward total was -18.000000. running mean: -20.454525\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.459980\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -20.465380\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -20.460726\n",
            "resetting env. episode 1173.000000, reward total was -17.000000. running mean: -20.426119\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -20.431858\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -20.437539\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -20.443164\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -20.448732\n",
            "resetting env. episode 1178.000000, reward total was -21.000000. running mean: -20.454245\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -20.459702\n",
            "resetting env. episode 1180.000000, reward total was -20.000000. running mean: -20.455105\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -20.460554\n",
            "resetting env. episode 1182.000000, reward total was -21.000000. running mean: -20.465949\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -20.471289\n",
            "resetting env. episode 1184.000000, reward total was -19.000000. running mean: -20.456576\n",
            "resetting env. episode 1185.000000, reward total was -20.000000. running mean: -20.452011\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.457491\n",
            "resetting env. episode 1187.000000, reward total was -20.000000. running mean: -20.452916\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -20.458386\n",
            "resetting env. episode 1189.000000, reward total was -20.000000. running mean: -20.453803\n",
            "resetting env. episode 1190.000000, reward total was -20.000000. running mean: -20.449265\n",
            "resetting env. episode 1191.000000, reward total was -21.000000. running mean: -20.454772\n",
            "resetting env. episode 1192.000000, reward total was -20.000000. running mean: -20.450224\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.455722\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -20.461165\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.466553\n",
            "resetting env. episode 1196.000000, reward total was -16.000000. running mean: -20.421888\n",
            "resetting env. episode 1197.000000, reward total was -20.000000. running mean: -20.417669\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -20.423492\n",
            "resetting env. episode 1199.000000, reward total was -20.000000. running mean: -20.419257\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.425065\n",
            "resetting env. episode 1201.000000, reward total was -20.000000. running mean: -20.420814\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.426606\n",
            "resetting env. episode 1203.000000, reward total was -19.000000. running mean: -20.412340\n",
            "resetting env. episode 1204.000000, reward total was -20.000000. running mean: -20.408216\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -20.414134\n",
            "resetting env. episode 1206.000000, reward total was -21.000000. running mean: -20.419993\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -20.405793\n",
            "resetting env. episode 1208.000000, reward total was -20.000000. running mean: -20.401735\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -20.407718\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.413640\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -20.419504\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -20.425309\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -20.431056\n",
            "resetting env. episode 1214.000000, reward total was -21.000000. running mean: -20.436745\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -20.442378\n",
            "resetting env. episode 1216.000000, reward total was -21.000000. running mean: -20.447954\n",
            "resetting env. episode 1217.000000, reward total was -21.000000. running mean: -20.453475\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -20.458940\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -20.464350\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -20.469707\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -20.475010\n",
            "resetting env. episode 1222.000000, reward total was -21.000000. running mean: -20.480260\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -20.485457\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -20.490603\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -20.485697\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -20.490840\n",
            "resetting env. episode 1227.000000, reward total was -20.000000. running mean: -20.485931\n",
            "resetting env. episode 1228.000000, reward total was -20.000000. running mean: -20.481072\n",
            "resetting env. episode 1229.000000, reward total was -20.000000. running mean: -20.476261\n",
            "resetting env. episode 1230.000000, reward total was -20.000000. running mean: -20.471499\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -20.476784\n",
            "resetting env. episode 1232.000000, reward total was -20.000000. running mean: -20.472016\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -20.477296\n",
            "resetting env. episode 1234.000000, reward total was -17.000000. running mean: -20.442523\n",
            "resetting env. episode 1235.000000, reward total was -21.000000. running mean: -20.448097\n",
            "resetting env. episode 1236.000000, reward total was -19.000000. running mean: -20.433616\n",
            "resetting env. episode 1237.000000, reward total was -20.000000. running mean: -20.429280\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -20.434987\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -20.440638\n",
            "resetting env. episode 1240.000000, reward total was -21.000000. running mean: -20.446231\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -20.451769\n",
            "resetting env. episode 1242.000000, reward total was -20.000000. running mean: -20.447251\n",
            "resetting env. episode 1243.000000, reward total was -21.000000. running mean: -20.452779\n",
            "resetting env. episode 1244.000000, reward total was -20.000000. running mean: -20.448251\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -20.453768\n",
            "resetting env. episode 1246.000000, reward total was -20.000000. running mean: -20.449231\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -20.454738\n",
            "resetting env. episode 1248.000000, reward total was -20.000000. running mean: -20.450191\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.455689\n",
            "resetting env. episode 1250.000000, reward total was -21.000000. running mean: -20.461132\n",
            "resetting env. episode 1251.000000, reward total was -21.000000. running mean: -20.466521\n",
            "resetting env. episode 1252.000000, reward total was -19.000000. running mean: -20.451856\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -20.457337\n",
            "resetting env. episode 1254.000000, reward total was -20.000000. running mean: -20.452764\n",
            "resetting env. episode 1255.000000, reward total was -21.000000. running mean: -20.458236\n",
            "resetting env. episode 1256.000000, reward total was -20.000000. running mean: -20.453654\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -20.459117\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -20.464526\n",
            "resetting env. episode 1259.000000, reward total was -18.000000. running mean: -20.439881\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -20.445482\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.451027\n",
            "resetting env. episode 1262.000000, reward total was -20.000000. running mean: -20.446517\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -20.452052\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -20.457531\n",
            "resetting env. episode 1265.000000, reward total was -19.000000. running mean: -20.442956\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.448526\n",
            "resetting env. episode 1267.000000, reward total was -20.000000. running mean: -20.444041\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -20.439601\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.445205\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -20.450753\n",
            "resetting env. episode 1271.000000, reward total was -21.000000. running mean: -20.456245\n",
            "resetting env. episode 1272.000000, reward total was -21.000000. running mean: -20.461683\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -20.467066\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -20.472395\n",
            "resetting env. episode 1275.000000, reward total was -21.000000. running mean: -20.477671\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -20.482894\n",
            "resetting env. episode 1277.000000, reward total was -20.000000. running mean: -20.478066\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -20.483285\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -20.488452\n",
            "resetting env. episode 1280.000000, reward total was -21.000000. running mean: -20.493567\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -20.498632\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.503645\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -20.508609\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -20.513523\n",
            "resetting env. episode 1285.000000, reward total was -20.000000. running mean: -20.508388\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -20.513304\n",
            "resetting env. episode 1287.000000, reward total was -19.000000. running mean: -20.498171\n",
            "resetting env. episode 1288.000000, reward total was -21.000000. running mean: -20.503189\n",
            "resetting env. episode 1289.000000, reward total was -20.000000. running mean: -20.498157\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -20.503176\n",
            "resetting env. episode 1291.000000, reward total was -19.000000. running mean: -20.488144\n",
            "resetting env. episode 1292.000000, reward total was -18.000000. running mean: -20.463262\n",
            "resetting env. episode 1293.000000, reward total was -20.000000. running mean: -20.458630\n",
            "resetting env. episode 1294.000000, reward total was -19.000000. running mean: -20.444044\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -20.449603\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.455107\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -20.460556\n",
            "resetting env. episode 1298.000000, reward total was -20.000000. running mean: -20.455950\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -20.461391\n",
            "resetting env. episode 1300.000000, reward total was -19.000000. running mean: -20.446777\n",
            "resetting env. episode 1301.000000, reward total was -20.000000. running mean: -20.442309\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -20.447886\n",
            "resetting env. episode 1303.000000, reward total was -18.000000. running mean: -20.423407\n",
            "resetting env. episode 1304.000000, reward total was -19.000000. running mean: -20.409173\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.415081\n",
            "resetting env. episode 1306.000000, reward total was -21.000000. running mean: -20.420931\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -20.426721\n",
            "resetting env. episode 1308.000000, reward total was -21.000000. running mean: -20.432454\n",
            "resetting env. episode 1309.000000, reward total was -21.000000. running mean: -20.438130\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -20.443748\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.449311\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -20.454818\n",
            "resetting env. episode 1313.000000, reward total was -21.000000. running mean: -20.460270\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -20.465667\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -20.461010\n",
            "resetting env. episode 1316.000000, reward total was -19.000000. running mean: -20.446400\n",
            "resetting env. episode 1317.000000, reward total was -20.000000. running mean: -20.441936\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -20.447517\n",
            "resetting env. episode 1319.000000, reward total was -20.000000. running mean: -20.443042\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -20.448611\n",
            "resetting env. episode 1321.000000, reward total was -21.000000. running mean: -20.454125\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -20.449584\n",
            "resetting env. episode 1323.000000, reward total was -19.000000. running mean: -20.435088\n",
            "resetting env. episode 1324.000000, reward total was -21.000000. running mean: -20.440737\n",
            "resetting env. episode 1325.000000, reward total was -19.000000. running mean: -20.426330\n",
            "resetting env. episode 1326.000000, reward total was -20.000000. running mean: -20.422066\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.427846\n",
            "resetting env. episode 1328.000000, reward total was -20.000000. running mean: -20.423567\n",
            "resetting env. episode 1329.000000, reward total was -21.000000. running mean: -20.429332\n",
            "resetting env. episode 1330.000000, reward total was -21.000000. running mean: -20.435038\n",
            "resetting env. episode 1331.000000, reward total was -21.000000. running mean: -20.440688\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -20.446281\n",
            "resetting env. episode 1333.000000, reward total was -20.000000. running mean: -20.441818\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.447400\n",
            "resetting env. episode 1335.000000, reward total was -20.000000. running mean: -20.442926\n",
            "resetting env. episode 1336.000000, reward total was -17.000000. running mean: -20.408497\n",
            "resetting env. episode 1337.000000, reward total was -21.000000. running mean: -20.414412\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -20.420268\n",
            "resetting env. episode 1339.000000, reward total was -21.000000. running mean: -20.426065\n",
            "resetting env. episode 1340.000000, reward total was -17.000000. running mean: -20.391804\n",
            "resetting env. episode 1341.000000, reward total was -20.000000. running mean: -20.387886\n",
            "resetting env. episode 1342.000000, reward total was -19.000000. running mean: -20.374007\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -20.380267\n",
            "resetting env. episode 1344.000000, reward total was -21.000000. running mean: -20.386465\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.392600\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -20.398674\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -20.404687\n",
            "resetting env. episode 1348.000000, reward total was -21.000000. running mean: -20.410640\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -20.406534\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -20.412469\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -20.408344\n",
            "resetting env. episode 1352.000000, reward total was -21.000000. running mean: -20.414261\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.420118\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -20.425917\n",
            "resetting env. episode 1355.000000, reward total was -20.000000. running mean: -20.421658\n",
            "resetting env. episode 1356.000000, reward total was -20.000000. running mean: -20.417441\n",
            "resetting env. episode 1357.000000, reward total was -19.000000. running mean: -20.403267\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -20.409234\n",
            "resetting env. episode 1359.000000, reward total was -21.000000. running mean: -20.415142\n",
            "resetting env. episode 1360.000000, reward total was -20.000000. running mean: -20.410990\n",
            "resetting env. episode 1361.000000, reward total was -19.000000. running mean: -20.396880\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -20.402911\n",
            "resetting env. episode 1363.000000, reward total was -21.000000. running mean: -20.408882\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -20.414794\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -20.420646\n",
            "resetting env. episode 1366.000000, reward total was -20.000000. running mean: -20.416439\n",
            "resetting env. episode 1367.000000, reward total was -19.000000. running mean: -20.402275\n",
            "resetting env. episode 1368.000000, reward total was -21.000000. running mean: -20.408252\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -20.414170\n",
            "resetting env. episode 1370.000000, reward total was -20.000000. running mean: -20.410028\n",
            "resetting env. episode 1371.000000, reward total was -20.000000. running mean: -20.405928\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -20.411868\n",
            "resetting env. episode 1373.000000, reward total was -21.000000. running mean: -20.417750\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.423572\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -20.429336\n",
            "resetting env. episode 1376.000000, reward total was -19.000000. running mean: -20.415043\n",
            "resetting env. episode 1377.000000, reward total was -20.000000. running mean: -20.410893\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -20.416784\n",
            "resetting env. episode 1379.000000, reward total was -21.000000. running mean: -20.422616\n",
            "resetting env. episode 1380.000000, reward total was -21.000000. running mean: -20.428390\n",
            "resetting env. episode 1381.000000, reward total was -19.000000. running mean: -20.414106\n",
            "resetting env. episode 1382.000000, reward total was -19.000000. running mean: -20.399965\n",
            "resetting env. episode 1383.000000, reward total was -20.000000. running mean: -20.395965\n",
            "resetting env. episode 1384.000000, reward total was -20.000000. running mean: -20.392005\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -20.398085\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -20.404104\n",
            "resetting env. episode 1387.000000, reward total was -20.000000. running mean: -20.400063\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.406063\n",
            "resetting env. episode 1389.000000, reward total was -18.000000. running mean: -20.382002\n",
            "resetting env. episode 1390.000000, reward total was -19.000000. running mean: -20.368182\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -20.374500\n",
            "resetting env. episode 1392.000000, reward total was -21.000000. running mean: -20.380755\n",
            "resetting env. episode 1393.000000, reward total was -18.000000. running mean: -20.356948\n",
            "resetting env. episode 1394.000000, reward total was -21.000000. running mean: -20.363378\n",
            "resetting env. episode 1395.000000, reward total was -21.000000. running mean: -20.369745\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.376047\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -20.382287\n",
            "resetting env. episode 1398.000000, reward total was -20.000000. running mean: -20.378464\n",
            "resetting env. episode 1399.000000, reward total was -19.000000. running mean: -20.364679\n",
            "resetting env. episode 1400.000000, reward total was -20.000000. running mean: -20.361032\n",
            "resetting env. episode 1401.000000, reward total was -21.000000. running mean: -20.367422\n",
            "resetting env. episode 1402.000000, reward total was -20.000000. running mean: -20.363748\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.370110\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -20.376409\n",
            "resetting env. episode 1405.000000, reward total was -20.000000. running mean: -20.372645\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.378919\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -20.385129\n",
            "resetting env. episode 1408.000000, reward total was -21.000000. running mean: -20.391278\n",
            "resetting env. episode 1409.000000, reward total was -18.000000. running mean: -20.367365\n",
            "resetting env. episode 1410.000000, reward total was -17.000000. running mean: -20.333692\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -20.340355\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.346951\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.353482\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -20.359947\n",
            "resetting env. episode 1415.000000, reward total was -19.000000. running mean: -20.346347\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.352884\n",
            "resetting env. episode 1417.000000, reward total was -21.000000. running mean: -20.359355\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.365762\n",
            "resetting env. episode 1419.000000, reward total was -20.000000. running mean: -20.362104\n",
            "resetting env. episode 1420.000000, reward total was -20.000000. running mean: -20.358483\n",
            "resetting env. episode 1421.000000, reward total was -21.000000. running mean: -20.364898\n",
            "resetting env. episode 1422.000000, reward total was -21.000000. running mean: -20.371249\n",
            "resetting env. episode 1423.000000, reward total was -18.000000. running mean: -20.347537\n",
            "resetting env. episode 1424.000000, reward total was -20.000000. running mean: -20.344061\n",
            "resetting env. episode 1425.000000, reward total was -21.000000. running mean: -20.350621\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -20.357114\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -20.363543\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -20.369908\n",
            "resetting env. episode 1429.000000, reward total was -21.000000. running mean: -20.376209\n",
            "resetting env. episode 1430.000000, reward total was -21.000000. running mean: -20.382447\n",
            "resetting env. episode 1431.000000, reward total was -21.000000. running mean: -20.388622\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -20.394736\n",
            "resetting env. episode 1433.000000, reward total was -20.000000. running mean: -20.390789\n",
            "resetting env. episode 1434.000000, reward total was -18.000000. running mean: -20.366881\n",
            "resetting env. episode 1435.000000, reward total was -20.000000. running mean: -20.363212\n",
            "resetting env. episode 1436.000000, reward total was -21.000000. running mean: -20.369580\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -20.375884\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.382125\n",
            "resetting env. episode 1439.000000, reward total was -21.000000. running mean: -20.388304\n",
            "resetting env. episode 1440.000000, reward total was -20.000000. running mean: -20.384421\n",
            "resetting env. episode 1441.000000, reward total was -20.000000. running mean: -20.380577\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -20.386771\n",
            "resetting env. episode 1443.000000, reward total was -19.000000. running mean: -20.372903\n",
            "resetting env. episode 1444.000000, reward total was -21.000000. running mean: -20.379174\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -20.375382\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.381629\n",
            "resetting env. episode 1447.000000, reward total was -19.000000. running mean: -20.367812\n",
            "resetting env. episode 1448.000000, reward total was -21.000000. running mean: -20.374134\n",
            "resetting env. episode 1449.000000, reward total was -21.000000. running mean: -20.380393\n",
            "resetting env. episode 1450.000000, reward total was -21.000000. running mean: -20.386589\n",
            "resetting env. episode 1451.000000, reward total was -20.000000. running mean: -20.382723\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.388896\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -20.395007\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -20.401057\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -20.407046\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -20.412976\n",
            "resetting env. episode 1457.000000, reward total was -20.000000. running mean: -20.408846\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -20.414758\n",
            "resetting env. episode 1459.000000, reward total was -21.000000. running mean: -20.420610\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -20.426404\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -20.432140\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -20.437818\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -20.443440\n",
            "resetting env. episode 1464.000000, reward total was -20.000000. running mean: -20.439006\n",
            "resetting env. episode 1465.000000, reward total was -21.000000. running mean: -20.444616\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -20.450170\n",
            "resetting env. episode 1467.000000, reward total was -20.000000. running mean: -20.445668\n",
            "resetting env. episode 1468.000000, reward total was -20.000000. running mean: -20.441211\n",
            "resetting env. episode 1469.000000, reward total was -21.000000. running mean: -20.446799\n",
            "resetting env. episode 1470.000000, reward total was -21.000000. running mean: -20.452331\n",
            "resetting env. episode 1471.000000, reward total was -20.000000. running mean: -20.447808\n",
            "resetting env. episode 1472.000000, reward total was -19.000000. running mean: -20.433330\n",
            "resetting env. episode 1473.000000, reward total was -21.000000. running mean: -20.438996\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -20.444607\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -20.450160\n",
            "resetting env. episode 1476.000000, reward total was -18.000000. running mean: -20.425659\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -20.431402\n",
            "resetting env. episode 1478.000000, reward total was -19.000000. running mean: -20.417088\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -20.422917\n",
            "resetting env. episode 1480.000000, reward total was -19.000000. running mean: -20.408688\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -20.414601\n",
            "resetting env. episode 1482.000000, reward total was -20.000000. running mean: -20.410455\n",
            "resetting env. episode 1483.000000, reward total was -21.000000. running mean: -20.416351\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -20.422187\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -20.427965\n",
            "resetting env. episode 1486.000000, reward total was -20.000000. running mean: -20.423686\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.429449\n",
            "resetting env. episode 1488.000000, reward total was -21.000000. running mean: -20.435154\n",
            "resetting env. episode 1489.000000, reward total was -21.000000. running mean: -20.440803\n",
            "resetting env. episode 1490.000000, reward total was -21.000000. running mean: -20.446395\n",
            "resetting env. episode 1491.000000, reward total was -20.000000. running mean: -20.441931\n",
            "resetting env. episode 1492.000000, reward total was -21.000000. running mean: -20.447512\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -20.453036\n",
            "resetting env. episode 1494.000000, reward total was -20.000000. running mean: -20.448506\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -20.454021\n",
            "resetting env. episode 1496.000000, reward total was -21.000000. running mean: -20.459481\n",
            "resetting env. episode 1497.000000, reward total was -19.000000. running mean: -20.444886\n",
            "resetting env. episode 1498.000000, reward total was -20.000000. running mean: -20.440437\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -20.446033\n",
            "resetting env. episode 1500.000000, reward total was -21.000000. running mean: -20.451572\n",
            "CPU times: user 1h 13min 20s, sys: 32min 52s, total: 1h 46min 12s\n",
            "Wall time: 55min 5s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "CteN7XKMVGqg",
        "outputId": "5ac72f4a-63ac-4b26-b2cd-4dcf095eb5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ZYA0HgMoO77a"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# pickle.dump(model, open('model.pkl', 'wb'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "pg_from_scratch_(rate_5).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}