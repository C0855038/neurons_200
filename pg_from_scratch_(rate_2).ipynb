{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l9hHvTk6ec8"
      },
      "source": [
        "# Policy Gradient\n",
        "\n",
        "* http://karpathy.github.io/2016/05/31/rl/\n",
        "* https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
        "* https://github.com/gameofdimension/policy-gradient-pong\n",
        "* https://www.youtube.com/watch?v=tqrcjHuNdmQ\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqkOdLyN9Ylm"
      },
      "source": [
        "## Step 1: Installation for Colab - just execute these cells and do not worry too much\n",
        "\n",
        "* http://nbviewer.jupyter.org/github/patrickmineault/xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb \n",
        "* https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi\n",
        "* https://nyu-cds.github.io/python-mpi/setup/\n",
        "* https://medium.com/@kaleajit27/reinforcement-learning-on-google-colab-9cb2e1ef51e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF9MAVI16huj",
        "outputId": "320b1daf-082a-40f4-dba9-2559e2fec058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install python-opengl -y  >/dev/null\n",
        "!apt install xvfb -y >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fSC11TfN6p69"
      },
      "outputs": [],
      "source": [
        "!pip install pyvirtualdisplay >/dev/null\n",
        "!pip install piglet >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "caiHE2hy6xrf"
      },
      "outputs": [],
      "source": [
        "# from pyvirtualdisplay import Display\n",
        "# display = Display(visible=0, size=(1400, 900))\n",
        "# display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "e1d3b947-4013-479c-a5ff-087f91d84338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=8c047aca7adced65732c69579ad5f1a850e74c5ff4ae8f9540226cbb750eeabd\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "a90beaa9-95fa-4d9b-8177-c6db1a81de07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "f598c109-9a56-4d2d-9000-b1f9a7248a27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "b5bc7900-5a80-43cb-bff9-eb877d3892eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "2d466287-0947-49ee-a66a-bd4435a072ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  env.close()\n",
        "  display_frames_as_gif(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 3 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-2\n",
        "learning_rate = 1e-2\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "30ad8365-9f59-48e3-ae9f-2925c5b8fd37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -19.000000. running mean: -20.980000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.980200\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.980398\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.980594\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.980788\n",
            "resetting env. episode 7.000000, reward total was -19.000000. running mean: -20.960980\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.961370\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.961757\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.962139\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.962518\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.962893\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.953264\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.953731\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.954194\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.954652\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.945105\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.945654\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.946198\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.946736\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.927268\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.927996\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.918716\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.919529\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.920333\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.921130\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.911919\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.912799\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.913671\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.914535\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.915389\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.916235\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.917073\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.917902\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.918723\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.919536\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.920341\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.921137\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.921926\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.922707\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.923480\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.914245\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.915102\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.915951\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.906792\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.897724\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.898747\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.899759\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.900762\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.901754\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.902736\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.893709\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.894772\n",
            "resetting env. episode 54.000000, reward total was -19.000000. running mean: -20.875824\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -20.857066\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.858495\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.859910\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.861311\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.862698\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.864071\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.865431\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.866776\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.868108\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.869427\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.870733\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.872026\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.863306\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.864672\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.856026\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.857465\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.858891\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.860302\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.861699\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.863082\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.854451\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.845907\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.847448\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.848973\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.850483\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.841978\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.843559\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.845123\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.846672\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.848205\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.849723\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.851226\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.852714\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.854186\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.855645\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.857088\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.858517\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.859932\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.861333\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.862719\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.864092\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.865451\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.866797\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.868129\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.869448\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.860753\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.862146\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.863524\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.864889\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.866240\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.867578\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.868902\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.870213\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.861511\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.862896\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.864267\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.865624\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.866968\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.868298\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.869615\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.870919\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.872210\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.873488\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.874753\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.876005\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.877245\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.878473\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.879688\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.880891\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.882082\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.883261\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.884429\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.885584\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.886729\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.887861\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.888983\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.890093\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.881192\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.882380\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.883556\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.884721\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.875873\n",
            "resetting env. episode 137.000000, reward total was -19.000000. running mean: -20.857115\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.858544\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.859958\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.851359\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.842845\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.844417\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.845972\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.847513\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.849038\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.850547\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.842042\n",
            "resetting env. episode 148.000000, reward total was -19.000000. running mean: -20.823621\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.825385\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.827131\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.828860\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.830571\n",
            "resetting env. episode 153.000000, reward total was -19.000000. running mean: -20.812266\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.814143\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.816001\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.807841\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.809763\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.811665\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.813549\n",
            "resetting env. episode 160.000000, reward total was -19.000000. running mean: -20.795413\n",
            "resetting env. episode 161.000000, reward total was -18.000000. running mean: -20.767459\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.769785\n",
            "resetting env. episode 163.000000, reward total was -19.000000. running mean: -20.752087\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.754566\n",
            "resetting env. episode 165.000000, reward total was -19.000000. running mean: -20.737020\n",
            "resetting env. episode 166.000000, reward total was -19.000000. running mean: -20.719650\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.712453\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.705329\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.708276\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.711193\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.704081\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.707040\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.709970\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.712870\n",
            "resetting env. episode 175.000000, reward total was -18.000000. running mean: -20.685741\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.678884\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.672095\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.665374\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.658720\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.652133\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.645612\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.649156\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.652664\n",
            "resetting env. episode 184.000000, reward total was -17.000000. running mean: -20.616138\n",
            "resetting env. episode 185.000000, reward total was -18.000000. running mean: -20.589976\n",
            "resetting env. episode 186.000000, reward total was -18.000000. running mean: -20.564076\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.568436\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.562751\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.567124\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.571453\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.575738\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.579981\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.584181\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.588339\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.582456\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.576631\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.580865\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.585056\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.589206\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.583314\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.577480\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.581706\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.585889\n",
            "resetting env. episode 204.000000, reward total was -18.000000. running mean: -20.560030\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.544429\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.548985\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.543495\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.538060\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.542680\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.547253\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.541780\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.536363\n",
            "resetting env. episode 213.000000, reward total was -18.000000. running mean: -20.510999\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.515889\n",
            "resetting env. episode 215.000000, reward total was -19.000000. running mean: -20.500730\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.505723\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.510665\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.515559\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.520403\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.515199\n",
            "resetting env. episode 221.000000, reward total was -18.000000. running mean: -20.490047\n",
            "resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.475147\n",
            "resetting env. episode 223.000000, reward total was -18.000000. running mean: -20.450395\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -20.435891\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.431532\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.427217\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.432945\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.428615\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.424329\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.430086\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.435785\n",
            "resetting env. episode 232.000000, reward total was -17.000000. running mean: -20.401427\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.397413\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.403439\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.399405\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.405410\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.411356\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.417243\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.423070\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.418840\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.404651\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.410605\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.416499\n",
            "resetting env. episode 244.000000, reward total was -19.000000. running mean: -20.402334\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.408310\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.414227\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.420085\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.425884\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.431625\n",
            "resetting env. episode 250.000000, reward total was -19.000000. running mean: -20.417309\n",
            "resetting env. episode 251.000000, reward total was -19.000000. running mean: -20.403136\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.409105\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.415014\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.420863\n",
            "resetting env. episode 255.000000, reward total was -19.000000. running mean: -20.406655\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.412588\n",
            "resetting env. episode 257.000000, reward total was -19.000000. running mean: -20.398462\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.394478\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.400533\n",
            "resetting env. episode 260.000000, reward total was -17.000000. running mean: -20.366528\n",
            "resetting env. episode 261.000000, reward total was -19.000000. running mean: -20.352862\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.359334\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.355740\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.352183\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.348661\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.345175\n",
            "resetting env. episode 267.000000, reward total was -19.000000. running mean: -20.331723\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.338406\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.345022\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.351571\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.358056\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.364475\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.360830\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.347222\n",
            "resetting env. episode 275.000000, reward total was -19.000000. running mean: -20.333750\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.340412\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.347008\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.343538\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.350103\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.356602\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.353036\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.349505\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.356010\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.352450\n",
            "resetting env. episode 285.000000, reward total was -18.000000. running mean: -20.328926\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.325636\n",
            "resetting env. episode 287.000000, reward total was -18.000000. running mean: -20.302380\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.309356\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.306263\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.313200\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.310068\n",
            "resetting env. episode 292.000000, reward total was -18.000000. running mean: -20.286967\n",
            "resetting env. episode 293.000000, reward total was -19.000000. running mean: -20.274098\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.281357\n",
            "resetting env. episode 295.000000, reward total was -17.000000. running mean: -20.248543\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.246058\n",
            "resetting env. episode 297.000000, reward total was -18.000000. running mean: -20.223597\n",
            "resetting env. episode 298.000000, reward total was -19.000000. running mean: -20.211361\n",
            "resetting env. episode 299.000000, reward total was -18.000000. running mean: -20.189248\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.197355\n",
            "resetting env. episode 301.000000, reward total was -19.000000. running mean: -20.185382\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.193528\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.201592\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.189576\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.197681\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.195704\n",
            "resetting env. episode 307.000000, reward total was -19.000000. running mean: -20.183747\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.181909\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.170090\n",
            "resetting env. episode 310.000000, reward total was -19.000000. running mean: -20.158389\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.166806\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -20.155137\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.163586\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -20.151950\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.150431\n",
            "resetting env. episode 316.000000, reward total was -18.000000. running mean: -20.128926\n",
            "resetting env. episode 317.000000, reward total was -17.000000. running mean: -20.097637\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.106661\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.115594\n",
            "resetting env. episode 320.000000, reward total was -19.000000. running mean: -20.104438\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.113394\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.122260\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.131037\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.139727\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.148330\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.146846\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.155378\n",
            "resetting env. episode 328.000000, reward total was -17.000000. running mean: -20.123824\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.132586\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.141260\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.149847\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.158349\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.156765\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.155198\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.153646\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.152109\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.160588\n",
            "resetting env. episode 338.000000, reward total was -18.000000. running mean: -20.138982\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.127593\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.136317\n",
            "resetting env. episode 341.000000, reward total was -18.000000. running mean: -20.114953\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.123804\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.122566\n",
            "resetting env. episode 344.000000, reward total was -19.000000. running mean: -20.111340\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.110227\n",
            "resetting env. episode 346.000000, reward total was -19.000000. running mean: -20.099125\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.108133\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.117052\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.125881\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.124623\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.133376\n",
            "resetting env. episode 352.000000, reward total was -18.000000. running mean: -20.112043\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -20.100922\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.099913\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.108914\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.117825\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.116647\n",
            "resetting env. episode 358.000000, reward total was -19.000000. running mean: -20.105480\n",
            "resetting env. episode 359.000000, reward total was -19.000000. running mean: -20.094425\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.103481\n",
            "resetting env. episode 361.000000, reward total was -19.000000. running mean: -20.092446\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.091522\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.100606\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.099600\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.108604\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -20.097518\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.106543\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.115478\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.114323\n",
            "resetting env. episode 370.000000, reward total was -19.000000. running mean: -20.103180\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.112148\n",
            "resetting env. episode 372.000000, reward total was -19.000000. running mean: -20.101026\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.110016\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.118916\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.127727\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.126450\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.125185\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.123933\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.132694\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.141367\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.139953\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.138554\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.147168\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.155697\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.164140\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.172498\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.180773\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.188966\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.187076\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.185205\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.183353\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.191520\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.189604\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.187708\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.185831\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.193973\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.202033\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.200013\n",
            "resetting env. episode 399.000000, reward total was -19.000000. running mean: -20.188013\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.196133\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.204171\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.202130\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.200108\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.198107\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.206126\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.214065\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.221924\n",
            "resetting env. episode 408.000000, reward total was -17.000000. running mean: -20.189705\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.187808\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.185930\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.194071\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.202130\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.210108\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.198007\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.206027\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.203967\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.211927\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.209808\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.207710\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.215633\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.213477\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.221342\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.229128\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.226837\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.214569\n",
            "resetting env. episode 426.000000, reward total was -18.000000. running mean: -20.192423\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -20.180499\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.188694\n",
            "resetting env. episode 429.000000, reward total was -19.000000. running mean: -20.176807\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.185039\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.183188\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.191357\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.199443\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.197449\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.195474\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.193519\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.201584\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.209568\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.207473\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.215398\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.213244\n",
            "resetting env. episode 442.000000, reward total was -18.000000. running mean: -20.191111\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.199200\n",
            "resetting env. episode 444.000000, reward total was -19.000000. running mean: -20.187208\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.195336\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.183383\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.181549\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.169734\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.168036\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.176356\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.184592\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.182746\n",
            "resetting env. episode 453.000000, reward total was -19.000000. running mean: -20.170919\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.179210\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.177418\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.175643\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -20.163887\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -20.152248\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.160726\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.169118\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.167427\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.165753\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.164095\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.162455\n",
            "resetting env. episode 465.000000, reward total was -19.000000. running mean: -20.150830\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.159322\n",
            "resetting env. episode 467.000000, reward total was -18.000000. running mean: -20.137728\n",
            "resetting env. episode 468.000000, reward total was -17.000000. running mean: -20.106351\n",
            "resetting env. episode 469.000000, reward total was -19.000000. running mean: -20.095288\n",
            "resetting env. episode 470.000000, reward total was -15.000000. running mean: -20.044335\n",
            "resetting env. episode 471.000000, reward total was -19.000000. running mean: -20.033891\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.043553\n",
            "resetting env. episode 473.000000, reward total was -17.000000. running mean: -20.013117\n",
            "resetting env. episode 474.000000, reward total was -19.000000. running mean: -20.002986\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.002956\n",
            "resetting env. episode 476.000000, reward total was -19.000000. running mean: -19.992926\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -19.992997\n",
            "resetting env. episode 478.000000, reward total was -19.000000. running mean: -19.983067\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -19.983236\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -19.993404\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -19.993470\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.003535\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.013500\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -20.003365\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.003331\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.003298\n",
            "resetting env. episode 487.000000, reward total was -18.000000. running mean: -19.983265\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -19.983432\n",
            "resetting env. episode 489.000000, reward total was -17.000000. running mean: -19.953598\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -19.954062\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -19.954522\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -19.954976\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -19.965427\n",
            "resetting env. episode 494.000000, reward total was -18.000000. running mean: -19.945772\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -19.936315\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -19.926951\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -19.927682\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -19.938405\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -19.939021\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -19.949631\n",
            "CPU times: user 31min 17s, sys: 13min 24s, total: 44min 42s\n",
            "Wall time: 23min 10s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCZR5OV-z-YJ",
        "outputId": "a8b80842-cf56-42cb-e0fc-793b9b51fb38"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -20.970199\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.960497\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.950892\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.951383\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.941869\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.942451\n",
            "resetting env. episode 11.000000, reward total was -18.000000. running mean: -20.913026\n",
            "resetting env. episode 12.000000, reward total was -18.000000. running mean: -20.883896\n",
            "resetting env. episode 13.000000, reward total was -18.000000. running mean: -20.855057\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.846506\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.848041\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.849561\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.851065\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.842555\n",
            "resetting env. episode 19.000000, reward total was -19.000000. running mean: -20.824129\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.815888\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.797729\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.789752\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.781854\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.774036\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.766295\n",
            "resetting env. episode 26.000000, reward total was -17.000000. running mean: -20.728632\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.731346\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.724032\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.716792\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.719624\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.722428\n",
            "resetting env. episode 32.000000, reward total was -17.000000. running mean: -20.685204\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.688352\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.691468\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -20.674553\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -20.657808\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.661230\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.664618\n",
            "resetting env. episode 39.000000, reward total was -19.000000. running mean: -20.647971\n",
            "resetting env. episode 40.000000, reward total was -19.000000. running mean: -20.631492\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.635177\n",
            "resetting env. episode 42.000000, reward total was -18.000000. running mean: -20.608825\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.602737\n",
            "resetting env. episode 44.000000, reward total was -19.000000. running mean: -20.586709\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.570842\n",
            "resetting env. episode 46.000000, reward total was -19.000000. running mean: -20.555134\n",
            "resetting env. episode 47.000000, reward total was -19.000000. running mean: -20.539582\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.534187\n",
            "resetting env. episode 49.000000, reward total was -19.000000. running mean: -20.518845\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.513656\n",
            "resetting env. episode 51.000000, reward total was -18.000000. running mean: -20.488520\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.483635\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -20.468798\n",
            "resetting env. episode 54.000000, reward total was -19.000000. running mean: -20.454110\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.449569\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.455073\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.460523\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.465917\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -20.451258\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.446746\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.442278\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.447855\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.443377\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.438943\n",
            "resetting env. episode 65.000000, reward total was -18.000000. running mean: -20.414554\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.410408\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.406304\n",
            "resetting env. episode 68.000000, reward total was -16.000000. running mean: -20.362241\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.368619\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.364932\n",
            "resetting env. episode 71.000000, reward total was -19.000000. running mean: -20.351283\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.347770\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.354293\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.360750\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.367142\n",
            "resetting env. episode 76.000000, reward total was -19.000000. running mean: -20.353471\n",
            "resetting env. episode 77.000000, reward total was -18.000000. running mean: -20.329936\n",
            "resetting env. episode 78.000000, reward total was -17.000000. running mean: -20.296637\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.303670\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.300634\n",
            "resetting env. episode 81.000000, reward total was -17.000000. running mean: -20.267627\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.274951\n",
            "resetting env. episode 83.000000, reward total was -19.000000. running mean: -20.262202\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -20.249579\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.257084\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.254513\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.251968\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.259448\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.256854\n",
            "resetting env. episode 90.000000, reward total was -18.000000. running mean: -20.234285\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -20.221942\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.219723\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.217526\n",
            "resetting env. episode 94.000000, reward total was -16.000000. running mean: -20.175350\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -20.163597\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.161961\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.150341\n",
            "resetting env. episode 98.000000, reward total was -18.000000. running mean: -20.128838\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.127549\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.126274\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -20.115011\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.113861\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.122722\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.131495\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.130180\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.128878\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.127590\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.126314\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.125051\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.133800\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -20.122462\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.111238\n",
            "resetting env. episode 113.000000, reward total was -18.000000. running mean: -20.090125\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.099224\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.098232\n",
            "resetting env. episode 116.000000, reward total was -19.000000. running mean: -20.087249\n",
            "resetting env. episode 117.000000, reward total was -19.000000. running mean: -20.076377\n",
            "resetting env. episode 118.000000, reward total was -16.000000. running mean: -20.035613\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.045257\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -20.034804\n",
            "resetting env. episode 121.000000, reward total was -18.000000. running mean: -20.014456\n",
            "resetting env. episode 122.000000, reward total was -19.000000. running mean: -20.004312\n",
            "resetting env. episode 123.000000, reward total was -18.000000. running mean: -19.984269\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -19.984426\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -19.984582\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -19.994736\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.004789\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.004741\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.014693\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.024546\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.024301\n",
            "resetting env. episode 132.000000, reward total was -18.000000. running mean: -20.004058\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.014017\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.023877\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.033638\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.043302\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.052869\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.062340\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.071717\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.071000\n",
            "resetting env. episode 141.000000, reward total was -15.000000. running mean: -20.020290\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.030087\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.039786\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.049388\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.048894\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.058405\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.057821\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.067243\n",
            "resetting env. episode 149.000000, reward total was -18.000000. running mean: -20.046571\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.056105\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.055544\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.054988\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.054438\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.063894\n",
            "resetting env. episode 155.000000, reward total was -19.000000. running mean: -20.053255\n",
            "resetting env. episode 156.000000, reward total was -18.000000. running mean: -20.032723\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.032395\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.042071\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.041651\n",
            "resetting env. episode 160.000000, reward total was -18.000000. running mean: -20.021234\n",
            "resetting env. episode 161.000000, reward total was -16.000000. running mean: -19.981022\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -19.981212\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -19.991399\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -19.991486\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.001571\n",
            "resetting env. episode 166.000000, reward total was -19.000000. running mean: -19.991555\n",
            "resetting env. episode 167.000000, reward total was -16.000000. running mean: -19.951639\n",
            "resetting env. episode 168.000000, reward total was -17.000000. running mean: -19.922123\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -19.932902\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -19.943573\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -19.954137\n",
            "resetting env. episode 172.000000, reward total was -19.000000. running mean: -19.944596\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -19.955150\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -19.965598\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -19.965942\n",
            "resetting env. episode 176.000000, reward total was -16.000000. running mean: -19.926283\n",
            "resetting env. episode 177.000000, reward total was -18.000000. running mean: -19.907020\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -19.907950\n",
            "resetting env. episode 179.000000, reward total was -19.000000. running mean: -19.898870\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -19.909882\n",
            "resetting env. episode 181.000000, reward total was -19.000000. running mean: -19.900783\n",
            "resetting env. episode 182.000000, reward total was -16.000000. running mean: -19.861775\n",
            "resetting env. episode 183.000000, reward total was -17.000000. running mean: -19.833157\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -19.844826\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -19.836377\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -19.838014\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -19.829633\n",
            "resetting env. episode 188.000000, reward total was -19.000000. running mean: -19.821337\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -19.823124\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -19.834892\n",
            "resetting env. episode 191.000000, reward total was -19.000000. running mean: -19.826544\n",
            "resetting env. episode 192.000000, reward total was -19.000000. running mean: -19.818278\n",
            "resetting env. episode 193.000000, reward total was -18.000000. running mean: -19.800095\n",
            "resetting env. episode 194.000000, reward total was -16.000000. running mean: -19.762094\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -19.774473\n",
            "resetting env. episode 196.000000, reward total was -16.000000. running mean: -19.736729\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -19.739361\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -19.751968\n",
            "resetting env. episode 199.000000, reward total was -17.000000. running mean: -19.724448\n",
            "resetting env. episode 200.000000, reward total was -17.000000. running mean: -19.697204\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -19.710232\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -19.723129\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -19.715898\n",
            "resetting env. episode 204.000000, reward total was -17.000000. running mean: -19.688739\n",
            "resetting env. episode 205.000000, reward total was -18.000000. running mean: -19.671852\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -19.685133\n",
            "resetting env. episode 207.000000, reward total was -17.000000. running mean: -19.658282\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -19.671699\n",
            "resetting env. episode 209.000000, reward total was -14.000000. running mean: -19.614982\n",
            "resetting env. episode 210.000000, reward total was -19.000000. running mean: -19.608832\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -19.622744\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -19.636516\n",
            "resetting env. episode 213.000000, reward total was -18.000000. running mean: -19.620151\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -19.633950\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -19.647610\n",
            "resetting env. episode 216.000000, reward total was -17.000000. running mean: -19.621134\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -19.634923\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -19.648574\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -19.662088\n",
            "resetting env. episode 220.000000, reward total was -14.000000. running mean: -19.605467\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -19.619412\n",
            "resetting env. episode 222.000000, reward total was -16.000000. running mean: -19.583218\n",
            "resetting env. episode 223.000000, reward total was -18.000000. running mean: -19.567386\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -19.571712\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -19.575995\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -19.590235\n",
            "resetting env. episode 227.000000, reward total was -18.000000. running mean: -19.574333\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -19.588589\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -19.592703\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -19.606776\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -19.610709\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -19.624602\n",
            "resetting env. episode 233.000000, reward total was -19.000000. running mean: -19.618356\n",
            "resetting env. episode 234.000000, reward total was -18.000000. running mean: -19.602172\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -19.596150\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -19.610189\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -19.604087\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -19.618046\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -19.631866\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -19.635547\n",
            "resetting env. episode 241.000000, reward total was -18.000000. running mean: -19.619191\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -19.633000\n",
            "resetting env. episode 243.000000, reward total was -18.000000. running mean: -19.616670\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -19.620503\n",
            "resetting env. episode 245.000000, reward total was -18.000000. running mean: -19.604298\n",
            "resetting env. episode 246.000000, reward total was -17.000000. running mean: -19.578255\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -19.582472\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -19.596648\n",
            "resetting env. episode 249.000000, reward total was -18.000000. running mean: -19.580681\n",
            "resetting env. episode 250.000000, reward total was -15.000000. running mean: -19.534874\n",
            "resetting env. episode 251.000000, reward total was -16.000000. running mean: -19.499526\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -19.504530\n",
            "resetting env. episode 253.000000, reward total was -18.000000. running mean: -19.489485\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -19.504590\n",
            "resetting env. episode 255.000000, reward total was -19.000000. running mean: -19.499544\n",
            "resetting env. episode 256.000000, reward total was -19.000000. running mean: -19.494549\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -19.499603\n",
            "resetting env. episode 258.000000, reward total was -19.000000. running mean: -19.494607\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -19.489661\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -19.504765\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -19.519717\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -19.524520\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -19.529275\n",
            "resetting env. episode 264.000000, reward total was -15.000000. running mean: -19.483982\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -19.489142\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -19.494251\n",
            "resetting env. episode 267.000000, reward total was -19.000000. running mean: -19.489308\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -19.494415\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -19.499471\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -19.514476\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -19.519331\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -19.524138\n",
            "resetting env. episode 273.000000, reward total was -16.000000. running mean: -19.488897\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -19.504008\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -19.508968\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -19.523878\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -19.528639\n",
            "resetting env. episode 278.000000, reward total was -19.000000. running mean: -19.523353\n",
            "resetting env. episode 279.000000, reward total was -19.000000. running mean: -19.518119\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -19.532938\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -19.547609\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -19.552133\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -19.556611\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -19.571045\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -19.585335\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -19.599481\n",
            "resetting env. episode 287.000000, reward total was -17.000000. running mean: -19.573487\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -19.587752\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -19.591874\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -19.595955\n",
            "resetting env. episode 291.000000, reward total was -19.000000. running mean: -19.589996\n",
            "resetting env. episode 292.000000, reward total was -19.000000. running mean: -19.584096\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -19.598255\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -19.592272\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -19.606350\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -19.600286\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -19.604283\n",
            "resetting env. episode 298.000000, reward total was -19.000000. running mean: -19.598240\n",
            "resetting env. episode 299.000000, reward total was -17.000000. running mean: -19.572258\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -19.586536\n",
            "resetting env. episode 301.000000, reward total was -19.000000. running mean: -19.580670\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -19.594863\n",
            "resetting env. episode 303.000000, reward total was -19.000000. running mean: -19.588915\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -19.583026\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -19.597195\n",
            "resetting env. episode 306.000000, reward total was -18.000000. running mean: -19.581223\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -19.595411\n",
            "resetting env. episode 308.000000, reward total was -19.000000. running mean: -19.589457\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -19.593563\n",
            "resetting env. episode 310.000000, reward total was -19.000000. running mean: -19.587627\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -19.591751\n",
            "resetting env. episode 312.000000, reward total was -17.000000. running mean: -19.565833\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -19.580175\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -19.584373\n",
            "resetting env. episode 315.000000, reward total was -18.000000. running mean: -19.568529\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -19.572844\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -19.587116\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -19.601244\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -19.615232\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -19.619080\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -19.632889\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -19.646560\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -19.650094\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -19.663593\n",
            "resetting env. episode 325.000000, reward total was -18.000000. running mean: -19.646958\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -19.660488\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -19.673883\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -19.687144\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -19.680273\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -19.693470\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -19.706535\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -19.719470\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -19.732275\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -19.744953\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -19.757503\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -19.769928\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -19.782229\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -19.794406\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -19.796462\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -19.808498\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -19.820413\n",
            "resetting env. episode 342.000000, reward total was -19.000000. running mean: -19.812209\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -19.824087\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -19.835846\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -19.847487\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -19.859012\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -19.860422\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -19.871818\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -19.873100\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -19.884369\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -19.895525\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -19.896570\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -19.897604\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -19.908628\n",
            "resetting env. episode 355.000000, reward total was -19.000000. running mean: -19.899542\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -19.910546\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -19.921441\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -19.922227\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -19.933004\n",
            "resetting env. episode 360.000000, reward total was -19.000000. running mean: -19.923674\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -19.934438\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -19.945093\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -19.955642\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -19.966086\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -19.976425\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -19.976661\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -19.986894\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -19.987025\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -19.997155\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.007183\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.017112\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.026940\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.036671\n",
            "resetting env. episode 374.000000, reward total was -19.000000. running mean: -20.026304\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.036041\n",
            "resetting env. episode 376.000000, reward total was -19.000000. running mean: -20.025681\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.035424\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.045070\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.054619\n",
            "resetting env. episode 380.000000, reward total was -19.000000. running mean: -20.044073\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.053632\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.053096\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -20.042565\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.052139\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.051618\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.061102\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.070491\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.069786\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.079088\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.078297\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.087514\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.086639\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.095772\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.084815\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.093967\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.103027\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.111997\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.120877\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.129668\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.138371\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.136988\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.145618\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.144162\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.152720\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.151193\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.159681\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.168084\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.166403\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.164739\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.163092\n",
            "resetting env. episode 411.000000, reward total was -19.000000. running mean: -20.151461\n",
            "resetting env. episode 412.000000, reward total was -19.000000. running mean: -20.139946\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.148547\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.157061\n",
            "resetting env. episode 415.000000, reward total was -18.000000. running mean: -20.135491\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.134136\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.142794\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.151366\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.149853\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.158354\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.166771\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.165103\n",
            "resetting env. episode 423.000000, reward total was -18.000000. running mean: -20.143452\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.142017\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.150597\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.149091\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.157600\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.166024\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.174364\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.182620\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.190794\n",
            "resetting env. episode 432.000000, reward total was -18.000000. running mean: -20.168886\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.167197\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.165526\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.163870\n",
            "resetting env. episode 436.000000, reward total was -19.000000. running mean: -20.152232\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.150709\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.159202\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.167610\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.165934\n",
            "resetting env. episode 441.000000, reward total was -17.000000. running mean: -20.134275\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.132932\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.121603\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.120387\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -20.109183\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.108091\n",
            "resetting env. episode 447.000000, reward total was -18.000000. running mean: -20.087010\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.076140\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.075378\n",
            "resetting env. episode 450.000000, reward total was -15.000000. running mean: -20.024625\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.014378\n",
            "resetting env. episode 452.000000, reward total was -19.000000. running mean: -20.004235\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.004192\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.004150\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.014109\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.013968\n",
            "resetting env. episode 457.000000, reward total was -18.000000. running mean: -19.993828\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -19.983890\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -19.984051\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -19.984210\n",
            "resetting env. episode 461.000000, reward total was -17.000000. running mean: -19.954368\n",
            "resetting env. episode 462.000000, reward total was -19.000000. running mean: -19.944825\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -19.945376\n",
            "resetting env. episode 464.000000, reward total was -18.000000. running mean: -19.925923\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -19.936663\n",
            "resetting env. episode 466.000000, reward total was -18.000000. running mean: -19.917297\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -19.918124\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -19.918943\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -19.929753\n",
            "resetting env. episode 470.000000, reward total was -17.000000. running mean: -19.900456\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -19.911451\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -19.922337\n",
            "resetting env. episode 473.000000, reward total was -18.000000. running mean: -19.903113\n",
            "resetting env. episode 474.000000, reward total was -17.000000. running mean: -19.874082\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -19.885341\n",
            "resetting env. episode 476.000000, reward total was -17.000000. running mean: -19.856488\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -19.857923\n",
            "resetting env. episode 478.000000, reward total was -19.000000. running mean: -19.849344\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -19.840850\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -19.842442\n",
            "resetting env. episode 481.000000, reward total was -18.000000. running mean: -19.824017\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -19.825777\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -19.827519\n",
            "resetting env. episode 484.000000, reward total was -18.000000. running mean: -19.809244\n",
            "resetting env. episode 485.000000, reward total was -19.000000. running mean: -19.801152\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -19.803140\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -19.815109\n",
            "resetting env. episode 488.000000, reward total was -18.000000. running mean: -19.796958\n",
            "resetting env. episode 489.000000, reward total was -16.000000. running mean: -19.758988\n",
            "resetting env. episode 490.000000, reward total was -18.000000. running mean: -19.741398\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -19.753984\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -19.756444\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -19.768880\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -19.781191\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -19.783379\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -19.785546\n",
            "resetting env. episode 497.000000, reward total was -19.000000. running mean: -19.777690\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -19.769913\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -19.772214\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -19.764492\n",
            "CPU times: user 33min 58s, sys: 14min 39s, total: 48min 37s\n",
            "Wall time: 25min 10s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "c1906fa3-29e2-4796-86f3-2113729d6426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "a080adfc-9ed8-44b2-a019-5b3e68ddeadd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -18.000000. running mean: -18.000000\n",
            "resetting env. episode 2.000000, reward total was -18.000000. running mean: -18.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -18.030000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -18.059700\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -18.079103\n",
            "resetting env. episode 6.000000, reward total was -18.000000. running mean: -18.078312\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -18.097529\n",
            "resetting env. episode 8.000000, reward total was -19.000000. running mean: -18.106554\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -18.115488\n",
            "resetting env. episode 10.000000, reward total was -14.000000. running mean: -18.074333\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -18.103590\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -18.132554\n",
            "resetting env. episode 13.000000, reward total was -15.000000. running mean: -18.101228\n",
            "resetting env. episode 14.000000, reward total was -17.000000. running mean: -18.090216\n",
            "resetting env. episode 15.000000, reward total was -18.000000. running mean: -18.089314\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -18.108421\n",
            "resetting env. episode 17.000000, reward total was -17.000000. running mean: -18.097337\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -18.116363\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -18.135200\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -18.153848\n",
            "resetting env. episode 21.000000, reward total was -18.000000. running mean: -18.152309\n",
            "resetting env. episode 22.000000, reward total was -15.000000. running mean: -18.120786\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -18.139578\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -18.158182\n",
            "resetting env. episode 25.000000, reward total was -18.000000. running mean: -18.156601\n",
            "resetting env. episode 26.000000, reward total was -19.000000. running mean: -18.165035\n",
            "resetting env. episode 27.000000, reward total was -17.000000. running mean: -18.153384\n",
            "resetting env. episode 28.000000, reward total was -19.000000. running mean: -18.161850\n",
            "resetting env. episode 29.000000, reward total was -16.000000. running mean: -18.140232\n",
            "resetting env. episode 30.000000, reward total was -17.000000. running mean: -18.128830\n",
            "resetting env. episode 31.000000, reward total was -17.000000. running mean: -18.117541\n",
            "resetting env. episode 32.000000, reward total was -17.000000. running mean: -18.106366\n",
            "resetting env. episode 33.000000, reward total was -17.000000. running mean: -18.095302\n",
            "resetting env. episode 34.000000, reward total was -19.000000. running mean: -18.104349\n",
            "resetting env. episode 35.000000, reward total was -17.000000. running mean: -18.093306\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -18.102373\n",
            "resetting env. episode 37.000000, reward total was -19.000000. running mean: -18.111349\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -18.130235\n",
            "resetting env. episode 39.000000, reward total was -18.000000. running mean: -18.128933\n",
            "resetting env. episode 40.000000, reward total was -18.000000. running mean: -18.127644\n",
            "resetting env. episode 41.000000, reward total was -19.000000. running mean: -18.136367\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -18.155004\n",
            "resetting env. episode 43.000000, reward total was -17.000000. running mean: -18.143454\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -18.162019\n",
            "resetting env. episode 45.000000, reward total was -16.000000. running mean: -18.140399\n",
            "resetting env. episode 46.000000, reward total was -17.000000. running mean: -18.128995\n",
            "resetting env. episode 47.000000, reward total was -19.000000. running mean: -18.137705\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -18.156328\n",
            "resetting env. episode 49.000000, reward total was -18.000000. running mean: -18.154765\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -18.173217\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -18.201485\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -18.219470\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -18.247275\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -18.274802\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -18.302054\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -18.329034\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -18.355744\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -18.382186\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -18.408364\n",
            "resetting env. episode 60.000000, reward total was -19.000000. running mean: -18.414281\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -18.430138\n",
            "resetting env. episode 62.000000, reward total was -19.000000. running mean: -18.435836\n",
            "resetting env. episode 63.000000, reward total was -18.000000. running mean: -18.431478\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -18.457163\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -18.482592\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -18.507766\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -18.522688\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -18.547461\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -18.561987\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -18.586367\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -18.600503\n",
            "resetting env. episode 72.000000, reward total was -18.000000. running mean: -18.594498\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -18.618553\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -18.622368\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -18.646144\n",
            "resetting env. episode 76.000000, reward total was -19.000000. running mean: -18.649682\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -18.673186\n",
            "resetting env. episode 78.000000, reward total was -18.000000. running mean: -18.666454\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -18.689789\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -18.692891\n",
            "resetting env. episode 81.000000, reward total was -14.000000. running mean: -18.645962\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -18.659503\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -18.682908\n",
            "resetting env. episode 84.000000, reward total was -18.000000. running mean: -18.676079\n",
            "resetting env. episode 85.000000, reward total was -17.000000. running mean: -18.659318\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -18.682725\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -18.705897\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -18.728838\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -18.751550\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -18.774035\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -18.796294\n",
            "resetting env. episode 92.000000, reward total was -18.000000. running mean: -18.788331\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -18.810448\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -18.832343\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -18.854020\n",
            "resetting env. episode 96.000000, reward total was -17.000000. running mean: -18.835480\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -18.847125\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -18.868654\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -18.889967\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -18.911068\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -18.931957\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -18.942637\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -18.963211\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -18.983579\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.003743\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.023706\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.043469\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.063034\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.082404\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.101580\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -19.120564\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -19.119358\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -19.138165\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.156783\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -19.175215\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.193463\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -19.201528\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -19.209513\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -19.227418\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -19.245144\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.262692\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -19.270065\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -19.277365\n",
            "resetting env. episode 124.000000, reward total was -18.000000. running mean: -19.264591\n",
            "resetting env. episode 125.000000, reward total was -17.000000. running mean: -19.241945\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -19.259526\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -19.276930\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -19.294161\n",
            "resetting env. episode 129.000000, reward total was -18.000000. running mean: -19.281219\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -19.298407\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -19.315423\n",
            "resetting env. episode 132.000000, reward total was -18.000000. running mean: -19.302269\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -19.309246\n",
            "resetting env. episode 134.000000, reward total was -19.000000. running mean: -19.306154\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -19.323092\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -19.329861\n",
            "resetting env. episode 137.000000, reward total was -17.000000. running mean: -19.306563\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -19.323497\n",
            "resetting env. episode 139.000000, reward total was -17.000000. running mean: -19.300262\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -19.307260\n",
            "resetting env. episode 141.000000, reward total was -18.000000. running mean: -19.294187\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -19.311245\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -19.328133\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -19.344851\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -19.351403\n",
            "resetting env. episode 146.000000, reward total was -18.000000. running mean: -19.337889\n",
            "resetting env. episode 147.000000, reward total was -18.000000. running mean: -19.324510\n",
            "resetting env. episode 148.000000, reward total was -11.000000. running mean: -19.241265\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -19.258852\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -19.276264\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -19.293501\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -19.310566\n",
            "resetting env. episode 153.000000, reward total was -19.000000. running mean: -19.307460\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -19.324386\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -19.341142\n",
            "resetting env. episode 156.000000, reward total was -19.000000. running mean: -19.337730\n",
            "resetting env. episode 157.000000, reward total was -16.000000. running mean: -19.304353\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -19.321310\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -19.328096\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -19.344816\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -19.361367\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -19.367754\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -19.384076\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -19.400235\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -19.416233\n",
            "resetting env. episode 166.000000, reward total was -18.000000. running mean: -19.402071\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -19.418050\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -19.423869\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -19.439631\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -19.455234\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -19.470682\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -19.485975\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -19.501116\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -19.516104\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -19.530943\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -19.545634\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -19.560178\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -19.574576\n",
            "resetting env. episode 179.000000, reward total was -18.000000. running mean: -19.558830\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -19.563242\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -19.567609\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -19.581933\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -19.596114\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -19.590153\n",
            "resetting env. episode 185.000000, reward total was -17.000000. running mean: -19.564251\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -19.568609\n",
            "resetting env. episode 187.000000, reward total was -17.000000. running mean: -19.542923\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -19.547493\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -19.552018\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -19.556498\n",
            "resetting env. episode 191.000000, reward total was -17.000000. running mean: -19.530933\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -19.535624\n",
            "resetting env. episode 193.000000, reward total was -17.000000. running mean: -19.510268\n",
            "resetting env. episode 194.000000, reward total was -18.000000. running mean: -19.495165\n",
            "resetting env. episode 195.000000, reward total was -17.000000. running mean: -19.470213\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -19.485511\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -19.500656\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -19.495650\n",
            "resetting env. episode 199.000000, reward total was -19.000000. running mean: -19.490693\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -19.505786\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -19.520728\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -19.525521\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -19.520266\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -19.535063\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -19.549713\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -19.564215\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -19.578573\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -19.592788\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -19.596860\n",
            "resetting env. episode 210.000000, reward total was -18.000000. running mean: -19.580891\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -19.575082\n",
            "resetting env. episode 212.000000, reward total was -15.000000. running mean: -19.529331\n",
            "resetting env. episode 213.000000, reward total was -18.000000. running mean: -19.514038\n",
            "resetting env. episode 214.000000, reward total was -17.000000. running mean: -19.488898\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -19.494009\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -19.499069\n",
            "resetting env. episode 217.000000, reward total was -19.000000. running mean: -19.494078\n",
            "resetting env. episode 218.000000, reward total was -17.000000. running mean: -19.469137\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -19.484446\n",
            "resetting env. episode 220.000000, reward total was -18.000000. running mean: -19.469601\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -19.484905\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -19.500056\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -19.515056\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -19.529905\n",
            "resetting env. episode 225.000000, reward total was -18.000000. running mean: -19.514606\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -19.509460\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -19.524365\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -19.539122\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -19.533731\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -19.538393\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -19.553009\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -19.567479\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -19.581804\n",
            "resetting env. episode 234.000000, reward total was -18.000000. running mean: -19.565986\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -19.580326\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -19.594523\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -19.598578\n",
            "resetting env. episode 238.000000, reward total was -19.000000. running mean: -19.592592\n",
            "resetting env. episode 239.000000, reward total was -18.000000. running mean: -19.576666\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -19.590900\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -19.604991\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -19.598941\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -19.612951\n",
            "resetting env. episode 244.000000, reward total was -19.000000. running mean: -19.606822\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -19.610754\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -19.604646\n",
            "resetting env. episode 247.000000, reward total was -16.000000. running mean: -19.568600\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -19.582914\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -19.597084\n",
            "resetting env. episode 250.000000, reward total was -16.000000. running mean: -19.561114\n",
            "resetting env. episode 251.000000, reward total was -16.000000. running mean: -19.525502\n",
            "resetting env. episode 252.000000, reward total was -19.000000. running mean: -19.520247\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -19.535045\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -19.549695\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -19.564198\n",
            "resetting env. episode 256.000000, reward total was -19.000000. running mean: -19.558556\n",
            "resetting env. episode 257.000000, reward total was -18.000000. running mean: -19.542970\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -19.557540\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -19.571965\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -19.586245\n",
            "resetting env. episode 261.000000, reward total was -18.000000. running mean: -19.570383\n",
            "resetting env. episode 262.000000, reward total was -17.000000. running mean: -19.544679\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -19.559232\n",
            "resetting env. episode 264.000000, reward total was -19.000000. running mean: -19.553640\n",
            "resetting env. episode 265.000000, reward total was -19.000000. running mean: -19.548104\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -19.552622\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -19.557096\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -19.571525\n",
            "resetting env. episode 269.000000, reward total was -18.000000. running mean: -19.555810\n",
            "resetting env. episode 270.000000, reward total was -18.000000. running mean: -19.540252\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -19.544849\n",
            "resetting env. episode 272.000000, reward total was -18.000000. running mean: -19.529401\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -19.534107\n",
            "resetting env. episode 274.000000, reward total was -17.000000. running mean: -19.508766\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -19.523678\n",
            "resetting env. episode 276.000000, reward total was -19.000000. running mean: -19.518441\n",
            "resetting env. episode 277.000000, reward total was -14.000000. running mean: -19.463257\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -19.478624\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -19.483838\n",
            "resetting env. episode 280.000000, reward total was -18.000000. running mean: -19.469000\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -19.484310\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -19.489467\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -19.504572\n",
            "resetting env. episode 284.000000, reward total was -19.000000. running mean: -19.499526\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -19.514531\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -19.529386\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -19.524092\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -19.538851\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -19.553462\n",
            "resetting env. episode 290.000000, reward total was -18.000000. running mean: -19.537928\n",
            "resetting env. episode 291.000000, reward total was -19.000000. running mean: -19.532549\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -19.547223\n",
            "resetting env. episode 293.000000, reward total was -18.000000. running mean: -19.531751\n",
            "resetting env. episode 294.000000, reward total was -17.000000. running mean: -19.506433\n",
            "resetting env. episode 295.000000, reward total was -16.000000. running mean: -19.471369\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -19.466655\n",
            "resetting env. episode 297.000000, reward total was -17.000000. running mean: -19.441989\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -19.447569\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -19.453093\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -19.458562\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -19.463977\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -19.479337\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -19.484543\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -19.489698\n",
            "resetting env. episode 305.000000, reward total was -18.000000. running mean: -19.474801\n",
            "resetting env. episode 306.000000, reward total was -19.000000. running mean: -19.470053\n",
            "resetting env. episode 307.000000, reward total was -19.000000. running mean: -19.465353\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -19.470699\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -19.465992\n",
            "resetting env. episode 310.000000, reward total was -19.000000. running mean: -19.461332\n",
            "resetting env. episode 311.000000, reward total was -18.000000. running mean: -19.446719\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -19.452252\n",
            "resetting env. episode 313.000000, reward total was -18.000000. running mean: -19.437729\n",
            "resetting env. episode 314.000000, reward total was -16.000000. running mean: -19.403352\n",
            "resetting env. episode 315.000000, reward total was -19.000000. running mean: -19.399318\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -19.405325\n",
            "resetting env. episode 317.000000, reward total was -18.000000. running mean: -19.391272\n",
            "resetting env. episode 318.000000, reward total was -17.000000. running mean: -19.367359\n",
            "resetting env. episode 319.000000, reward total was -18.000000. running mean: -19.353686\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -19.370149\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -19.376447\n",
            "resetting env. episode 322.000000, reward total was -18.000000. running mean: -19.362683\n",
            "resetting env. episode 323.000000, reward total was -18.000000. running mean: -19.349056\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -19.355565\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -19.362010\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -19.378390\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -19.384606\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -19.390760\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -19.406852\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -19.412783\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -19.418656\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -19.424469\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -19.430224\n",
            "resetting env. episode 334.000000, reward total was -19.000000. running mean: -19.425922\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -19.431663\n",
            "resetting env. episode 336.000000, reward total was -17.000000. running mean: -19.407346\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -19.423273\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -19.439040\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -19.434650\n",
            "resetting env. episode 340.000000, reward total was -18.000000. running mean: -19.420303\n",
            "resetting env. episode 341.000000, reward total was -19.000000. running mean: -19.416100\n",
            "resetting env. episode 342.000000, reward total was -18.000000. running mean: -19.401939\n",
            "resetting env. episode 343.000000, reward total was -15.000000. running mean: -19.357920\n",
            "resetting env. episode 344.000000, reward total was -17.000000. running mean: -19.334341\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -19.340997\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -19.357587\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -19.374011\n",
            "resetting env. episode 348.000000, reward total was -19.000000. running mean: -19.370271\n",
            "resetting env. episode 349.000000, reward total was -18.000000. running mean: -19.356569\n",
            "resetting env. episode 350.000000, reward total was -18.000000. running mean: -19.343003\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -19.339573\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -19.346177\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -19.362715\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -19.369088\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -19.375397\n",
            "resetting env. episode 356.000000, reward total was -18.000000. running mean: -19.361643\n",
            "resetting env. episode 357.000000, reward total was -16.000000. running mean: -19.328027\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -19.334747\n",
            "resetting env. episode 359.000000, reward total was -16.000000. running mean: -19.301399\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -19.318385\n",
            "resetting env. episode 361.000000, reward total was -18.000000. running mean: -19.305201\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -19.312149\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -19.309028\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -19.315938\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -19.332778\n",
            "resetting env. episode 366.000000, reward total was -18.000000. running mean: -19.319450\n",
            "resetting env. episode 367.000000, reward total was -18.000000. running mean: -19.306256\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -19.303193\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -19.310161\n",
            "resetting env. episode 370.000000, reward total was -18.000000. running mean: -19.297060\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -19.304089\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -19.321048\n",
            "resetting env. episode 373.000000, reward total was -19.000000. running mean: -19.317838\n",
            "resetting env. episode 374.000000, reward total was -19.000000. running mean: -19.314659\n",
            "resetting env. episode 375.000000, reward total was -19.000000. running mean: -19.311513\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -19.328398\n",
            "resetting env. episode 377.000000, reward total was -19.000000. running mean: -19.325114\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -19.331863\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -19.328544\n",
            "resetting env. episode 380.000000, reward total was -18.000000. running mean: -19.315258\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -19.332106\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -19.338785\n",
            "resetting env. episode 383.000000, reward total was -17.000000. running mean: -19.315397\n",
            "resetting env. episode 384.000000, reward total was -19.000000. running mean: -19.312243\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -19.329121\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -19.345829\n",
            "resetting env. episode 387.000000, reward total was -17.000000. running mean: -19.322371\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -19.339147\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -19.355756\n",
            "resetting env. episode 390.000000, reward total was -18.000000. running mean: -19.342198\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -19.338776\n",
            "resetting env. episode 392.000000, reward total was -17.000000. running mean: -19.315389\n",
            "resetting env. episode 393.000000, reward total was -17.000000. running mean: -19.292235\n",
            "resetting env. episode 394.000000, reward total was -18.000000. running mean: -19.279312\n",
            "resetting env. episode 395.000000, reward total was -18.000000. running mean: -19.266519\n",
            "resetting env. episode 396.000000, reward total was -19.000000. running mean: -19.263854\n",
            "resetting env. episode 397.000000, reward total was -18.000000. running mean: -19.251216\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -19.258703\n",
            "resetting env. episode 399.000000, reward total was -18.000000. running mean: -19.246116\n",
            "resetting env. episode 400.000000, reward total was -18.000000. running mean: -19.233655\n",
            "resetting env. episode 401.000000, reward total was -17.000000. running mean: -19.211319\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -19.209205\n",
            "resetting env. episode 403.000000, reward total was -19.000000. running mean: -19.207113\n",
            "resetting env. episode 404.000000, reward total was -18.000000. running mean: -19.195042\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -19.213092\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -19.230961\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -19.248651\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -19.246165\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -19.253703\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -19.271166\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -19.278454\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -19.285670\n",
            "resetting env. episode 413.000000, reward total was -15.000000. running mean: -19.242813\n",
            "resetting env. episode 414.000000, reward total was -18.000000. running mean: -19.230385\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -19.248081\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -19.265600\n",
            "resetting env. episode 417.000000, reward total was -19.000000. running mean: -19.262944\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -19.280315\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -19.277512\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -19.294737\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -19.311789\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -19.328671\n",
            "resetting env. episode 423.000000, reward total was -18.000000. running mean: -19.315385\n",
            "resetting env. episode 424.000000, reward total was -16.000000. running mean: -19.282231\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -19.279409\n",
            "resetting env. episode 426.000000, reward total was -18.000000. running mean: -19.266614\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -19.273948\n",
            "resetting env. episode 428.000000, reward total was -19.000000. running mean: -19.271209\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -19.278497\n",
            "resetting env. episode 430.000000, reward total was -17.000000. running mean: -19.255712\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -19.263155\n",
            "resetting env. episode 432.000000, reward total was -19.000000. running mean: -19.260523\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -19.277918\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -19.295139\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -19.302187\n",
            "resetting env. episode 436.000000, reward total was -19.000000. running mean: -19.299165\n",
            "resetting env. episode 437.000000, reward total was -18.000000. running mean: -19.286174\n",
            "resetting env. episode 438.000000, reward total was -19.000000. running mean: -19.283312\n",
            "resetting env. episode 439.000000, reward total was -19.000000. running mean: -19.280479\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -19.297674\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -19.304697\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -19.311650\n",
            "resetting env. episode 443.000000, reward total was -17.000000. running mean: -19.288534\n",
            "resetting env. episode 444.000000, reward total was -19.000000. running mean: -19.285649\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -19.292792\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -19.299864\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -19.316866\n",
            "resetting env. episode 448.000000, reward total was -17.000000. running mean: -19.293697\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -19.290760\n",
            "resetting env. episode 450.000000, reward total was -18.000000. running mean: -19.277852\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -19.275074\n",
            "resetting env. episode 452.000000, reward total was -18.000000. running mean: -19.262323\n",
            "resetting env. episode 453.000000, reward total was -17.000000. running mean: -19.239700\n",
            "resetting env. episode 454.000000, reward total was -16.000000. running mean: -19.207303\n",
            "resetting env. episode 455.000000, reward total was -19.000000. running mean: -19.205230\n",
            "resetting env. episode 456.000000, reward total was -18.000000. running mean: -19.193178\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -19.191246\n",
            "resetting env. episode 458.000000, reward total was -18.000000. running mean: -19.179333\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -19.187540\n",
            "resetting env. episode 460.000000, reward total was -17.000000. running mean: -19.165665\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -19.184008\n",
            "resetting env. episode 462.000000, reward total was -18.000000. running mean: -19.172168\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -19.190446\n",
            "resetting env. episode 464.000000, reward total was -18.000000. running mean: -19.178542\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -19.196756\n",
            "resetting env. episode 466.000000, reward total was -19.000000. running mean: -19.194789\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -19.192841\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -19.210912\n",
            "resetting env. episode 469.000000, reward total was -14.000000. running mean: -19.158803\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -19.177215\n",
            "resetting env. episode 471.000000, reward total was -18.000000. running mean: -19.165443\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -19.173789\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -19.172051\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -19.180330\n",
            "resetting env. episode 475.000000, reward total was -18.000000. running mean: -19.168527\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -19.176842\n",
            "resetting env. episode 477.000000, reward total was -18.000000. running mean: -19.165073\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -19.173423\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -19.181688\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -19.189871\n",
            "resetting env. episode 481.000000, reward total was -13.000000. running mean: -19.127973\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -19.136693\n",
            "resetting env. episode 483.000000, reward total was -17.000000. running mean: -19.115326\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -19.134173\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -19.142831\n",
            "resetting env. episode 486.000000, reward total was -18.000000. running mean: -19.131403\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -19.150089\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -19.158588\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -19.167002\n",
            "resetting env. episode 490.000000, reward total was -19.000000. running mean: -19.165332\n",
            "resetting env. episode 491.000000, reward total was -18.000000. running mean: -19.153679\n",
            "resetting env. episode 492.000000, reward total was -17.000000. running mean: -19.132142\n",
            "resetting env. episode 493.000000, reward total was -18.000000. running mean: -19.120820\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -19.129612\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -19.138316\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -19.146933\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -19.155464\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -19.153909\n",
            "resetting env. episode 499.000000, reward total was -18.000000. running mean: -19.142370\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -19.150946\n",
            "resetting env. episode 501.000000, reward total was -19.000000. running mean: -19.149437\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -19.167942\n",
            "resetting env. episode 503.000000, reward total was -20.000000. running mean: -19.176263\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -19.194500\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -19.212555\n",
            "resetting env. episode 506.000000, reward total was -18.000000. running mean: -19.200430\n",
            "resetting env. episode 507.000000, reward total was -21.000000. running mean: -19.218425\n",
            "resetting env. episode 508.000000, reward total was -21.000000. running mean: -19.236241\n",
            "resetting env. episode 509.000000, reward total was -19.000000. running mean: -19.233879\n",
            "resetting env. episode 510.000000, reward total was -19.000000. running mean: -19.231540\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -19.239225\n",
            "resetting env. episode 512.000000, reward total was -19.000000. running mean: -19.236832\n",
            "resetting env. episode 513.000000, reward total was -17.000000. running mean: -19.214464\n",
            "resetting env. episode 514.000000, reward total was -19.000000. running mean: -19.212319\n",
            "resetting env. episode 515.000000, reward total was -20.000000. running mean: -19.220196\n",
            "resetting env. episode 516.000000, reward total was -19.000000. running mean: -19.217994\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -19.235814\n",
            "resetting env. episode 518.000000, reward total was -17.000000. running mean: -19.213456\n",
            "resetting env. episode 519.000000, reward total was -19.000000. running mean: -19.211322\n",
            "resetting env. episode 520.000000, reward total was -16.000000. running mean: -19.179208\n",
            "resetting env. episode 521.000000, reward total was -19.000000. running mean: -19.177416\n",
            "resetting env. episode 522.000000, reward total was -20.000000. running mean: -19.185642\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -19.193786\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -19.201848\n",
            "resetting env. episode 525.000000, reward total was -18.000000. running mean: -19.189829\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -19.207931\n",
            "resetting env. episode 527.000000, reward total was -20.000000. running mean: -19.215852\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -19.233693\n",
            "resetting env. episode 529.000000, reward total was -20.000000. running mean: -19.241356\n",
            "resetting env. episode 530.000000, reward total was -20.000000. running mean: -19.248943\n",
            "resetting env. episode 531.000000, reward total was -20.000000. running mean: -19.256453\n",
            "resetting env. episode 532.000000, reward total was -20.000000. running mean: -19.263889\n",
            "resetting env. episode 533.000000, reward total was -18.000000. running mean: -19.251250\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -19.268737\n",
            "resetting env. episode 535.000000, reward total was -18.000000. running mean: -19.256050\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -19.273490\n",
            "resetting env. episode 537.000000, reward total was -19.000000. running mean: -19.270755\n",
            "resetting env. episode 538.000000, reward total was -19.000000. running mean: -19.268047\n",
            "resetting env. episode 539.000000, reward total was -19.000000. running mean: -19.265367\n",
            "resetting env. episode 540.000000, reward total was -20.000000. running mean: -19.272713\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -19.289986\n",
            "resetting env. episode 542.000000, reward total was -20.000000. running mean: -19.297086\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -19.314115\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -19.330974\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -19.347664\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -19.364188\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -19.380546\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -19.396740\n",
            "resetting env. episode 549.000000, reward total was -18.000000. running mean: -19.382773\n",
            "resetting env. episode 550.000000, reward total was -19.000000. running mean: -19.378945\n",
            "resetting env. episode 551.000000, reward total was -17.000000. running mean: -19.355156\n",
            "resetting env. episode 552.000000, reward total was -18.000000. running mean: -19.341604\n",
            "resetting env. episode 553.000000, reward total was -16.000000. running mean: -19.308188\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -19.325106\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -19.341855\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -19.348437\n",
            "resetting env. episode 557.000000, reward total was -17.000000. running mean: -19.324952\n",
            "resetting env. episode 558.000000, reward total was -20.000000. running mean: -19.331703\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -19.348386\n",
            "resetting env. episode 560.000000, reward total was -19.000000. running mean: -19.344902\n",
            "resetting env. episode 561.000000, reward total was -21.000000. running mean: -19.361453\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -19.377838\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -19.394060\n",
            "resetting env. episode 564.000000, reward total was -16.000000. running mean: -19.360119\n",
            "resetting env. episode 565.000000, reward total was -20.000000. running mean: -19.366518\n",
            "resetting env. episode 566.000000, reward total was -18.000000. running mean: -19.352853\n",
            "resetting env. episode 567.000000, reward total was -18.000000. running mean: -19.339324\n",
            "resetting env. episode 568.000000, reward total was -17.000000. running mean: -19.315931\n",
            "resetting env. episode 569.000000, reward total was -17.000000. running mean: -19.292772\n",
            "resetting env. episode 570.000000, reward total was -20.000000. running mean: -19.299844\n",
            "resetting env. episode 571.000000, reward total was -15.000000. running mean: -19.256846\n",
            "resetting env. episode 572.000000, reward total was -19.000000. running mean: -19.254277\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -19.261734\n",
            "resetting env. episode 574.000000, reward total was -19.000000. running mean: -19.259117\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -19.276526\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -19.293761\n",
            "resetting env. episode 577.000000, reward total was -16.000000. running mean: -19.260823\n",
            "resetting env. episode 578.000000, reward total was -20.000000. running mean: -19.268215\n",
            "resetting env. episode 579.000000, reward total was -20.000000. running mean: -19.275533\n",
            "resetting env. episode 580.000000, reward total was -18.000000. running mean: -19.262777\n",
            "resetting env. episode 581.000000, reward total was -18.000000. running mean: -19.250150\n",
            "resetting env. episode 582.000000, reward total was -15.000000. running mean: -19.207648\n",
            "resetting env. episode 583.000000, reward total was -19.000000. running mean: -19.205572\n",
            "resetting env. episode 584.000000, reward total was -18.000000. running mean: -19.193516\n",
            "resetting env. episode 585.000000, reward total was -12.000000. running mean: -19.121581\n",
            "resetting env. episode 586.000000, reward total was -20.000000. running mean: -19.130365\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -19.149061\n",
            "resetting env. episode 588.000000, reward total was -18.000000. running mean: -19.137571\n",
            "resetting env. episode 589.000000, reward total was -19.000000. running mean: -19.136195\n",
            "resetting env. episode 590.000000, reward total was -20.000000. running mean: -19.144833\n",
            "resetting env. episode 591.000000, reward total was -18.000000. running mean: -19.133385\n",
            "resetting env. episode 592.000000, reward total was -19.000000. running mean: -19.132051\n",
            "resetting env. episode 593.000000, reward total was -17.000000. running mean: -19.110730\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -19.119623\n",
            "resetting env. episode 595.000000, reward total was -18.000000. running mean: -19.108427\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -19.117343\n",
            "resetting env. episode 597.000000, reward total was -17.000000. running mean: -19.096169\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -19.105207\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -19.124155\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -19.142914\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -19.161485\n",
            "resetting env. episode 602.000000, reward total was -18.000000. running mean: -19.149870\n",
            "resetting env. episode 603.000000, reward total was -18.000000. running mean: -19.138371\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -19.156987\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -19.175417\n",
            "resetting env. episode 606.000000, reward total was -16.000000. running mean: -19.143663\n",
            "resetting env. episode 607.000000, reward total was -17.000000. running mean: -19.122227\n",
            "resetting env. episode 608.000000, reward total was -18.000000. running mean: -19.111004\n",
            "resetting env. episode 609.000000, reward total was -12.000000. running mean: -19.039894\n",
            "resetting env. episode 610.000000, reward total was -20.000000. running mean: -19.049495\n",
            "resetting env. episode 611.000000, reward total was -15.000000. running mean: -19.009000\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -19.028910\n",
            "resetting env. episode 613.000000, reward total was -20.000000. running mean: -19.038621\n",
            "resetting env. episode 614.000000, reward total was -20.000000. running mean: -19.048235\n",
            "resetting env. episode 615.000000, reward total was -16.000000. running mean: -19.017753\n",
            "resetting env. episode 616.000000, reward total was -20.000000. running mean: -19.027575\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -19.047300\n",
            "resetting env. episode 618.000000, reward total was -20.000000. running mean: -19.056827\n",
            "resetting env. episode 619.000000, reward total was -18.000000. running mean: -19.046258\n",
            "resetting env. episode 620.000000, reward total was -20.000000. running mean: -19.055796\n",
            "resetting env. episode 621.000000, reward total was -18.000000. running mean: -19.045238\n",
            "resetting env. episode 622.000000, reward total was -19.000000. running mean: -19.044785\n",
            "resetting env. episode 623.000000, reward total was -18.000000. running mean: -19.034337\n",
            "resetting env. episode 624.000000, reward total was -19.000000. running mean: -19.033994\n",
            "resetting env. episode 625.000000, reward total was -19.000000. running mean: -19.033654\n",
            "resetting env. episode 626.000000, reward total was -19.000000. running mean: -19.033318\n",
            "resetting env. episode 627.000000, reward total was -16.000000. running mean: -19.002984\n",
            "resetting env. episode 628.000000, reward total was -19.000000. running mean: -19.002955\n",
            "resetting env. episode 629.000000, reward total was -20.000000. running mean: -19.012925\n",
            "resetting env. episode 630.000000, reward total was -20.000000. running mean: -19.022796\n",
            "resetting env. episode 631.000000, reward total was -20.000000. running mean: -19.032568\n",
            "resetting env. episode 632.000000, reward total was -19.000000. running mean: -19.032242\n",
            "resetting env. episode 633.000000, reward total was -17.000000. running mean: -19.011920\n",
            "resetting env. episode 634.000000, reward total was -19.000000. running mean: -19.011801\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -19.031683\n",
            "resetting env. episode 636.000000, reward total was -18.000000. running mean: -19.021366\n",
            "resetting env. episode 637.000000, reward total was -17.000000. running mean: -19.001152\n",
            "resetting env. episode 638.000000, reward total was -20.000000. running mean: -19.011141\n",
            "resetting env. episode 639.000000, reward total was -19.000000. running mean: -19.011029\n",
            "resetting env. episode 640.000000, reward total was -18.000000. running mean: -19.000919\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -19.020910\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -19.040701\n",
            "resetting env. episode 643.000000, reward total was -17.000000. running mean: -19.020294\n",
            "resetting env. episode 644.000000, reward total was -18.000000. running mean: -19.010091\n",
            "resetting env. episode 645.000000, reward total was -18.000000. running mean: -18.999990\n",
            "resetting env. episode 646.000000, reward total was -19.000000. running mean: -18.999990\n",
            "resetting env. episode 647.000000, reward total was -18.000000. running mean: -18.989990\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -19.010090\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -19.029989\n",
            "resetting env. episode 650.000000, reward total was -19.000000. running mean: -19.029689\n",
            "resetting env. episode 651.000000, reward total was -18.000000. running mean: -19.019392\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -19.039198\n",
            "resetting env. episode 653.000000, reward total was -20.000000. running mean: -19.048806\n",
            "resetting env. episode 654.000000, reward total was -20.000000. running mean: -19.058318\n",
            "resetting env. episode 655.000000, reward total was -16.000000. running mean: -19.027735\n",
            "resetting env. episode 656.000000, reward total was -21.000000. running mean: -19.047458\n",
            "resetting env. episode 657.000000, reward total was -20.000000. running mean: -19.056983\n",
            "resetting env. episode 658.000000, reward total was -20.000000. running mean: -19.066413\n",
            "resetting env. episode 659.000000, reward total was -13.000000. running mean: -19.005749\n",
            "resetting env. episode 660.000000, reward total was -19.000000. running mean: -19.005692\n",
            "resetting env. episode 661.000000, reward total was -20.000000. running mean: -19.015635\n",
            "resetting env. episode 662.000000, reward total was -19.000000. running mean: -19.015479\n",
            "resetting env. episode 663.000000, reward total was -19.000000. running mean: -19.015324\n",
            "resetting env. episode 664.000000, reward total was -20.000000. running mean: -19.025171\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -19.044919\n",
            "resetting env. episode 666.000000, reward total was -20.000000. running mean: -19.054470\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -19.073925\n",
            "resetting env. episode 668.000000, reward total was -17.000000. running mean: -19.053186\n",
            "resetting env. episode 669.000000, reward total was -19.000000. running mean: -19.052654\n",
            "resetting env. episode 670.000000, reward total was -19.000000. running mean: -19.052127\n",
            "resetting env. episode 671.000000, reward total was -19.000000. running mean: -19.051606\n",
            "resetting env. episode 672.000000, reward total was -19.000000. running mean: -19.051090\n",
            "resetting env. episode 673.000000, reward total was -20.000000. running mean: -19.060579\n",
            "resetting env. episode 674.000000, reward total was -18.000000. running mean: -19.049973\n",
            "resetting env. episode 675.000000, reward total was -20.000000. running mean: -19.059474\n",
            "resetting env. episode 676.000000, reward total was -17.000000. running mean: -19.038879\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -19.048490\n",
            "resetting env. episode 678.000000, reward total was -19.000000. running mean: -19.048005\n",
            "resetting env. episode 679.000000, reward total was -19.000000. running mean: -19.047525\n",
            "resetting env. episode 680.000000, reward total was -16.000000. running mean: -19.017050\n",
            "resetting env. episode 681.000000, reward total was -20.000000. running mean: -19.026879\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -19.036611\n",
            "resetting env. episode 683.000000, reward total was -15.000000. running mean: -18.996244\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -19.006282\n",
            "resetting env. episode 685.000000, reward total was -19.000000. running mean: -19.006219\n",
            "resetting env. episode 686.000000, reward total was -20.000000. running mean: -19.016157\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -19.025995\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -19.045735\n",
            "resetting env. episode 689.000000, reward total was -20.000000. running mean: -19.055278\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -19.064725\n",
            "resetting env. episode 691.000000, reward total was -19.000000. running mean: -19.064078\n",
            "resetting env. episode 692.000000, reward total was -19.000000. running mean: -19.063437\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -19.082803\n",
            "resetting env. episode 694.000000, reward total was -18.000000. running mean: -19.071975\n",
            "resetting env. episode 695.000000, reward total was -20.000000. running mean: -19.081255\n",
            "resetting env. episode 696.000000, reward total was -17.000000. running mean: -19.060443\n",
            "resetting env. episode 697.000000, reward total was -18.000000. running mean: -19.049838\n",
            "resetting env. episode 698.000000, reward total was -12.000000. running mean: -18.979340\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -18.999546\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -19.009551\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -19.029455\n",
            "resetting env. episode 702.000000, reward total was -19.000000. running mean: -19.029161\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -19.048869\n",
            "resetting env. episode 704.000000, reward total was -19.000000. running mean: -19.048381\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -19.057897\n",
            "resetting env. episode 706.000000, reward total was -15.000000. running mean: -19.017318\n",
            "resetting env. episode 707.000000, reward total was -19.000000. running mean: -19.017145\n",
            "resetting env. episode 708.000000, reward total was -18.000000. running mean: -19.006973\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -19.026903\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -19.046634\n",
            "resetting env. episode 711.000000, reward total was -20.000000. running mean: -19.056168\n",
            "resetting env. episode 712.000000, reward total was -12.000000. running mean: -18.985606\n",
            "resetting env. episode 713.000000, reward total was -19.000000. running mean: -18.985750\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -18.995893\n",
            "resetting env. episode 715.000000, reward total was -16.000000. running mean: -18.965934\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -18.986275\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -19.006412\n",
            "resetting env. episode 718.000000, reward total was -17.000000. running mean: -18.986348\n",
            "resetting env. episode 719.000000, reward total was -19.000000. running mean: -18.986484\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -19.006619\n",
            "resetting env. episode 721.000000, reward total was -20.000000. running mean: -19.016553\n",
            "resetting env. episode 722.000000, reward total was -19.000000. running mean: -19.016388\n",
            "resetting env. episode 723.000000, reward total was -19.000000. running mean: -19.016224\n",
            "resetting env. episode 724.000000, reward total was -19.000000. running mean: -19.016061\n",
            "resetting env. episode 725.000000, reward total was -20.000000. running mean: -19.025901\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -19.035642\n",
            "resetting env. episode 727.000000, reward total was -20.000000. running mean: -19.045285\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -19.064833\n",
            "resetting env. episode 729.000000, reward total was -18.000000. running mean: -19.054184\n",
            "resetting env. episode 730.000000, reward total was -19.000000. running mean: -19.053642\n",
            "resetting env. episode 731.000000, reward total was -19.000000. running mean: -19.053106\n",
            "resetting env. episode 732.000000, reward total was -18.000000. running mean: -19.042575\n",
            "resetting env. episode 733.000000, reward total was -19.000000. running mean: -19.042149\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -19.051728\n",
            "resetting env. episode 735.000000, reward total was -18.000000. running mean: -19.041210\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -19.050798\n",
            "resetting env. episode 737.000000, reward total was -20.000000. running mean: -19.060290\n",
            "resetting env. episode 738.000000, reward total was -19.000000. running mean: -19.059687\n",
            "resetting env. episode 739.000000, reward total was -16.000000. running mean: -19.029091\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -19.048800\n",
            "resetting env. episode 741.000000, reward total was -19.000000. running mean: -19.048312\n",
            "resetting env. episode 742.000000, reward total was -19.000000. running mean: -19.047829\n",
            "resetting env. episode 743.000000, reward total was -14.000000. running mean: -18.997350\n",
            "resetting env. episode 744.000000, reward total was -19.000000. running mean: -18.997377\n",
            "resetting env. episode 745.000000, reward total was -18.000000. running mean: -18.987403\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -18.997529\n",
            "resetting env. episode 747.000000, reward total was -19.000000. running mean: -18.997554\n",
            "resetting env. episode 748.000000, reward total was -17.000000. running mean: -18.977578\n",
            "resetting env. episode 749.000000, reward total was -19.000000. running mean: -18.977802\n",
            "resetting env. episode 750.000000, reward total was -19.000000. running mean: -18.978024\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -18.998244\n",
            "resetting env. episode 752.000000, reward total was -20.000000. running mean: -19.008262\n",
            "resetting env. episode 753.000000, reward total was -17.000000. running mean: -18.988179\n",
            "resetting env. episode 754.000000, reward total was -20.000000. running mean: -18.998297\n",
            "resetting env. episode 755.000000, reward total was -16.000000. running mean: -18.968314\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -18.988631\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -19.008745\n",
            "resetting env. episode 758.000000, reward total was -16.000000. running mean: -18.978657\n",
            "resetting env. episode 759.000000, reward total was -19.000000. running mean: -18.978871\n",
            "resetting env. episode 760.000000, reward total was -21.000000. running mean: -18.999082\n",
            "resetting env. episode 761.000000, reward total was -17.000000. running mean: -18.979091\n",
            "resetting env. episode 762.000000, reward total was -18.000000. running mean: -18.969300\n",
            "resetting env. episode 763.000000, reward total was -19.000000. running mean: -18.969607\n",
            "resetting env. episode 764.000000, reward total was -18.000000. running mean: -18.959911\n",
            "resetting env. episode 765.000000, reward total was -19.000000. running mean: -18.960312\n",
            "resetting env. episode 766.000000, reward total was -20.000000. running mean: -18.970709\n",
            "resetting env. episode 767.000000, reward total was -19.000000. running mean: -18.971002\n",
            "resetting env. episode 768.000000, reward total was -19.000000. running mean: -18.971292\n",
            "resetting env. episode 769.000000, reward total was -19.000000. running mean: -18.971579\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -18.991863\n",
            "resetting env. episode 771.000000, reward total was -19.000000. running mean: -18.991945\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -19.012025\n",
            "resetting env. episode 773.000000, reward total was -17.000000. running mean: -18.991905\n",
            "resetting env. episode 774.000000, reward total was -15.000000. running mean: -18.951986\n",
            "resetting env. episode 775.000000, reward total was -18.000000. running mean: -18.942466\n",
            "resetting env. episode 776.000000, reward total was -20.000000. running mean: -18.953041\n",
            "resetting env. episode 777.000000, reward total was -18.000000. running mean: -18.943511\n",
            "resetting env. episode 778.000000, reward total was -19.000000. running mean: -18.944076\n",
            "resetting env. episode 779.000000, reward total was -19.000000. running mean: -18.944635\n",
            "resetting env. episode 780.000000, reward total was -20.000000. running mean: -18.955189\n",
            "resetting env. episode 781.000000, reward total was -20.000000. running mean: -18.965637\n",
            "resetting env. episode 782.000000, reward total was -19.000000. running mean: -18.965980\n",
            "resetting env. episode 783.000000, reward total was -19.000000. running mean: -18.966321\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -18.986657\n",
            "resetting env. episode 785.000000, reward total was -20.000000. running mean: -18.996791\n",
            "resetting env. episode 786.000000, reward total was -18.000000. running mean: -18.986823\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -19.006955\n",
            "resetting env. episode 788.000000, reward total was -17.000000. running mean: -18.986885\n",
            "resetting env. episode 789.000000, reward total was -19.000000. running mean: -18.987016\n",
            "resetting env. episode 790.000000, reward total was -20.000000. running mean: -18.997146\n",
            "resetting env. episode 791.000000, reward total was -16.000000. running mean: -18.967175\n",
            "resetting env. episode 792.000000, reward total was -20.000000. running mean: -18.977503\n",
            "resetting env. episode 793.000000, reward total was -13.000000. running mean: -18.917728\n",
            "resetting env. episode 794.000000, reward total was -20.000000. running mean: -18.928551\n",
            "resetting env. episode 795.000000, reward total was -17.000000. running mean: -18.909265\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -18.930172\n",
            "resetting env. episode 797.000000, reward total was -18.000000. running mean: -18.920871\n",
            "resetting env. episode 798.000000, reward total was -19.000000. running mean: -18.921662\n",
            "resetting env. episode 799.000000, reward total was -20.000000. running mean: -18.932445\n",
            "resetting env. episode 800.000000, reward total was -18.000000. running mean: -18.923121\n",
            "resetting env. episode 801.000000, reward total was -18.000000. running mean: -18.913890\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -18.934751\n",
            "resetting env. episode 803.000000, reward total was -13.000000. running mean: -18.875403\n",
            "resetting env. episode 804.000000, reward total was -16.000000. running mean: -18.846649\n",
            "resetting env. episode 805.000000, reward total was -19.000000. running mean: -18.848183\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -18.869701\n",
            "resetting env. episode 807.000000, reward total was -18.000000. running mean: -18.861004\n",
            "resetting env. episode 808.000000, reward total was -18.000000. running mean: -18.852394\n",
            "resetting env. episode 809.000000, reward total was -17.000000. running mean: -18.833870\n",
            "resetting env. episode 810.000000, reward total was -16.000000. running mean: -18.805531\n",
            "resetting env. episode 811.000000, reward total was -18.000000. running mean: -18.797476\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -18.819501\n",
            "resetting env. episode 813.000000, reward total was -20.000000. running mean: -18.831306\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -18.852993\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -18.864463\n",
            "resetting env. episode 816.000000, reward total was -19.000000. running mean: -18.865819\n",
            "resetting env. episode 817.000000, reward total was -18.000000. running mean: -18.857160\n",
            "resetting env. episode 818.000000, reward total was -19.000000. running mean: -18.858589\n",
            "resetting env. episode 819.000000, reward total was -18.000000. running mean: -18.850003\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -18.861503\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -18.882888\n",
            "resetting env. episode 822.000000, reward total was -20.000000. running mean: -18.894059\n",
            "resetting env. episode 823.000000, reward total was -17.000000. running mean: -18.875118\n",
            "resetting env. episode 824.000000, reward total was -18.000000. running mean: -18.866367\n",
            "resetting env. episode 825.000000, reward total was -16.000000. running mean: -18.837704\n",
            "resetting env. episode 826.000000, reward total was -20.000000. running mean: -18.849326\n",
            "resetting env. episode 827.000000, reward total was -18.000000. running mean: -18.840833\n",
            "resetting env. episode 828.000000, reward total was -20.000000. running mean: -18.852425\n",
            "resetting env. episode 829.000000, reward total was -20.000000. running mean: -18.863901\n",
            "resetting env. episode 830.000000, reward total was -19.000000. running mean: -18.865262\n",
            "resetting env. episode 831.000000, reward total was -17.000000. running mean: -18.846609\n",
            "resetting env. episode 832.000000, reward total was -19.000000. running mean: -18.848143\n",
            "resetting env. episode 833.000000, reward total was -20.000000. running mean: -18.859662\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -18.871065\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -18.892354\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -18.903431\n",
            "resetting env. episode 837.000000, reward total was -21.000000. running mean: -18.924396\n",
            "resetting env. episode 838.000000, reward total was -19.000000. running mean: -18.925152\n",
            "resetting env. episode 839.000000, reward total was -18.000000. running mean: -18.915901\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -18.926742\n",
            "resetting env. episode 841.000000, reward total was -19.000000. running mean: -18.927474\n",
            "resetting env. episode 842.000000, reward total was -18.000000. running mean: -18.918200\n",
            "resetting env. episode 843.000000, reward total was -19.000000. running mean: -18.919018\n",
            "resetting env. episode 844.000000, reward total was -20.000000. running mean: -18.929828\n",
            "resetting env. episode 845.000000, reward total was -19.000000. running mean: -18.930529\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -18.951224\n",
            "resetting env. episode 847.000000, reward total was -18.000000. running mean: -18.941712\n",
            "resetting env. episode 848.000000, reward total was -17.000000. running mean: -18.922295\n",
            "resetting env. episode 849.000000, reward total was -20.000000. running mean: -18.933072\n",
            "resetting env. episode 850.000000, reward total was -20.000000. running mean: -18.943741\n",
            "resetting env. episode 851.000000, reward total was -14.000000. running mean: -18.894304\n",
            "resetting env. episode 852.000000, reward total was -15.000000. running mean: -18.855361\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -18.876807\n",
            "resetting env. episode 854.000000, reward total was -18.000000. running mean: -18.868039\n",
            "resetting env. episode 855.000000, reward total was -20.000000. running mean: -18.879358\n",
            "resetting env. episode 856.000000, reward total was -19.000000. running mean: -18.880565\n",
            "resetting env. episode 857.000000, reward total was -20.000000. running mean: -18.891759\n",
            "resetting env. episode 858.000000, reward total was -19.000000. running mean: -18.892842\n",
            "resetting env. episode 859.000000, reward total was -16.000000. running mean: -18.863913\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -18.885274\n",
            "resetting env. episode 861.000000, reward total was -20.000000. running mean: -18.896421\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -18.917457\n",
            "resetting env. episode 863.000000, reward total was -18.000000. running mean: -18.908283\n",
            "resetting env. episode 864.000000, reward total was -18.000000. running mean: -18.899200\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -18.920208\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -18.941006\n",
            "resetting env. episode 867.000000, reward total was -19.000000. running mean: -18.941596\n",
            "resetting env. episode 868.000000, reward total was -18.000000. running mean: -18.932180\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -18.952858\n",
            "resetting env. episode 870.000000, reward total was -18.000000. running mean: -18.943329\n",
            "resetting env. episode 871.000000, reward total was -19.000000. running mean: -18.943896\n",
            "resetting env. episode 872.000000, reward total was -18.000000. running mean: -18.934457\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -18.955112\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -18.975561\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -18.985806\n",
            "resetting env. episode 876.000000, reward total was -14.000000. running mean: -18.935948\n",
            "resetting env. episode 877.000000, reward total was -17.000000. running mean: -18.916588\n",
            "resetting env. episode 878.000000, reward total was -18.000000. running mean: -18.907422\n",
            "resetting env. episode 879.000000, reward total was -18.000000. running mean: -18.898348\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -18.919365\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -18.940171\n",
            "resetting env. episode 882.000000, reward total was -17.000000. running mean: -18.920769\n",
            "resetting env. episode 883.000000, reward total was -19.000000. running mean: -18.921562\n",
            "resetting env. episode 884.000000, reward total was -19.000000. running mean: -18.922346\n",
            "resetting env. episode 885.000000, reward total was -19.000000. running mean: -18.923122\n",
            "resetting env. episode 886.000000, reward total was -17.000000. running mean: -18.903891\n",
            "resetting env. episode 887.000000, reward total was -16.000000. running mean: -18.874852\n",
            "resetting env. episode 888.000000, reward total was -17.000000. running mean: -18.856104\n",
            "resetting env. episode 889.000000, reward total was -19.000000. running mean: -18.857543\n",
            "resetting env. episode 890.000000, reward total was -15.000000. running mean: -18.818967\n",
            "resetting env. episode 891.000000, reward total was -20.000000. running mean: -18.830778\n",
            "resetting env. episode 892.000000, reward total was -18.000000. running mean: -18.822470\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -18.834245\n",
            "resetting env. episode 894.000000, reward total was -20.000000. running mean: -18.845903\n",
            "resetting env. episode 895.000000, reward total was -20.000000. running mean: -18.857444\n",
            "resetting env. episode 896.000000, reward total was -18.000000. running mean: -18.848869\n",
            "resetting env. episode 897.000000, reward total was -20.000000. running mean: -18.860381\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -18.881777\n",
            "resetting env. episode 899.000000, reward total was -20.000000. running mean: -18.892959\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -18.914029\n",
            "resetting env. episode 901.000000, reward total was -19.000000. running mean: -18.914889\n",
            "resetting env. episode 902.000000, reward total was -19.000000. running mean: -18.915740\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -18.936583\n",
            "resetting env. episode 904.000000, reward total was -20.000000. running mean: -18.947217\n",
            "resetting env. episode 905.000000, reward total was -19.000000. running mean: -18.947745\n",
            "resetting env. episode 906.000000, reward total was -19.000000. running mean: -18.948267\n",
            "resetting env. episode 907.000000, reward total was -20.000000. running mean: -18.958785\n",
            "resetting env. episode 908.000000, reward total was -19.000000. running mean: -18.959197\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -18.969605\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -18.989909\n",
            "resetting env. episode 911.000000, reward total was -18.000000. running mean: -18.980010\n",
            "resetting env. episode 912.000000, reward total was -19.000000. running mean: -18.980210\n",
            "resetting env. episode 913.000000, reward total was -16.000000. running mean: -18.950408\n",
            "resetting env. episode 914.000000, reward total was -19.000000. running mean: -18.950903\n",
            "resetting env. episode 915.000000, reward total was -17.000000. running mean: -18.931394\n",
            "resetting env. episode 916.000000, reward total was -20.000000. running mean: -18.942081\n",
            "resetting env. episode 917.000000, reward total was -19.000000. running mean: -18.942660\n",
            "resetting env. episode 918.000000, reward total was -17.000000. running mean: -18.923233\n",
            "resetting env. episode 919.000000, reward total was -16.000000. running mean: -18.894001\n",
            "resetting env. episode 920.000000, reward total was -17.000000. running mean: -18.875061\n",
            "resetting env. episode 921.000000, reward total was -16.000000. running mean: -18.846310\n",
            "resetting env. episode 922.000000, reward total was -20.000000. running mean: -18.857847\n",
            "resetting env. episode 923.000000, reward total was -18.000000. running mean: -18.849269\n",
            "resetting env. episode 924.000000, reward total was -19.000000. running mean: -18.850776\n",
            "resetting env. episode 925.000000, reward total was -20.000000. running mean: -18.862268\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -18.883645\n",
            "resetting env. episode 927.000000, reward total was -19.000000. running mean: -18.884809\n",
            "resetting env. episode 928.000000, reward total was -19.000000. running mean: -18.885961\n",
            "resetting env. episode 929.000000, reward total was -16.000000. running mean: -18.857101\n",
            "resetting env. episode 930.000000, reward total was -19.000000. running mean: -18.858530\n",
            "resetting env. episode 931.000000, reward total was -17.000000. running mean: -18.839945\n",
            "resetting env. episode 932.000000, reward total was -17.000000. running mean: -18.821546\n",
            "resetting env. episode 933.000000, reward total was -19.000000. running mean: -18.823330\n",
            "resetting env. episode 934.000000, reward total was -19.000000. running mean: -18.825097\n",
            "resetting env. episode 935.000000, reward total was -18.000000. running mean: -18.816846\n",
            "resetting env. episode 936.000000, reward total was -17.000000. running mean: -18.798677\n",
            "resetting env. episode 937.000000, reward total was -18.000000. running mean: -18.790691\n",
            "resetting env. episode 938.000000, reward total was -15.000000. running mean: -18.752784\n",
            "resetting env. episode 939.000000, reward total was -18.000000. running mean: -18.745256\n",
            "resetting env. episode 940.000000, reward total was -19.000000. running mean: -18.747803\n",
            "resetting env. episode 941.000000, reward total was -16.000000. running mean: -18.720325\n",
            "resetting env. episode 942.000000, reward total was -17.000000. running mean: -18.703122\n",
            "resetting env. episode 943.000000, reward total was -21.000000. running mean: -18.726091\n",
            "resetting env. episode 944.000000, reward total was -12.000000. running mean: -18.658830\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -18.682242\n",
            "resetting env. episode 946.000000, reward total was -18.000000. running mean: -18.675419\n",
            "resetting env. episode 947.000000, reward total was -20.000000. running mean: -18.688665\n",
            "resetting env. episode 948.000000, reward total was -20.000000. running mean: -18.701778\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -18.724761\n",
            "resetting env. episode 950.000000, reward total was -20.000000. running mean: -18.737513\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -18.760138\n",
            "resetting env. episode 952.000000, reward total was -20.000000. running mean: -18.772536\n",
            "resetting env. episode 953.000000, reward total was -18.000000. running mean: -18.764811\n",
            "resetting env. episode 954.000000, reward total was -17.000000. running mean: -18.747163\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -18.769691\n",
            "resetting env. episode 956.000000, reward total was -17.000000. running mean: -18.751994\n",
            "resetting env. episode 957.000000, reward total was -18.000000. running mean: -18.744474\n",
            "resetting env. episode 958.000000, reward total was -20.000000. running mean: -18.757030\n",
            "resetting env. episode 959.000000, reward total was -20.000000. running mean: -18.769459\n",
            "resetting env. episode 960.000000, reward total was -20.000000. running mean: -18.781765\n",
            "resetting env. episode 961.000000, reward total was -19.000000. running mean: -18.783947\n",
            "resetting env. episode 962.000000, reward total was -20.000000. running mean: -18.796108\n",
            "resetting env. episode 963.000000, reward total was -19.000000. running mean: -18.798147\n",
            "resetting env. episode 964.000000, reward total was -20.000000. running mean: -18.810165\n",
            "resetting env. episode 965.000000, reward total was -21.000000. running mean: -18.832064\n",
            "resetting env. episode 966.000000, reward total was -20.000000. running mean: -18.843743\n",
            "resetting env. episode 967.000000, reward total was -18.000000. running mean: -18.835305\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -18.856952\n",
            "resetting env. episode 969.000000, reward total was -19.000000. running mean: -18.858383\n",
            "resetting env. episode 970.000000, reward total was -20.000000. running mean: -18.869799\n",
            "resetting env. episode 971.000000, reward total was -20.000000. running mean: -18.881101\n",
            "resetting env. episode 972.000000, reward total was -15.000000. running mean: -18.842290\n",
            "resetting env. episode 973.000000, reward total was -19.000000. running mean: -18.843867\n",
            "resetting env. episode 974.000000, reward total was -18.000000. running mean: -18.835428\n",
            "resetting env. episode 975.000000, reward total was -19.000000. running mean: -18.837074\n",
            "resetting env. episode 976.000000, reward total was -19.000000. running mean: -18.838703\n",
            "resetting env. episode 977.000000, reward total was -20.000000. running mean: -18.850316\n",
            "resetting env. episode 978.000000, reward total was -18.000000. running mean: -18.841813\n",
            "resetting env. episode 979.000000, reward total was -18.000000. running mean: -18.833395\n",
            "resetting env. episode 980.000000, reward total was -17.000000. running mean: -18.815061\n",
            "resetting env. episode 981.000000, reward total was -20.000000. running mean: -18.826911\n",
            "resetting env. episode 982.000000, reward total was -20.000000. running mean: -18.838641\n",
            "resetting env. episode 983.000000, reward total was -18.000000. running mean: -18.830255\n",
            "resetting env. episode 984.000000, reward total was -21.000000. running mean: -18.851952\n",
            "resetting env. episode 985.000000, reward total was -19.000000. running mean: -18.853433\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -18.874899\n",
            "resetting env. episode 987.000000, reward total was -21.000000. running mean: -18.896150\n",
            "resetting env. episode 988.000000, reward total was -19.000000. running mean: -18.897188\n",
            "resetting env. episode 989.000000, reward total was -19.000000. running mean: -18.898216\n",
            "resetting env. episode 990.000000, reward total was -19.000000. running mean: -18.899234\n",
            "resetting env. episode 991.000000, reward total was -19.000000. running mean: -18.900242\n",
            "resetting env. episode 992.000000, reward total was -19.000000. running mean: -18.901239\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -18.922227\n",
            "resetting env. episode 994.000000, reward total was -19.000000. running mean: -18.923005\n",
            "resetting env. episode 995.000000, reward total was -20.000000. running mean: -18.933775\n",
            "resetting env. episode 996.000000, reward total was -20.000000. running mean: -18.944437\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -18.964993\n",
            "resetting env. episode 998.000000, reward total was -17.000000. running mean: -18.945343\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -18.965889\n",
            "resetting env. episode 1000.000000, reward total was -20.000000. running mean: -18.976230\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -18.996468\n",
            "resetting env. episode 1002.000000, reward total was -20.000000. running mean: -19.006503\n",
            "resetting env. episode 1003.000000, reward total was -20.000000. running mean: -19.016438\n",
            "resetting env. episode 1004.000000, reward total was -20.000000. running mean: -19.026274\n",
            "resetting env. episode 1005.000000, reward total was -16.000000. running mean: -18.996011\n",
            "resetting env. episode 1006.000000, reward total was -20.000000. running mean: -19.006051\n",
            "resetting env. episode 1007.000000, reward total was -19.000000. running mean: -19.005991\n",
            "resetting env. episode 1008.000000, reward total was -18.000000. running mean: -18.995931\n",
            "resetting env. episode 1009.000000, reward total was -18.000000. running mean: -18.985971\n",
            "resetting env. episode 1010.000000, reward total was -19.000000. running mean: -18.986112\n",
            "resetting env. episode 1011.000000, reward total was -18.000000. running mean: -18.976250\n",
            "resetting env. episode 1012.000000, reward total was -17.000000. running mean: -18.956488\n",
            "resetting env. episode 1013.000000, reward total was -17.000000. running mean: -18.936923\n",
            "resetting env. episode 1014.000000, reward total was -18.000000. running mean: -18.927554\n",
            "resetting env. episode 1015.000000, reward total was -19.000000. running mean: -18.928278\n",
            "resetting env. episode 1016.000000, reward total was -19.000000. running mean: -18.928996\n",
            "resetting env. episode 1017.000000, reward total was -18.000000. running mean: -18.919706\n",
            "resetting env. episode 1018.000000, reward total was -14.000000. running mean: -18.870509\n",
            "resetting env. episode 1019.000000, reward total was -18.000000. running mean: -18.861803\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -18.883185\n",
            "resetting env. episode 1021.000000, reward total was -19.000000. running mean: -18.884354\n",
            "resetting env. episode 1022.000000, reward total was -20.000000. running mean: -18.895510\n",
            "resetting env. episode 1023.000000, reward total was -16.000000. running mean: -18.866555\n",
            "resetting env. episode 1024.000000, reward total was -19.000000. running mean: -18.867889\n",
            "resetting env. episode 1025.000000, reward total was -17.000000. running mean: -18.849210\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -18.870718\n",
            "resetting env. episode 1027.000000, reward total was -19.000000. running mean: -18.872011\n",
            "resetting env. episode 1028.000000, reward total was -18.000000. running mean: -18.863291\n",
            "resetting env. episode 1029.000000, reward total was -18.000000. running mean: -18.854658\n",
            "resetting env. episode 1030.000000, reward total was -18.000000. running mean: -18.846112\n",
            "resetting env. episode 1031.000000, reward total was -19.000000. running mean: -18.847650\n",
            "resetting env. episode 1032.000000, reward total was -17.000000. running mean: -18.829174\n",
            "resetting env. episode 1033.000000, reward total was -19.000000. running mean: -18.830882\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -18.852573\n",
            "resetting env. episode 1035.000000, reward total was -19.000000. running mean: -18.854048\n",
            "resetting env. episode 1036.000000, reward total was -19.000000. running mean: -18.855507\n",
            "resetting env. episode 1037.000000, reward total was -20.000000. running mean: -18.866952\n",
            "resetting env. episode 1038.000000, reward total was -19.000000. running mean: -18.868283\n",
            "resetting env. episode 1039.000000, reward total was -19.000000. running mean: -18.869600\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -18.890904\n",
            "resetting env. episode 1041.000000, reward total was -19.000000. running mean: -18.891995\n",
            "resetting env. episode 1042.000000, reward total was -19.000000. running mean: -18.893075\n",
            "resetting env. episode 1043.000000, reward total was -16.000000. running mean: -18.864144\n",
            "resetting env. episode 1044.000000, reward total was -18.000000. running mean: -18.855503\n",
            "resetting env. episode 1045.000000, reward total was -19.000000. running mean: -18.856948\n",
            "resetting env. episode 1046.000000, reward total was -17.000000. running mean: -18.838378\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -18.859994\n",
            "resetting env. episode 1048.000000, reward total was -19.000000. running mean: -18.861394\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -18.872780\n",
            "resetting env. episode 1050.000000, reward total was -19.000000. running mean: -18.874053\n",
            "resetting env. episode 1051.000000, reward total was -17.000000. running mean: -18.855312\n",
            "resetting env. episode 1052.000000, reward total was -19.000000. running mean: -18.856759\n",
            "resetting env. episode 1053.000000, reward total was -20.000000. running mean: -18.868191\n",
            "resetting env. episode 1054.000000, reward total was -19.000000. running mean: -18.869509\n",
            "resetting env. episode 1055.000000, reward total was -20.000000. running mean: -18.880814\n",
            "resetting env. episode 1056.000000, reward total was -18.000000. running mean: -18.872006\n",
            "resetting env. episode 1057.000000, reward total was -20.000000. running mean: -18.883286\n",
            "resetting env. episode 1058.000000, reward total was -18.000000. running mean: -18.874453\n",
            "resetting env. episode 1059.000000, reward total was -20.000000. running mean: -18.885709\n",
            "resetting env. episode 1060.000000, reward total was -20.000000. running mean: -18.896852\n",
            "resetting env. episode 1061.000000, reward total was -19.000000. running mean: -18.897883\n",
            "resetting env. episode 1062.000000, reward total was -18.000000. running mean: -18.888904\n",
            "resetting env. episode 1063.000000, reward total was -20.000000. running mean: -18.900015\n",
            "resetting env. episode 1064.000000, reward total was -17.000000. running mean: -18.881015\n",
            "resetting env. episode 1065.000000, reward total was -15.000000. running mean: -18.842205\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -18.853783\n",
            "resetting env. episode 1067.000000, reward total was -18.000000. running mean: -18.845245\n",
            "resetting env. episode 1068.000000, reward total was -21.000000. running mean: -18.866793\n",
            "resetting env. episode 1069.000000, reward total was -19.000000. running mean: -18.868125\n",
            "resetting env. episode 1070.000000, reward total was -18.000000. running mean: -18.859443\n",
            "resetting env. episode 1071.000000, reward total was -19.000000. running mean: -18.860849\n",
            "resetting env. episode 1072.000000, reward total was -18.000000. running mean: -18.852241\n",
            "resetting env. episode 1073.000000, reward total was -19.000000. running mean: -18.853718\n",
            "resetting env. episode 1074.000000, reward total was -20.000000. running mean: -18.865181\n",
            "resetting env. episode 1075.000000, reward total was -20.000000. running mean: -18.876529\n",
            "resetting env. episode 1076.000000, reward total was -16.000000. running mean: -18.847764\n",
            "resetting env. episode 1077.000000, reward total was -17.000000. running mean: -18.829286\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -18.850993\n",
            "resetting env. episode 1079.000000, reward total was -19.000000. running mean: -18.852483\n",
            "resetting env. episode 1080.000000, reward total was -20.000000. running mean: -18.863959\n",
            "resetting env. episode 1081.000000, reward total was -20.000000. running mean: -18.875319\n",
            "resetting env. episode 1082.000000, reward total was -19.000000. running mean: -18.876566\n",
            "resetting env. episode 1083.000000, reward total was -19.000000. running mean: -18.877800\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -18.899022\n",
            "resetting env. episode 1085.000000, reward total was -19.000000. running mean: -18.900032\n",
            "resetting env. episode 1086.000000, reward total was -18.000000. running mean: -18.891032\n",
            "resetting env. episode 1087.000000, reward total was -18.000000. running mean: -18.882121\n",
            "resetting env. episode 1088.000000, reward total was -19.000000. running mean: -18.883300\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -18.894467\n",
            "resetting env. episode 1090.000000, reward total was -18.000000. running mean: -18.885522\n",
            "resetting env. episode 1091.000000, reward total was -20.000000. running mean: -18.896667\n",
            "resetting env. episode 1092.000000, reward total was -17.000000. running mean: -18.877701\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -18.898924\n",
            "resetting env. episode 1094.000000, reward total was -20.000000. running mean: -18.909934\n",
            "resetting env. episode 1095.000000, reward total was -18.000000. running mean: -18.900835\n",
            "resetting env. episode 1096.000000, reward total was -17.000000. running mean: -18.881827\n",
            "resetting env. episode 1097.000000, reward total was -18.000000. running mean: -18.873008\n",
            "resetting env. episode 1098.000000, reward total was -20.000000. running mean: -18.884278\n",
            "resetting env. episode 1099.000000, reward total was -17.000000. running mean: -18.865435\n",
            "resetting env. episode 1100.000000, reward total was -17.000000. running mean: -18.846781\n",
            "resetting env. episode 1101.000000, reward total was -20.000000. running mean: -18.858313\n",
            "resetting env. episode 1102.000000, reward total was -17.000000. running mean: -18.839730\n",
            "resetting env. episode 1103.000000, reward total was -20.000000. running mean: -18.851333\n",
            "resetting env. episode 1104.000000, reward total was -20.000000. running mean: -18.862820\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -18.884191\n",
            "resetting env. episode 1106.000000, reward total was -16.000000. running mean: -18.855349\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -18.876796\n",
            "resetting env. episode 1108.000000, reward total was -18.000000. running mean: -18.868028\n",
            "resetting env. episode 1109.000000, reward total was -19.000000. running mean: -18.869348\n",
            "resetting env. episode 1110.000000, reward total was -19.000000. running mean: -18.870654\n",
            "resetting env. episode 1111.000000, reward total was -18.000000. running mean: -18.861948\n",
            "resetting env. episode 1112.000000, reward total was -16.000000. running mean: -18.833328\n",
            "resetting env. episode 1113.000000, reward total was -20.000000. running mean: -18.844995\n",
            "resetting env. episode 1114.000000, reward total was -19.000000. running mean: -18.846545\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -18.868080\n",
            "resetting env. episode 1116.000000, reward total was -20.000000. running mean: -18.879399\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -18.900605\n",
            "resetting env. episode 1118.000000, reward total was -20.000000. running mean: -18.911599\n",
            "resetting env. episode 1119.000000, reward total was -20.000000. running mean: -18.922483\n",
            "resetting env. episode 1120.000000, reward total was -20.000000. running mean: -18.933258\n",
            "resetting env. episode 1121.000000, reward total was -18.000000. running mean: -18.923925\n",
            "resetting env. episode 1122.000000, reward total was -17.000000. running mean: -18.904686\n",
            "resetting env. episode 1123.000000, reward total was -19.000000. running mean: -18.905639\n",
            "resetting env. episode 1124.000000, reward total was -20.000000. running mean: -18.916583\n",
            "resetting env. episode 1125.000000, reward total was -17.000000. running mean: -18.897417\n",
            "resetting env. episode 1126.000000, reward total was -18.000000. running mean: -18.888443\n",
            "resetting env. episode 1127.000000, reward total was -19.000000. running mean: -18.889558\n",
            "resetting env. episode 1128.000000, reward total was -20.000000. running mean: -18.900663\n",
            "resetting env. episode 1129.000000, reward total was -20.000000. running mean: -18.911656\n",
            "resetting env. episode 1130.000000, reward total was -19.000000. running mean: -18.912540\n",
            "resetting env. episode 1131.000000, reward total was -20.000000. running mean: -18.923414\n",
            "resetting env. episode 1132.000000, reward total was -18.000000. running mean: -18.914180\n",
            "resetting env. episode 1133.000000, reward total was -18.000000. running mean: -18.905038\n",
            "resetting env. episode 1134.000000, reward total was -20.000000. running mean: -18.915988\n",
            "resetting env. episode 1135.000000, reward total was -17.000000. running mean: -18.896828\n",
            "resetting env. episode 1136.000000, reward total was -21.000000. running mean: -18.917860\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -18.938681\n",
            "resetting env. episode 1138.000000, reward total was -18.000000. running mean: -18.929294\n",
            "resetting env. episode 1139.000000, reward total was -18.000000. running mean: -18.920001\n",
            "resetting env. episode 1140.000000, reward total was -20.000000. running mean: -18.930801\n",
            "resetting env. episode 1141.000000, reward total was -19.000000. running mean: -18.931493\n",
            "resetting env. episode 1142.000000, reward total was -20.000000. running mean: -18.942178\n",
            "resetting env. episode 1143.000000, reward total was -19.000000. running mean: -18.942757\n",
            "resetting env. episode 1144.000000, reward total was -17.000000. running mean: -18.923329\n",
            "resetting env. episode 1145.000000, reward total was -16.000000. running mean: -18.894096\n",
            "resetting env. episode 1146.000000, reward total was -14.000000. running mean: -18.845155\n",
            "resetting env. episode 1147.000000, reward total was -21.000000. running mean: -18.866703\n",
            "resetting env. episode 1148.000000, reward total was -21.000000. running mean: -18.888036\n",
            "resetting env. episode 1149.000000, reward total was -17.000000. running mean: -18.869156\n",
            "resetting env. episode 1150.000000, reward total was -18.000000. running mean: -18.860464\n",
            "resetting env. episode 1151.000000, reward total was -20.000000. running mean: -18.871860\n",
            "resetting env. episode 1152.000000, reward total was -18.000000. running mean: -18.863141\n",
            "resetting env. episode 1153.000000, reward total was -19.000000. running mean: -18.864510\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -18.885865\n",
            "resetting env. episode 1155.000000, reward total was -17.000000. running mean: -18.867006\n",
            "resetting env. episode 1156.000000, reward total was -20.000000. running mean: -18.878336\n",
            "resetting env. episode 1157.000000, reward total was -19.000000. running mean: -18.879552\n",
            "resetting env. episode 1158.000000, reward total was -19.000000. running mean: -18.880757\n",
            "resetting env. episode 1159.000000, reward total was -18.000000. running mean: -18.871949\n",
            "resetting env. episode 1160.000000, reward total was -20.000000. running mean: -18.883230\n",
            "resetting env. episode 1161.000000, reward total was -18.000000. running mean: -18.874398\n",
            "resetting env. episode 1162.000000, reward total was -18.000000. running mean: -18.865654\n",
            "resetting env. episode 1163.000000, reward total was -19.000000. running mean: -18.866997\n",
            "resetting env. episode 1164.000000, reward total was -12.000000. running mean: -18.798327\n",
            "resetting env. episode 1165.000000, reward total was -19.000000. running mean: -18.800344\n",
            "resetting env. episode 1166.000000, reward total was -15.000000. running mean: -18.762340\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -18.784717\n",
            "resetting env. episode 1168.000000, reward total was -18.000000. running mean: -18.776870\n",
            "resetting env. episode 1169.000000, reward total was -18.000000. running mean: -18.769101\n",
            "resetting env. episode 1170.000000, reward total was -16.000000. running mean: -18.741410\n",
            "resetting env. episode 1171.000000, reward total was -19.000000. running mean: -18.743996\n",
            "resetting env. episode 1172.000000, reward total was -19.000000. running mean: -18.746556\n",
            "resetting env. episode 1173.000000, reward total was -20.000000. running mean: -18.759091\n",
            "resetting env. episode 1174.000000, reward total was -18.000000. running mean: -18.751500\n",
            "resetting env. episode 1175.000000, reward total was -15.000000. running mean: -18.713985\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -18.736845\n",
            "resetting env. episode 1177.000000, reward total was -12.000000. running mean: -18.669476\n",
            "resetting env. episode 1178.000000, reward total was -15.000000. running mean: -18.632782\n",
            "resetting env. episode 1179.000000, reward total was -15.000000. running mean: -18.596454\n",
            "resetting env. episode 1180.000000, reward total was -18.000000. running mean: -18.590489\n",
            "resetting env. episode 1181.000000, reward total was -19.000000. running mean: -18.594584\n",
            "resetting env. episode 1182.000000, reward total was -19.000000. running mean: -18.598638\n",
            "resetting env. episode 1183.000000, reward total was -15.000000. running mean: -18.562652\n",
            "resetting env. episode 1184.000000, reward total was -17.000000. running mean: -18.547026\n",
            "resetting env. episode 1185.000000, reward total was -17.000000. running mean: -18.531555\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -18.556240\n",
            "resetting env. episode 1187.000000, reward total was -19.000000. running mean: -18.560677\n",
            "resetting env. episode 1188.000000, reward total was -14.000000. running mean: -18.515071\n",
            "resetting env. episode 1189.000000, reward total was -14.000000. running mean: -18.469920\n",
            "resetting env. episode 1190.000000, reward total was -20.000000. running mean: -18.485221\n",
            "resetting env. episode 1191.000000, reward total was -18.000000. running mean: -18.480368\n",
            "resetting env. episode 1192.000000, reward total was -19.000000. running mean: -18.485565\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -18.510709\n",
            "resetting env. episode 1194.000000, reward total was -16.000000. running mean: -18.485602\n",
            "resetting env. episode 1195.000000, reward total was -19.000000. running mean: -18.490746\n",
            "resetting env. episode 1196.000000, reward total was -19.000000. running mean: -18.495839\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -18.520880\n",
            "resetting env. episode 1198.000000, reward total was -12.000000. running mean: -18.455671\n",
            "resetting env. episode 1199.000000, reward total was -19.000000. running mean: -18.461115\n",
            "resetting env. episode 1200.000000, reward total was -20.000000. running mean: -18.476504\n",
            "resetting env. episode 1201.000000, reward total was -20.000000. running mean: -18.491738\n",
            "resetting env. episode 1202.000000, reward total was -16.000000. running mean: -18.466821\n",
            "resetting env. episode 1203.000000, reward total was -17.000000. running mean: -18.452153\n",
            "resetting env. episode 1204.000000, reward total was -19.000000. running mean: -18.457631\n",
            "resetting env. episode 1205.000000, reward total was -13.000000. running mean: -18.403055\n",
            "resetting env. episode 1206.000000, reward total was -16.000000. running mean: -18.379024\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -18.385234\n",
            "resetting env. episode 1208.000000, reward total was -20.000000. running mean: -18.401382\n",
            "resetting env. episode 1209.000000, reward total was -17.000000. running mean: -18.387368\n",
            "resetting env. episode 1210.000000, reward total was -15.000000. running mean: -18.353494\n",
            "resetting env. episode 1211.000000, reward total was -16.000000. running mean: -18.329959\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -18.356660\n",
            "resetting env. episode 1213.000000, reward total was -19.000000. running mean: -18.363093\n",
            "resetting env. episode 1214.000000, reward total was -18.000000. running mean: -18.359462\n",
            "resetting env. episode 1215.000000, reward total was -16.000000. running mean: -18.335868\n",
            "resetting env. episode 1216.000000, reward total was -17.000000. running mean: -18.322509\n",
            "resetting env. episode 1217.000000, reward total was -18.000000. running mean: -18.319284\n",
            "resetting env. episode 1218.000000, reward total was -18.000000. running mean: -18.316091\n",
            "resetting env. episode 1219.000000, reward total was -19.000000. running mean: -18.322930\n",
            "resetting env. episode 1220.000000, reward total was -18.000000. running mean: -18.319701\n",
            "resetting env. episode 1221.000000, reward total was -15.000000. running mean: -18.286504\n",
            "resetting env. episode 1222.000000, reward total was -20.000000. running mean: -18.303639\n",
            "resetting env. episode 1223.000000, reward total was -17.000000. running mean: -18.290602\n",
            "resetting env. episode 1224.000000, reward total was -18.000000. running mean: -18.287696\n",
            "resetting env. episode 1225.000000, reward total was -18.000000. running mean: -18.284819\n",
            "resetting env. episode 1226.000000, reward total was -16.000000. running mean: -18.261971\n",
            "resetting env. episode 1227.000000, reward total was -17.000000. running mean: -18.249352\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -18.276858\n",
            "resetting env. episode 1229.000000, reward total was -16.000000. running mean: -18.254089\n",
            "resetting env. episode 1230.000000, reward total was -19.000000. running mean: -18.261549\n",
            "resetting env. episode 1231.000000, reward total was -19.000000. running mean: -18.268933\n",
            "resetting env. episode 1232.000000, reward total was -14.000000. running mean: -18.226244\n",
            "resetting env. episode 1233.000000, reward total was -20.000000. running mean: -18.243981\n",
            "resetting env. episode 1234.000000, reward total was -19.000000. running mean: -18.251541\n",
            "resetting env. episode 1235.000000, reward total was -18.000000. running mean: -18.249026\n",
            "resetting env. episode 1236.000000, reward total was -18.000000. running mean: -18.246536\n",
            "resetting env. episode 1237.000000, reward total was -16.000000. running mean: -18.224070\n",
            "resetting env. episode 1238.000000, reward total was -15.000000. running mean: -18.191830\n",
            "resetting env. episode 1239.000000, reward total was -20.000000. running mean: -18.209911\n",
            "resetting env. episode 1240.000000, reward total was -17.000000. running mean: -18.197812\n",
            "resetting env. episode 1241.000000, reward total was -18.000000. running mean: -18.195834\n",
            "resetting env. episode 1242.000000, reward total was -19.000000. running mean: -18.203876\n",
            "resetting env. episode 1243.000000, reward total was -19.000000. running mean: -18.211837\n",
            "resetting env. episode 1244.000000, reward total was -17.000000. running mean: -18.199719\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -18.227722\n",
            "resetting env. episode 1246.000000, reward total was -16.000000. running mean: -18.205444\n",
            "resetting env. episode 1247.000000, reward total was -19.000000. running mean: -18.213390\n",
            "resetting env. episode 1248.000000, reward total was -17.000000. running mean: -18.201256\n",
            "resetting env. episode 1249.000000, reward total was -17.000000. running mean: -18.189243\n",
            "resetting env. episode 1250.000000, reward total was -14.000000. running mean: -18.147351\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -18.165878\n",
            "resetting env. episode 1252.000000, reward total was -18.000000. running mean: -18.164219\n",
            "resetting env. episode 1253.000000, reward total was -14.000000. running mean: -18.122577\n",
            "resetting env. episode 1254.000000, reward total was -18.000000. running mean: -18.121351\n",
            "resetting env. episode 1255.000000, reward total was -15.000000. running mean: -18.090137\n",
            "resetting env. episode 1256.000000, reward total was -19.000000. running mean: -18.099236\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -18.128244\n",
            "resetting env. episode 1258.000000, reward total was -16.000000. running mean: -18.106961\n",
            "resetting env. episode 1259.000000, reward total was -17.000000. running mean: -18.095891\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -18.114933\n",
            "resetting env. episode 1261.000000, reward total was -19.000000. running mean: -18.123783\n",
            "resetting env. episode 1262.000000, reward total was -18.000000. running mean: -18.122545\n",
            "resetting env. episode 1263.000000, reward total was -19.000000. running mean: -18.131320\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -18.160007\n",
            "resetting env. episode 1265.000000, reward total was -17.000000. running mean: -18.148407\n",
            "resetting env. episode 1266.000000, reward total was -15.000000. running mean: -18.116923\n",
            "resetting env. episode 1267.000000, reward total was -19.000000. running mean: -18.125753\n",
            "resetting env. episode 1268.000000, reward total was -19.000000. running mean: -18.134496\n",
            "resetting env. episode 1269.000000, reward total was -15.000000. running mean: -18.103151\n",
            "resetting env. episode 1270.000000, reward total was -19.000000. running mean: -18.112119\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -18.130998\n",
            "resetting env. episode 1272.000000, reward total was -19.000000. running mean: -18.139688\n",
            "resetting env. episode 1273.000000, reward total was -17.000000. running mean: -18.128291\n",
            "resetting env. episode 1274.000000, reward total was -20.000000. running mean: -18.147008\n",
            "resetting env. episode 1275.000000, reward total was -17.000000. running mean: -18.135538\n",
            "resetting env. episode 1276.000000, reward total was -20.000000. running mean: -18.154183\n",
            "resetting env. episode 1277.000000, reward total was -19.000000. running mean: -18.162641\n",
            "resetting env. episode 1278.000000, reward total was -19.000000. running mean: -18.171015\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -18.199305\n",
            "resetting env. episode 1280.000000, reward total was -21.000000. running mean: -18.227312\n",
            "resetting env. episode 1281.000000, reward total was -13.000000. running mean: -18.175038\n",
            "resetting env. episode 1282.000000, reward total was -20.000000. running mean: -18.193288\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -18.221355\n",
            "resetting env. episode 1284.000000, reward total was -18.000000. running mean: -18.219142\n",
            "resetting env. episode 1285.000000, reward total was -17.000000. running mean: -18.206950\n",
            "resetting env. episode 1286.000000, reward total was -19.000000. running mean: -18.214881\n",
            "resetting env. episode 1287.000000, reward total was -21.000000. running mean: -18.242732\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -18.260305\n",
            "resetting env. episode 1289.000000, reward total was -18.000000. running mean: -18.257702\n",
            "resetting env. episode 1290.000000, reward total was -20.000000. running mean: -18.275124\n",
            "resetting env. episode 1291.000000, reward total was -20.000000. running mean: -18.292373\n",
            "resetting env. episode 1292.000000, reward total was -16.000000. running mean: -18.269450\n",
            "resetting env. episode 1293.000000, reward total was -16.000000. running mean: -18.246755\n",
            "resetting env. episode 1294.000000, reward total was -16.000000. running mean: -18.224287\n",
            "resetting env. episode 1295.000000, reward total was -14.000000. running mean: -18.182045\n",
            "resetting env. episode 1296.000000, reward total was -18.000000. running mean: -18.180224\n",
            "resetting env. episode 1297.000000, reward total was -20.000000. running mean: -18.198422\n",
            "resetting env. episode 1298.000000, reward total was -19.000000. running mean: -18.206438\n",
            "resetting env. episode 1299.000000, reward total was -20.000000. running mean: -18.224373\n",
            "resetting env. episode 1300.000000, reward total was -20.000000. running mean: -18.242130\n",
            "resetting env. episode 1301.000000, reward total was -15.000000. running mean: -18.209708\n",
            "resetting env. episode 1302.000000, reward total was -16.000000. running mean: -18.187611\n",
            "resetting env. episode 1303.000000, reward total was -16.000000. running mean: -18.165735\n",
            "resetting env. episode 1304.000000, reward total was -18.000000. running mean: -18.164078\n",
            "resetting env. episode 1305.000000, reward total was -14.000000. running mean: -18.122437\n",
            "resetting env. episode 1306.000000, reward total was -19.000000. running mean: -18.131213\n",
            "resetting env. episode 1307.000000, reward total was -17.000000. running mean: -18.119900\n",
            "resetting env. episode 1308.000000, reward total was -20.000000. running mean: -18.138701\n",
            "resetting env. episode 1309.000000, reward total was -21.000000. running mean: -18.167314\n",
            "resetting env. episode 1310.000000, reward total was -17.000000. running mean: -18.155641\n",
            "resetting env. episode 1311.000000, reward total was -15.000000. running mean: -18.124085\n",
            "resetting env. episode 1312.000000, reward total was -14.000000. running mean: -18.082844\n",
            "resetting env. episode 1313.000000, reward total was -15.000000. running mean: -18.052016\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -18.081495\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -18.100680\n",
            "resetting env. episode 1316.000000, reward total was -15.000000. running mean: -18.069674\n",
            "resetting env. episode 1317.000000, reward total was -14.000000. running mean: -18.028977\n",
            "resetting env. episode 1318.000000, reward total was -19.000000. running mean: -18.038687\n",
            "resetting env. episode 1319.000000, reward total was -18.000000. running mean: -18.038300\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -18.067917\n",
            "resetting env. episode 1321.000000, reward total was -20.000000. running mean: -18.087238\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -18.106366\n",
            "resetting env. episode 1323.000000, reward total was -20.000000. running mean: -18.125302\n",
            "resetting env. episode 1324.000000, reward total was -19.000000. running mean: -18.134049\n",
            "resetting env. episode 1325.000000, reward total was -19.000000. running mean: -18.142709\n",
            "resetting env. episode 1326.000000, reward total was -17.000000. running mean: -18.131281\n",
            "resetting env. episode 1327.000000, reward total was -18.000000. running mean: -18.129969\n",
            "resetting env. episode 1328.000000, reward total was -15.000000. running mean: -18.098669\n",
            "resetting env. episode 1329.000000, reward total was -17.000000. running mean: -18.087682\n",
            "resetting env. episode 1330.000000, reward total was -14.000000. running mean: -18.046805\n",
            "resetting env. episode 1331.000000, reward total was -20.000000. running mean: -18.066337\n",
            "resetting env. episode 1332.000000, reward total was -20.000000. running mean: -18.085674\n",
            "resetting env. episode 1333.000000, reward total was -17.000000. running mean: -18.074817\n",
            "resetting env. episode 1334.000000, reward total was -14.000000. running mean: -18.034069\n",
            "resetting env. episode 1335.000000, reward total was -18.000000. running mean: -18.033728\n",
            "resetting env. episode 1336.000000, reward total was -19.000000. running mean: -18.043391\n",
            "resetting env. episode 1337.000000, reward total was -12.000000. running mean: -17.982957\n",
            "resetting env. episode 1338.000000, reward total was -13.000000. running mean: -17.933128\n",
            "resetting env. episode 1339.000000, reward total was -19.000000. running mean: -17.943796\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -17.974358\n",
            "resetting env. episode 1341.000000, reward total was -19.000000. running mean: -17.984615\n",
            "resetting env. episode 1342.000000, reward total was -14.000000. running mean: -17.944769\n",
            "resetting env. episode 1343.000000, reward total was -14.000000. running mean: -17.905321\n",
            "resetting env. episode 1344.000000, reward total was -21.000000. running mean: -17.936268\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -17.966905\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -17.997236\n",
            "resetting env. episode 1347.000000, reward total was -18.000000. running mean: -17.997264\n",
            "resetting env. episode 1348.000000, reward total was -21.000000. running mean: -18.027291\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -18.047018\n",
            "resetting env. episode 1350.000000, reward total was -20.000000. running mean: -18.066548\n",
            "resetting env. episode 1351.000000, reward total was -21.000000. running mean: -18.095883\n",
            "resetting env. episode 1352.000000, reward total was -13.000000. running mean: -18.044924\n",
            "resetting env. episode 1353.000000, reward total was -16.000000. running mean: -18.024474\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -18.054230\n",
            "resetting env. episode 1355.000000, reward total was -18.000000. running mean: -18.053687\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -18.083151\n",
            "resetting env. episode 1357.000000, reward total was -21.000000. running mean: -18.112319\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -18.141196\n",
            "resetting env. episode 1359.000000, reward total was -19.000000. running mean: -18.149784\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -18.178286\n",
            "resetting env. episode 1361.000000, reward total was -21.000000. running mean: -18.206503\n",
            "resetting env. episode 1362.000000, reward total was -20.000000. running mean: -18.224438\n",
            "resetting env. episode 1363.000000, reward total was -20.000000. running mean: -18.242194\n",
            "resetting env. episode 1364.000000, reward total was -15.000000. running mean: -18.209772\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -18.237674\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -18.265297\n",
            "resetting env. episode 1367.000000, reward total was -20.000000. running mean: -18.282644\n",
            "resetting env. episode 1368.000000, reward total was -19.000000. running mean: -18.289818\n",
            "resetting env. episode 1369.000000, reward total was -19.000000. running mean: -18.296920\n",
            "resetting env. episode 1370.000000, reward total was -20.000000. running mean: -18.313951\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -18.340811\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -18.367403\n",
            "resetting env. episode 1373.000000, reward total was -21.000000. running mean: -18.393729\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -18.419792\n",
            "resetting env. episode 1375.000000, reward total was -16.000000. running mean: -18.395594\n",
            "resetting env. episode 1376.000000, reward total was -20.000000. running mean: -18.411638\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -18.437521\n",
            "resetting env. episode 1378.000000, reward total was -18.000000. running mean: -18.433146\n",
            "resetting env. episode 1379.000000, reward total was -17.000000. running mean: -18.418815\n",
            "resetting env. episode 1380.000000, reward total was -21.000000. running mean: -18.444627\n",
            "resetting env. episode 1381.000000, reward total was -17.000000. running mean: -18.430180\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -18.455879\n",
            "resetting env. episode 1383.000000, reward total was -21.000000. running mean: -18.481320\n",
            "resetting env. episode 1384.000000, reward total was -21.000000. running mean: -18.506507\n",
            "resetting env. episode 1385.000000, reward total was -17.000000. running mean: -18.491441\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -18.516527\n",
            "resetting env. episode 1387.000000, reward total was -18.000000. running mean: -18.511362\n",
            "resetting env. episode 1388.000000, reward total was -12.000000. running mean: -18.446248\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -18.471786\n",
            "resetting env. episode 1390.000000, reward total was -16.000000. running mean: -18.447068\n",
            "resetting env. episode 1391.000000, reward total was -19.000000. running mean: -18.452597\n",
            "resetting env. episode 1392.000000, reward total was -15.000000. running mean: -18.418071\n",
            "resetting env. episode 1393.000000, reward total was -18.000000. running mean: -18.413890\n",
            "resetting env. episode 1394.000000, reward total was -16.000000. running mean: -18.389752\n",
            "resetting env. episode 1395.000000, reward total was -19.000000. running mean: -18.395854\n",
            "resetting env. episode 1396.000000, reward total was -18.000000. running mean: -18.391895\n",
            "resetting env. episode 1397.000000, reward total was -13.000000. running mean: -18.337977\n",
            "resetting env. episode 1398.000000, reward total was -16.000000. running mean: -18.314597\n",
            "resetting env. episode 1399.000000, reward total was -20.000000. running mean: -18.331451\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -18.358136\n",
            "resetting env. episode 1401.000000, reward total was -17.000000. running mean: -18.344555\n",
            "resetting env. episode 1402.000000, reward total was -17.000000. running mean: -18.331109\n",
            "resetting env. episode 1403.000000, reward total was -20.000000. running mean: -18.347798\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -18.374320\n",
            "resetting env. episode 1405.000000, reward total was -15.000000. running mean: -18.340577\n",
            "resetting env. episode 1406.000000, reward total was -19.000000. running mean: -18.347171\n",
            "resetting env. episode 1407.000000, reward total was -18.000000. running mean: -18.343700\n",
            "resetting env. episode 1408.000000, reward total was -17.000000. running mean: -18.330263\n",
            "resetting env. episode 1409.000000, reward total was -20.000000. running mean: -18.346960\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -18.373490\n",
            "resetting env. episode 1411.000000, reward total was -17.000000. running mean: -18.359755\n",
            "resetting env. episode 1412.000000, reward total was -17.000000. running mean: -18.346158\n",
            "resetting env. episode 1413.000000, reward total was -20.000000. running mean: -18.362696\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -18.389069\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -18.415179\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -18.441027\n",
            "resetting env. episode 1417.000000, reward total was -17.000000. running mean: -18.426617\n",
            "resetting env. episode 1418.000000, reward total was -20.000000. running mean: -18.442350\n",
            "resetting env. episode 1419.000000, reward total was -18.000000. running mean: -18.437927\n",
            "resetting env. episode 1420.000000, reward total was -14.000000. running mean: -18.393548\n",
            "resetting env. episode 1421.000000, reward total was -13.000000. running mean: -18.339612\n",
            "resetting env. episode 1422.000000, reward total was -14.000000. running mean: -18.296216\n",
            "resetting env. episode 1423.000000, reward total was -18.000000. running mean: -18.293254\n",
            "resetting env. episode 1424.000000, reward total was -18.000000. running mean: -18.290321\n",
            "resetting env. episode 1425.000000, reward total was -18.000000. running mean: -18.287418\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -18.314544\n",
            "resetting env. episode 1427.000000, reward total was -16.000000. running mean: -18.291399\n",
            "resetting env. episode 1428.000000, reward total was -16.000000. running mean: -18.268485\n",
            "resetting env. episode 1429.000000, reward total was -17.000000. running mean: -18.255800\n",
            "resetting env. episode 1430.000000, reward total was -19.000000. running mean: -18.263242\n",
            "resetting env. episode 1431.000000, reward total was -20.000000. running mean: -18.280609\n",
            "resetting env. episode 1432.000000, reward total was -14.000000. running mean: -18.237803\n",
            "resetting env. episode 1433.000000, reward total was -15.000000. running mean: -18.205425\n",
            "resetting env. episode 1434.000000, reward total was -15.000000. running mean: -18.173371\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -18.201637\n",
            "resetting env. episode 1436.000000, reward total was -17.000000. running mean: -18.189621\n",
            "resetting env. episode 1437.000000, reward total was -17.000000. running mean: -18.177725\n",
            "resetting env. episode 1438.000000, reward total was -16.000000. running mean: -18.155947\n",
            "resetting env. episode 1439.000000, reward total was -20.000000. running mean: -18.174388\n",
            "resetting env. episode 1440.000000, reward total was -17.000000. running mean: -18.162644\n",
            "resetting env. episode 1441.000000, reward total was -14.000000. running mean: -18.121018\n",
            "resetting env. episode 1442.000000, reward total was -18.000000. running mean: -18.119807\n",
            "resetting env. episode 1443.000000, reward total was -19.000000. running mean: -18.128609\n",
            "resetting env. episode 1444.000000, reward total was -21.000000. running mean: -18.157323\n",
            "resetting env. episode 1445.000000, reward total was -14.000000. running mean: -18.115750\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -18.144593\n",
            "resetting env. episode 1447.000000, reward total was -13.000000. running mean: -18.093147\n",
            "resetting env. episode 1448.000000, reward total was -21.000000. running mean: -18.122215\n",
            "resetting env. episode 1449.000000, reward total was -21.000000. running mean: -18.150993\n",
            "resetting env. episode 1450.000000, reward total was -18.000000. running mean: -18.149483\n",
            "resetting env. episode 1451.000000, reward total was -20.000000. running mean: -18.167988\n",
            "resetting env. episode 1452.000000, reward total was -14.000000. running mean: -18.126308\n",
            "resetting env. episode 1453.000000, reward total was -20.000000. running mean: -18.145045\n",
            "resetting env. episode 1454.000000, reward total was -14.000000. running mean: -18.103595\n",
            "resetting env. episode 1455.000000, reward total was -16.000000. running mean: -18.082559\n",
            "resetting env. episode 1456.000000, reward total was -20.000000. running mean: -18.101733\n",
            "resetting env. episode 1457.000000, reward total was -19.000000. running mean: -18.110716\n",
            "resetting env. episode 1458.000000, reward total was -15.000000. running mean: -18.079609\n",
            "resetting env. episode 1459.000000, reward total was -16.000000. running mean: -18.058813\n",
            "resetting env. episode 1460.000000, reward total was -14.000000. running mean: -18.018225\n",
            "resetting env. episode 1461.000000, reward total was -18.000000. running mean: -18.018042\n",
            "resetting env. episode 1462.000000, reward total was -16.000000. running mean: -17.997862\n",
            "resetting env. episode 1463.000000, reward total was -14.000000. running mean: -17.957883\n",
            "resetting env. episode 1464.000000, reward total was -20.000000. running mean: -17.978304\n",
            "resetting env. episode 1465.000000, reward total was -19.000000. running mean: -17.988521\n",
            "resetting env. episode 1466.000000, reward total was -17.000000. running mean: -17.978636\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -18.008850\n",
            "resetting env. episode 1468.000000, reward total was -19.000000. running mean: -18.018761\n",
            "resetting env. episode 1469.000000, reward total was -21.000000. running mean: -18.048574\n",
            "resetting env. episode 1470.000000, reward total was -19.000000. running mean: -18.058088\n",
            "resetting env. episode 1471.000000, reward total was -16.000000. running mean: -18.037507\n",
            "resetting env. episode 1472.000000, reward total was -19.000000. running mean: -18.047132\n",
            "resetting env. episode 1473.000000, reward total was -15.000000. running mean: -18.016661\n",
            "resetting env. episode 1474.000000, reward total was -18.000000. running mean: -18.016494\n",
            "resetting env. episode 1475.000000, reward total was -17.000000. running mean: -18.006329\n",
            "resetting env. episode 1476.000000, reward total was -16.000000. running mean: -17.986266\n",
            "resetting env. episode 1477.000000, reward total was -19.000000. running mean: -17.996403\n",
            "resetting env. episode 1478.000000, reward total was -18.000000. running mean: -17.996439\n",
            "resetting env. episode 1479.000000, reward total was -19.000000. running mean: -18.006475\n",
            "resetting env. episode 1480.000000, reward total was -13.000000. running mean: -17.956410\n",
            "resetting env. episode 1481.000000, reward total was -18.000000. running mean: -17.956846\n",
            "resetting env. episode 1482.000000, reward total was -19.000000. running mean: -17.967277\n",
            "resetting env. episode 1483.000000, reward total was -16.000000. running mean: -17.947605\n",
            "resetting env. episode 1484.000000, reward total was -20.000000. running mean: -17.968129\n",
            "resetting env. episode 1485.000000, reward total was -17.000000. running mean: -17.958447\n",
            "resetting env. episode 1486.000000, reward total was -16.000000. running mean: -17.938863\n",
            "resetting env. episode 1487.000000, reward total was -18.000000. running mean: -17.939474\n",
            "resetting env. episode 1488.000000, reward total was -17.000000. running mean: -17.930080\n",
            "resetting env. episode 1489.000000, reward total was -21.000000. running mean: -17.960779\n",
            "resetting env. episode 1490.000000, reward total was -11.000000. running mean: -17.891171\n",
            "resetting env. episode 1491.000000, reward total was -15.000000. running mean: -17.862259\n",
            "resetting env. episode 1492.000000, reward total was -17.000000. running mean: -17.853637\n",
            "resetting env. episode 1493.000000, reward total was -17.000000. running mean: -17.845100\n",
            "resetting env. episode 1494.000000, reward total was -17.000000. running mean: -17.836649\n",
            "resetting env. episode 1495.000000, reward total was -19.000000. running mean: -17.848283\n",
            "resetting env. episode 1496.000000, reward total was -18.000000. running mean: -17.849800\n",
            "resetting env. episode 1497.000000, reward total was -18.000000. running mean: -17.851302\n",
            "resetting env. episode 1498.000000, reward total was -20.000000. running mean: -17.872789\n",
            "resetting env. episode 1499.000000, reward total was -16.000000. running mean: -17.854061\n",
            "resetting env. episode 1500.000000, reward total was -19.000000. running mean: -17.865520\n",
            "CPU times: user 2h 7min 30s, sys: 54min 48s, total: 3h 2min 18s\n",
            "Wall time: 1h 34min 29s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "CteN7XKMVGqg",
        "outputId": "5b4e6904-c19d-467c-ef64-97df6a507932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZYA0HgMoO77a"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# pickle.dump(model, open('model.pkl', 'wb'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "pg_from_scratch_(rate_2).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}