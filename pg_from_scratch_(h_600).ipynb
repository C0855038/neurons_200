{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l9hHvTk6ec8"
      },
      "source": [
        "# Policy Gradient\n",
        "\n",
        "* http://karpathy.github.io/2016/05/31/rl/\n",
        "* https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
        "* https://github.com/gameofdimension/policy-gradient-pong\n",
        "* https://www.youtube.com/watch?v=tqrcjHuNdmQ\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqkOdLyN9Ylm"
      },
      "source": [
        "## Step 1: Installation for Colab - just execute these cells and do not worry too much\n",
        "\n",
        "* http://nbviewer.jupyter.org/github/patrickmineault/xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb \n",
        "* https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi\n",
        "* https://nyu-cds.github.io/python-mpi/setup/\n",
        "* https://medium.com/@kaleajit27/reinforcement-learning-on-google-colab-9cb2e1ef51e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF9MAVI16huj",
        "outputId": "eef27bc7-4969-4d82-c2cf-db954011c379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install python-opengl -y  >/dev/null\n",
        "!apt install xvfb -y >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fSC11TfN6p69"
      },
      "outputs": [],
      "source": [
        "!pip install pyvirtualdisplay >/dev/null\n",
        "!pip install piglet >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "caiHE2hy6xrf"
      },
      "outputs": [],
      "source": [
        "# from pyvirtualdisplay import Display\n",
        "# display = Display(visible=0, size=(1400, 900))\n",
        "# display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "960b1b9a-7e5f-4599-ff4b-103ac1d87b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=862b414e10c8222e7cfc54f6f52fb946e1c54beb9053d003386ddda9ae5aa6da\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "49656ada-6ccc-4d9a-e494-161200f48893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "d6b45401-0c8c-48d3-dd87-c7deeb8a4d05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "a3058df3-8b7e-4573-f82a-2b22dedcf455"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "0e19967f-e62b-4b6e-e802-52a0483aff79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -16.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  env.close()\n",
        "  display_frames_as_gif(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 600 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 3 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "5e6fb492-a45b-421b-8534-ec8058fa51f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -19.020000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -19.029800\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.049502\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -19.059007\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -19.068417\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.087733\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -19.096855\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -19.105887\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -19.114828\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -19.133680\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -19.152343\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -19.160819\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -19.169211\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -19.187519\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -19.205644\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -19.223588\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -19.241352\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -19.248938\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -19.266449\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -19.283784\n",
            "resetting env. episode 22.000000, reward total was -19.000000. running mean: -19.280946\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.298137\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -19.315156\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.332004\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.348684\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -19.355197\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -19.361645\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.378029\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.394248\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -19.410306\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -19.416203\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -19.422041\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -19.437820\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -19.433442\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -19.429108\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -19.444817\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.460369\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -19.475765\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.491007\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -19.506097\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -19.521036\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -19.535826\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.550468\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -19.564963\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -19.579313\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -19.593520\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.607585\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -19.611509\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.625394\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -19.629140\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -19.642849\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -19.656420\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -19.659856\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -19.663257\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.676625\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -19.689859\n",
            "resetting env. episode 58.000000, reward total was -19.000000. running mean: -19.682960\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -19.676130\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -19.679369\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -19.682575\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.695750\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.708792\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.721704\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -19.724487\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.737242\n",
            "resetting env. episode 67.000000, reward total was -19.000000. running mean: -19.729870\n",
            "resetting env. episode 68.000000, reward total was -19.000000. running mean: -19.722571\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -19.725346\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.738092\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.750711\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -19.753204\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.765672\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -19.768015\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -19.770335\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -19.772632\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.784905\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.797056\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -19.799086\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -19.811095\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -19.822984\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.834754\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.846407\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -19.837943\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.849563\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -19.851068\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.862557\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.873931\n",
            "resetting env. episode 89.000000, reward total was -18.000000. running mean: -19.855192\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.866640\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -19.867974\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.879294\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -19.880501\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -19.891696\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.902779\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -19.903751\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.914714\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -19.905567\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.916511\n",
            "resetting env. episode 100.000000, reward total was -19.000000. running mean: -19.907346\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -19.918272\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -19.929090\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -19.939799\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -19.940401\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -19.940997\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.951587\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.962071\n",
            "resetting env. episode 108.000000, reward total was -19.000000. running mean: -19.952450\n",
            "resetting env. episode 109.000000, reward total was -19.000000. running mean: -19.942926\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.953496\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -19.943961\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.954522\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -19.964977\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -19.965327\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -19.975674\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.985917\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -19.986058\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.996197\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.996235\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -19.996273\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -19.996310\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.006347\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.006283\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.016221\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.026058\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.035798\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.045440\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.054985\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -20.044436\n",
            "resetting env. episode 130.000000, reward total was -18.000000. running mean: -20.023991\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.033751\n",
            "resetting env. episode 132.000000, reward total was -19.000000. running mean: -20.023414\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.033180\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.032848\n",
            "resetting env. episode 135.000000, reward total was -19.000000. running mean: -20.022519\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.032294\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.041971\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.041552\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.041136\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.040725\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.040317\n",
            "resetting env. episode 142.000000, reward total was -19.000000. running mean: -20.029914\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.039615\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.039219\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.048827\n",
            "resetting env. episode 146.000000, reward total was -19.000000. running mean: -20.038339\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.037955\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.037576\n",
            "resetting env. episode 149.000000, reward total was -18.000000. running mean: -20.017200\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.027028\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.026758\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.026490\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.036225\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.045863\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.055404\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.064850\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.064202\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.073560\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.082824\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.081996\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.091176\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.090264\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.099361\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -20.088368\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.087484\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.086609\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.085743\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.094886\n",
            "resetting env. episode 169.000000, reward total was -19.000000. running mean: -20.083937\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.083098\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.082267\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.081444\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.080629\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.089823\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.088925\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.078036\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.087255\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.096383\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.105419\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.114365\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.113221\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.112089\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.110968\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -20.099858\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.108860\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.117771\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.126593\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.135328\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.143974\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.142534\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.151109\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.149598\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -20.138102\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.146721\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.145254\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.153801\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.152263\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.160741\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.169133\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.177442\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.185668\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.193811\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -20.181873\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.190054\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.188153\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.196272\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.194309\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.202366\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.210342\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.218239\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -20.206057\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.213996\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.221856\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.229638\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.237341\n",
            "resetting env. episode 216.000000, reward total was -18.000000. running mean: -20.214968\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.222818\n",
            "resetting env. episode 218.000000, reward total was -19.000000. running mean: -20.210590\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.218484\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.216299\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.214136\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.211995\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.219875\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.227676\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.235399\n",
            "resetting env. episode 226.000000, reward total was -18.000000. running mean: -20.213045\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.210915\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.218806\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.226618\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.224352\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.232108\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.229787\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.227489\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.235214\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.232862\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.240533\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.248128\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.255647\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.253090\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.260559\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.267954\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.275274\n",
            "resetting env. episode 243.000000, reward total was -18.000000. running mean: -20.252522\n",
            "resetting env. episode 244.000000, reward total was -19.000000. running mean: -20.239996\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.247596\n",
            "resetting env. episode 246.000000, reward total was -18.000000. running mean: -20.225120\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.232869\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.230541\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.238235\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.235853\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.233494\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.241159\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.248748\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.246260\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.243798\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.251360\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.248846\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.256358\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.243794\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.241356\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.238942\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.236553\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.244188\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.251746\n",
            "resetting env. episode 265.000000, reward total was -19.000000. running mean: -20.239228\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.246836\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.244368\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.241924\n",
            "resetting env. episode 269.000000, reward total was -19.000000. running mean: -20.229505\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.227210\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.224938\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.232688\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.240361\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.247958\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.245478\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.253023\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.260493\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.267888\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.275209\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.272457\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.279733\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.276935\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.274166\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.281424\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.288610\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.285724\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.292867\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.299938\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.296939\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.293969\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.301030\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.308019\n",
            "resetting env. episode 293.000000, reward total was -19.000000. running mean: -20.294939\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.301990\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.308970\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.295880\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.292921\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.289992\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.297092\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -20.284121\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.281280\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.278467\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.275683\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.272926\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.270196\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.277494\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.284720\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.281872\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.289054\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.286163\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.283301\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.290468\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.287564\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.294688\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.291741\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.298824\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.295836\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.302877\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.299848\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.296850\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.303881\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.310843\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.317734\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.324557\n",
            "resetting env. episode 325.000000, reward total was -19.000000. running mean: -20.311311\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.318198\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.325016\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.321766\n",
            "resetting env. episode 329.000000, reward total was -18.000000. running mean: -20.298548\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.305563\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.312507\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.319382\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.326188\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.332927\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.329597\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.336301\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.332938\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.329609\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.336313\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.332950\n",
            "resetting env. episode 341.000000, reward total was -19.000000. running mean: -20.319620\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.326424\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.333160\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.329828\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.336530\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.343165\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.349733\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.346236\n",
            "resetting env. episode 349.000000, reward total was -17.000000. running mean: -20.312773\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.319645\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.316449\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.323285\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.330052\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.336751\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.343384\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.349950\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.356450\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.352886\n",
            "resetting env. episode 359.000000, reward total was -19.000000. running mean: -20.339357\n",
            "resetting env. episode 360.000000, reward total was -19.000000. running mean: -20.325963\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.332704\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -20.319377\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.326183\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.332921\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.339592\n",
            "resetting env. episode 366.000000, reward total was -18.000000. running mean: -20.316196\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.323034\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.319804\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.326606\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.333340\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.340006\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.336606\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.333240\n",
            "resetting env. episode 374.000000, reward total was -19.000000. running mean: -20.319908\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.326709\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.333442\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.330107\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -20.316806\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -20.303638\n",
            "resetting env. episode 380.000000, reward total was -19.000000. running mean: -20.290602\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.287696\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.294819\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.291870\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.298952\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.295962\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.303003\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.299973\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.306973\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.313903\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.320764\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.327556\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.334281\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.330938\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.327629\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.314352\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.311209\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.318097\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.314916\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.321767\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.308549\n",
            "resetting env. episode 401.000000, reward total was -18.000000. running mean: -20.285464\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.292609\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.299683\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.306686\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.303619\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.310583\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.307477\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.304402\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.311358\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.318245\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.325062\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.331812\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.338494\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.345109\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.351657\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.348141\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.354660\n",
            "resetting env. episode 418.000000, reward total was -19.000000. running mean: -20.341113\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.337702\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.344325\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.340882\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.347473\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.343998\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.350558\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.357052\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.363482\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.369847\n",
            "resetting env. episode 428.000000, reward total was -18.000000. running mean: -20.346149\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.342687\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.349260\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.355768\n",
            "resetting env. episode 432.000000, reward total was -19.000000. running mean: -20.342210\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.348788\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.355300\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.351747\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.348230\n",
            "resetting env. episode 437.000000, reward total was -19.000000. running mean: -20.334747\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.341400\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.347986\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.354506\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.350961\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.347451\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.353977\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.360437\n",
            "resetting env. episode 445.000000, reward total was -18.000000. running mean: -20.336833\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.323464\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.320230\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.317027\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.313857\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.320718\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.317511\n",
            "resetting env. episode 452.000000, reward total was -18.000000. running mean: -20.294336\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.301393\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.298379\n",
            "resetting env. episode 455.000000, reward total was -18.000000. running mean: -20.275395\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.262641\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.260015\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.267415\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.274740\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.271993\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.269273\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.276580\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.283815\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.290976\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.288067\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.295186\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.302234\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.299212\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.296220\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.293257\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.290325\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.297422\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.284447\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.291603\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.288687\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.295800\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.292842\n",
            "resetting env. episode 478.000000, reward total was -18.000000. running mean: -20.269914\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.277215\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.274442\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.271698\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.268981\n",
            "resetting env. episode 483.000000, reward total was -19.000000. running mean: -20.256291\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.253728\n",
            "resetting env. episode 485.000000, reward total was -18.000000. running mean: -20.231191\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.238879\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.246490\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.254025\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.251485\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.258970\n",
            "resetting env. episode 491.000000, reward total was -18.000000. running mean: -20.236381\n",
            "resetting env. episode 492.000000, reward total was -18.000000. running mean: -20.214017\n",
            "resetting env. episode 493.000000, reward total was -18.000000. running mean: -20.191877\n",
            "resetting env. episode 494.000000, reward total was -18.000000. running mean: -20.169958\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.178258\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.176476\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.184711\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.192864\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.200935\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.198926\n",
            "CPU times: user 1h 12min 18s, sys: 14min 37s, total: 1h 26min 56s\n",
            "Wall time: 44min 56s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "abf0d1c5-5737-410e-e85c-b1d6045500b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -14.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "68c704fb-3ba9-4841-8f37-0f4496937284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 6.000000, reward total was -19.000000. running mean: -20.970297\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.960594\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.960988\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.961378\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -20.941764\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.932347\n",
            "resetting env. episode 12.000000, reward total was -18.000000. running mean: -20.903023\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.903993\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.904953\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.895904\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.896945\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.897975\n",
            "resetting env. episode 18.000000, reward total was -19.000000. running mean: -20.878995\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.870205\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.871503\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.862788\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.854160\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.855619\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.857063\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.848492\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.850007\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.851507\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.842992\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.834562\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.826216\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.817954\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.819775\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.821577\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.813361\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.815228\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.807075\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.809005\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -20.790915\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.783005\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.785175\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.777324\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.769550\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.761855\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.764236\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.756594\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.749028\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.751538\n",
            "resetting env. episode 48.000000, reward total was -19.000000. running mean: -20.734022\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.736682\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -20.719315\n",
            "resetting env. episode 51.000000, reward total was -19.000000. running mean: -20.702122\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.695101\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.698150\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.691168\n",
            "resetting env. episode 55.000000, reward total was -17.000000. running mean: -20.654257\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.657714\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.651137\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.654626\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.658079\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.661499\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.664884\n",
            "resetting env. episode 62.000000, reward total was -19.000000. running mean: -20.648235\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.651752\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.655235\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.658683\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.652096\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.655575\n",
            "resetting env. episode 68.000000, reward total was -19.000000. running mean: -20.639019\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.632629\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.636303\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.639940\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.643540\n",
            "resetting env. episode 73.000000, reward total was -19.000000. running mean: -20.627105\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.620834\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.624625\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.628379\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.632095\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.625774\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.629517\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.633221\n",
            "resetting env. episode 81.000000, reward total was -19.000000. running mean: -20.616889\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.620720\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.614513\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.608368\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.612284\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.606161\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.600100\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.604099\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.608058\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.611977\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.615858\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.619699\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.623502\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.617267\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.621094\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.624883\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.628634\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.632348\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.616025\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.609864\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.603766\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.607728\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.611651\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.605534\n",
            "resetting env. episode 105.000000, reward total was -17.000000. running mean: -20.569479\n",
            "resetting env. episode 106.000000, reward total was -18.000000. running mean: -20.543784\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.538346\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.532963\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.537633\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.542257\n",
            "resetting env. episode 111.000000, reward total was -18.000000. running mean: -20.516834\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.501666\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.496649\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.501683\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.496666\n",
            "resetting env. episode 116.000000, reward total was -17.000000. running mean: -20.461699\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.457082\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.452512\n",
            "resetting env. episode 119.000000, reward total was -19.000000. running mean: -20.437986\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.443607\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.449171\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.454679\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.450132\n",
            "resetting env. episode 124.000000, reward total was -19.000000. running mean: -20.435631\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.441274\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.446862\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.452393\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.447869\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.443390\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.448956\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.454467\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.449922\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.455423\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.460869\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.466260\n",
            "resetting env. episode 136.000000, reward total was -19.000000. running mean: -20.451598\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.457082\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.462511\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.467886\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.473207\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.468475\n",
            "resetting env. episode 142.000000, reward total was -18.000000. running mean: -20.443790\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.439352\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.434959\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.430609\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.436303\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.441940\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.437520\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.433145\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.438814\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.444426\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.439981\n",
            "resetting env. episode 153.000000, reward total was -16.000000. running mean: -20.395582\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.401626\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.397609\n",
            "resetting env. episode 156.000000, reward total was -19.000000. running mean: -20.383633\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.379797\n",
            "resetting env. episode 158.000000, reward total was -18.000000. running mean: -20.355999\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.362439\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.358815\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.355227\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.351674\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.358158\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.354576\n",
            "resetting env. episode 165.000000, reward total was -19.000000. running mean: -20.341030\n",
            "resetting env. episode 166.000000, reward total was -19.000000. running mean: -20.327620\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.324344\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.311100\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.307989\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.304909\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.311860\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.318742\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.315554\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.312399\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.309275\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.306182\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -20.293120\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.300189\n",
            "resetting env. episode 179.000000, reward total was -18.000000. running mean: -20.277187\n",
            "resetting env. episode 180.000000, reward total was -19.000000. running mean: -20.264415\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.271771\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.279053\n",
            "resetting env. episode 183.000000, reward total was -18.000000. running mean: -20.256263\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -20.243700\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.251263\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.248751\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.246263\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.253800\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.251262\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.258750\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.256162\n",
            "resetting env. episode 192.000000, reward total was -19.000000. running mean: -20.243601\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.241165\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.238753\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.236365\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.244002\n",
            "resetting env. episode 197.000000, reward total was -17.000000. running mean: -20.211562\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.209446\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.207352\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.215278\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.223125\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.230894\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.238585\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.236199\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.243837\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.251399\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.258885\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.256296\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.243733\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.251296\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.258783\n",
            "resetting env. episode 212.000000, reward total was -19.000000. running mean: -20.246195\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.253733\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.251196\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.258684\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.256097\n",
            "resetting env. episode 217.000000, reward total was -18.000000. running mean: -20.233536\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.231201\n",
            "resetting env. episode 219.000000, reward total was -19.000000. running mean: -20.218889\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.226700\n",
            "resetting env. episode 221.000000, reward total was -18.000000. running mean: -20.204433\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.212388\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.220265\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.228062\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.225781\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.233524\n",
            "resetting env. episode 227.000000, reward total was -19.000000. running mean: -20.221188\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.218976\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.226787\n",
            "resetting env. episode 230.000000, reward total was -18.000000. running mean: -20.204519\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.212474\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.220349\n",
            "resetting env. episode 233.000000, reward total was -18.000000. running mean: -20.198145\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.196164\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.204202\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.192160\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.190239\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.188336\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.196453\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.204488\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.212443\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -20.200319\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.198316\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.196333\n",
            "resetting env. episode 245.000000, reward total was -19.000000. running mean: -20.184369\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.192526\n",
            "resetting env. episode 247.000000, reward total was -17.000000. running mean: -20.160600\n",
            "resetting env. episode 248.000000, reward total was -19.000000. running mean: -20.148994\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.147504\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.146029\n",
            "resetting env. episode 251.000000, reward total was -19.000000. running mean: -20.134569\n",
            "resetting env. episode 252.000000, reward total was -19.000000. running mean: -20.123223\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.131991\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.140671\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.149265\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.157772\n",
            "resetting env. episode 257.000000, reward total was -19.000000. running mean: -20.146194\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.144732\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.143285\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.131852\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.130534\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.139228\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.137836\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.136458\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.145093\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.143642\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.142206\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.150784\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.149276\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -20.137783\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.136405\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.145041\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.143591\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.142155\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.140733\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.139326\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.137933\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.146553\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.145088\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.143637\n",
            "resetting env. episode 281.000000, reward total was -19.000000. running mean: -20.132201\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.140879\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.149470\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.147975\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.156495\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.164930\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.153281\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.161748\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.160131\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.158529\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.156944\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.165375\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.163721\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -20.152084\n",
            "resetting env. episode 295.000000, reward total was -19.000000. running mean: -20.140563\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.139157\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.137766\n",
            "resetting env. episode 298.000000, reward total was -19.000000. running mean: -20.126388\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.135124\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.143773\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.152335\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.150812\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.149304\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.147811\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.156333\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.164769\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.163122\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.161490\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.159875\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.168277\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.166594\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -20.154928\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.163379\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.161745\n",
            "resetting env. episode 315.000000, reward total was -19.000000. running mean: -20.150127\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.158626\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.167040\n",
            "resetting env. episode 318.000000, reward total was -18.000000. running mean: -20.145370\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.153916\n",
            "resetting env. episode 320.000000, reward total was -18.000000. running mean: -20.132377\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.131053\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.139742\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.148345\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.146862\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.155393\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.143839\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.152401\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.150877\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.149368\n",
            "resetting env. episode 330.000000, reward total was -19.000000. running mean: -20.137874\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.146495\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -20.135030\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.143680\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.152243\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.160721\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.169114\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.177423\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.185648\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.183792\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.191954\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.190034\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.198134\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.206153\n",
            "resetting env. episode 344.000000, reward total was -19.000000. running mean: -20.194091\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.192150\n",
            "resetting env. episode 346.000000, reward total was -18.000000. running mean: -20.170229\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.178526\n",
            "resetting env. episode 348.000000, reward total was -19.000000. running mean: -20.166741\n",
            "resetting env. episode 349.000000, reward total was -19.000000. running mean: -20.155074\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.153523\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.161988\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.170368\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.168664\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.166978\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.165308\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.173655\n",
            "resetting env. episode 357.000000, reward total was -18.000000. running mean: -20.151918\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.150399\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.158895\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.167306\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.175633\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.173877\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.172138\n",
            "resetting env. episode 364.000000, reward total was -19.000000. running mean: -20.160417\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -20.148812\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -20.137324\n",
            "resetting env. episode 367.000000, reward total was -19.000000. running mean: -20.125951\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.134692\n",
            "resetting env. episode 369.000000, reward total was -18.000000. running mean: -20.113345\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.122211\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.130989\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.139679\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.138282\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.136900\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.145531\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.144075\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.152634\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.151108\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.149597\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.158101\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.156520\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.164955\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.173305\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.181572\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.189757\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.197859\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.205880\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.203822\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.201783\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.189766\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.197868\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.205889\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.213830\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.211692\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.209575\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.217479\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.225305\n",
            "resetting env. episode 398.000000, reward total was -19.000000. running mean: -20.213052\n",
            "resetting env. episode 399.000000, reward total was -19.000000. running mean: -20.200921\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.198912\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.206923\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.194853\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.202905\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.200876\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -20.188867\n",
            "resetting env. episode 406.000000, reward total was -17.000000. running mean: -20.156978\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.155409\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.153855\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.162316\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.170693\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.178986\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.177196\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.185424\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.193570\n",
            "resetting env. episode 415.000000, reward total was -18.000000. running mean: -20.171634\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.179918\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.188119\n",
            "resetting env. episode 418.000000, reward total was -17.000000. running mean: -20.156237\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.164675\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.163028\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.161398\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.169784\n",
            "resetting env. episode 423.000000, reward total was -19.000000. running mean: -20.158086\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.156505\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.144940\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.153491\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.151956\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.160436\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.168832\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.167144\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.155472\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.163918\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.172278\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.180556\n",
            "resetting env. episode 435.000000, reward total was -18.000000. running mean: -20.158750\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.157163\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.165591\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.163935\n",
            "resetting env. episode 439.000000, reward total was -19.000000. running mean: -20.152296\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.160773\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.169165\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.177473\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.175699\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.183942\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -20.172102\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.160381\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.158777\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.167190\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -20.155518\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.153963\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.162423\n",
            "resetting env. episode 452.000000, reward total was -19.000000. running mean: -20.150799\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.159291\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.167698\n",
            "resetting env. episode 455.000000, reward total was -18.000000. running mean: -20.146021\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.154561\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -20.143015\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.141585\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.150169\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.148667\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -20.137181\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.135809\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.134451\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.133106\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.131775\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.140457\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.149053\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.157562\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.155987\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.164427\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.172783\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.171055\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.169344\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.167651\n",
            "resetting env. episode 475.000000, reward total was -19.000000. running mean: -20.155974\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.164414\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.172770\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.171043\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.179332\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.177539\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.185764\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -20.173906\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.182167\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.190345\n",
            "resetting env. episode 485.000000, reward total was -18.000000. running mean: -20.168442\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.176757\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.184990\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.193140\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.201208\n",
            "resetting env. episode 490.000000, reward total was -19.000000. running mean: -20.189196\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.197304\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.195331\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.203378\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.211344\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.219231\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.227038\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.224768\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.222520\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.220295\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -20.208092\n",
            "resetting env. episode 501.000000, reward total was -19.000000. running mean: -20.196011\n",
            "resetting env. episode 502.000000, reward total was -20.000000. running mean: -20.194051\n",
            "resetting env. episode 503.000000, reward total was -19.000000. running mean: -20.182111\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -20.180290\n",
            "resetting env. episode 505.000000, reward total was -20.000000. running mean: -20.178487\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.186702\n",
            "resetting env. episode 507.000000, reward total was -18.000000. running mean: -20.164835\n",
            "resetting env. episode 508.000000, reward total was -19.000000. running mean: -20.153186\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.161655\n",
            "resetting env. episode 510.000000, reward total was -18.000000. running mean: -20.140038\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.138638\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.147251\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.155779\n",
            "resetting env. episode 514.000000, reward total was -17.000000. running mean: -20.124221\n",
            "resetting env. episode 515.000000, reward total was -19.000000. running mean: -20.112979\n",
            "resetting env. episode 516.000000, reward total was -21.000000. running mean: -20.121849\n",
            "resetting env. episode 517.000000, reward total was -20.000000. running mean: -20.120631\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.129424\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.138130\n",
            "resetting env. episode 520.000000, reward total was -19.000000. running mean: -20.126749\n",
            "resetting env. episode 521.000000, reward total was -20.000000. running mean: -20.125481\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.134226\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -20.132884\n",
            "resetting env. episode 524.000000, reward total was -21.000000. running mean: -20.141555\n",
            "resetting env. episode 525.000000, reward total was -19.000000. running mean: -20.130140\n",
            "resetting env. episode 526.000000, reward total was -18.000000. running mean: -20.108838\n",
            "resetting env. episode 527.000000, reward total was -19.000000. running mean: -20.097750\n",
            "resetting env. episode 528.000000, reward total was -19.000000. running mean: -20.086772\n",
            "resetting env. episode 529.000000, reward total was -20.000000. running mean: -20.085905\n",
            "resetting env. episode 530.000000, reward total was -20.000000. running mean: -20.085046\n",
            "resetting env. episode 531.000000, reward total was -20.000000. running mean: -20.084195\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.093353\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.102420\n",
            "resetting env. episode 534.000000, reward total was -18.000000. running mean: -20.081396\n",
            "resetting env. episode 535.000000, reward total was -21.000000. running mean: -20.090582\n",
            "resetting env. episode 536.000000, reward total was -20.000000. running mean: -20.089676\n",
            "resetting env. episode 537.000000, reward total was -18.000000. running mean: -20.068779\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.078091\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.077310\n",
            "resetting env. episode 540.000000, reward total was -19.000000. running mean: -20.066537\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.075872\n",
            "resetting env. episode 542.000000, reward total was -20.000000. running mean: -20.075113\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.084362\n",
            "resetting env. episode 544.000000, reward total was -18.000000. running mean: -20.063518\n",
            "resetting env. episode 545.000000, reward total was -20.000000. running mean: -20.062883\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.072254\n",
            "resetting env. episode 547.000000, reward total was -20.000000. running mean: -20.071532\n",
            "resetting env. episode 548.000000, reward total was -20.000000. running mean: -20.070816\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.080108\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.089307\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.098414\n",
            "resetting env. episode 552.000000, reward total was -18.000000. running mean: -20.077430\n",
            "resetting env. episode 553.000000, reward total was -19.000000. running mean: -20.066656\n",
            "resetting env. episode 554.000000, reward total was -16.000000. running mean: -20.025989\n",
            "resetting env. episode 555.000000, reward total was -20.000000. running mean: -20.025729\n",
            "resetting env. episode 556.000000, reward total was -21.000000. running mean: -20.035472\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.045117\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.054666\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.064119\n",
            "resetting env. episode 560.000000, reward total was -17.000000. running mean: -20.033478\n",
            "resetting env. episode 561.000000, reward total was -21.000000. running mean: -20.043143\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.052712\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.062185\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.071563\n",
            "resetting env. episode 565.000000, reward total was -20.000000. running mean: -20.070847\n",
            "resetting env. episode 566.000000, reward total was -20.000000. running mean: -20.070139\n",
            "resetting env. episode 567.000000, reward total was -19.000000. running mean: -20.059438\n",
            "resetting env. episode 568.000000, reward total was -19.000000. running mean: -20.048843\n",
            "resetting env. episode 569.000000, reward total was -19.000000. running mean: -20.038355\n",
            "resetting env. episode 570.000000, reward total was -18.000000. running mean: -20.017971\n",
            "resetting env. episode 571.000000, reward total was -20.000000. running mean: -20.017791\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -20.017614\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -20.017437\n",
            "resetting env. episode 574.000000, reward total was -20.000000. running mean: -20.017263\n",
            "resetting env. episode 575.000000, reward total was -19.000000. running mean: -20.007090\n",
            "resetting env. episode 576.000000, reward total was -20.000000. running mean: -20.007020\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -20.006949\n",
            "resetting env. episode 578.000000, reward total was -19.000000. running mean: -19.996880\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -20.006911\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -20.006842\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.016774\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.026606\n",
            "resetting env. episode 583.000000, reward total was -19.000000. running mean: -20.016340\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.026176\n",
            "resetting env. episode 585.000000, reward total was -19.000000. running mean: -20.015915\n",
            "resetting env. episode 586.000000, reward total was -19.000000. running mean: -20.005755\n",
            "resetting env. episode 587.000000, reward total was -20.000000. running mean: -20.005698\n",
            "resetting env. episode 588.000000, reward total was -20.000000. running mean: -20.005641\n",
            "resetting env. episode 589.000000, reward total was -20.000000. running mean: -20.005584\n",
            "resetting env. episode 590.000000, reward total was -20.000000. running mean: -20.005529\n",
            "resetting env. episode 591.000000, reward total was -20.000000. running mean: -20.005473\n",
            "resetting env. episode 592.000000, reward total was -20.000000. running mean: -20.005419\n",
            "resetting env. episode 593.000000, reward total was -20.000000. running mean: -20.005364\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -20.005311\n",
            "resetting env. episode 595.000000, reward total was -20.000000. running mean: -20.005258\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -20.015205\n",
            "resetting env. episode 597.000000, reward total was -20.000000. running mean: -20.015053\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -20.024903\n",
            "resetting env. episode 599.000000, reward total was -20.000000. running mean: -20.024653\n",
            "resetting env. episode 600.000000, reward total was -19.000000. running mean: -20.014407\n",
            "resetting env. episode 601.000000, reward total was -19.000000. running mean: -20.004263\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -20.014220\n",
            "resetting env. episode 603.000000, reward total was -20.000000. running mean: -20.014078\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.023937\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.033698\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -20.033361\n",
            "resetting env. episode 607.000000, reward total was -19.000000. running mean: -20.023027\n",
            "resetting env. episode 608.000000, reward total was -20.000000. running mean: -20.022797\n",
            "resetting env. episode 609.000000, reward total was -21.000000. running mean: -20.032569\n",
            "resetting env. episode 610.000000, reward total was -20.000000. running mean: -20.032243\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.041921\n",
            "resetting env. episode 612.000000, reward total was -19.000000. running mean: -20.031502\n",
            "resetting env. episode 613.000000, reward total was -18.000000. running mean: -20.011187\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.021075\n",
            "resetting env. episode 615.000000, reward total was -20.000000. running mean: -20.020864\n",
            "resetting env. episode 616.000000, reward total was -20.000000. running mean: -20.020655\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.030449\n",
            "resetting env. episode 618.000000, reward total was -20.000000. running mean: -20.030144\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.029843\n",
            "resetting env. episode 620.000000, reward total was -20.000000. running mean: -20.029545\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.039249\n",
            "resetting env. episode 622.000000, reward total was -20.000000. running mean: -20.038857\n",
            "resetting env. episode 623.000000, reward total was -19.000000. running mean: -20.028468\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.038183\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.047802\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.057324\n",
            "resetting env. episode 627.000000, reward total was -20.000000. running mean: -20.056750\n",
            "resetting env. episode 628.000000, reward total was -20.000000. running mean: -20.056183\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.065621\n",
            "resetting env. episode 630.000000, reward total was -19.000000. running mean: -20.054965\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.064415\n",
            "resetting env. episode 632.000000, reward total was -20.000000. running mean: -20.063771\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -20.073133\n",
            "resetting env. episode 634.000000, reward total was -20.000000. running mean: -20.072402\n",
            "resetting env. episode 635.000000, reward total was -20.000000. running mean: -20.071678\n",
            "resetting env. episode 636.000000, reward total was -19.000000. running mean: -20.060961\n",
            "resetting env. episode 637.000000, reward total was -19.000000. running mean: -20.050351\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.059848\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.069249\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.078557\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.087771\n",
            "resetting env. episode 642.000000, reward total was -20.000000. running mean: -20.086894\n",
            "resetting env. episode 643.000000, reward total was -20.000000. running mean: -20.086025\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.095165\n",
            "resetting env. episode 645.000000, reward total was -20.000000. running mean: -20.094213\n",
            "resetting env. episode 646.000000, reward total was -20.000000. running mean: -20.093271\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.102338\n",
            "resetting env. episode 648.000000, reward total was -18.000000. running mean: -20.081315\n",
            "resetting env. episode 649.000000, reward total was -19.000000. running mean: -20.070502\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.079796\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.088999\n",
            "resetting env. episode 652.000000, reward total was -20.000000. running mean: -20.088109\n",
            "resetting env. episode 653.000000, reward total was -19.000000. running mean: -20.077227\n",
            "resetting env. episode 654.000000, reward total was -20.000000. running mean: -20.076455\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.085691\n",
            "resetting env. episode 656.000000, reward total was -21.000000. running mean: -20.094834\n",
            "resetting env. episode 657.000000, reward total was -19.000000. running mean: -20.083885\n",
            "resetting env. episode 658.000000, reward total was -21.000000. running mean: -20.093047\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -20.102116\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.111095\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.119984\n",
            "resetting env. episode 662.000000, reward total was -19.000000. running mean: -20.108784\n",
            "resetting env. episode 663.000000, reward total was -19.000000. running mean: -20.097696\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.106719\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.115652\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.124496\n",
            "resetting env. episode 667.000000, reward total was -20.000000. running mean: -20.123251\n",
            "resetting env. episode 668.000000, reward total was -19.000000. running mean: -20.112018\n",
            "resetting env. episode 669.000000, reward total was -18.000000. running mean: -20.090898\n",
            "resetting env. episode 670.000000, reward total was -18.000000. running mean: -20.069989\n",
            "resetting env. episode 671.000000, reward total was -21.000000. running mean: -20.079289\n",
            "resetting env. episode 672.000000, reward total was -19.000000. running mean: -20.068496\n",
            "resetting env. episode 673.000000, reward total was -19.000000. running mean: -20.057811\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.067233\n",
            "resetting env. episode 675.000000, reward total was -19.000000. running mean: -20.056561\n",
            "resetting env. episode 676.000000, reward total was -21.000000. running mean: -20.065995\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.065335\n",
            "resetting env. episode 678.000000, reward total was -20.000000. running mean: -20.064682\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -20.064035\n",
            "resetting env. episode 680.000000, reward total was -20.000000. running mean: -20.063395\n",
            "resetting env. episode 681.000000, reward total was -20.000000. running mean: -20.062761\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -20.062133\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -20.071512\n",
            "resetting env. episode 684.000000, reward total was -18.000000. running mean: -20.050797\n",
            "resetting env. episode 685.000000, reward total was -20.000000. running mean: -20.050289\n",
            "resetting env. episode 686.000000, reward total was -20.000000. running mean: -20.049786\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -20.049288\n",
            "resetting env. episode 688.000000, reward total was -19.000000. running mean: -20.038795\n",
            "resetting env. episode 689.000000, reward total was -17.000000. running mean: -20.008407\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.008323\n",
            "resetting env. episode 691.000000, reward total was -19.000000. running mean: -19.998240\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.008257\n",
            "resetting env. episode 693.000000, reward total was -20.000000. running mean: -20.008175\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.018093\n",
            "resetting env. episode 695.000000, reward total was -20.000000. running mean: -20.017912\n",
            "resetting env. episode 696.000000, reward total was -20.000000. running mean: -20.017733\n",
            "resetting env. episode 697.000000, reward total was -19.000000. running mean: -20.007556\n",
            "resetting env. episode 698.000000, reward total was -20.000000. running mean: -20.007480\n",
            "resetting env. episode 699.000000, reward total was -18.000000. running mean: -19.987405\n",
            "resetting env. episode 700.000000, reward total was -19.000000. running mean: -19.977531\n",
            "resetting env. episode 701.000000, reward total was -20.000000. running mean: -19.977756\n",
            "resetting env. episode 702.000000, reward total was -19.000000. running mean: -19.967978\n",
            "resetting env. episode 703.000000, reward total was -17.000000. running mean: -19.938299\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -19.948916\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -19.949427\n",
            "resetting env. episode 706.000000, reward total was -19.000000. running mean: -19.939932\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -19.950533\n",
            "resetting env. episode 708.000000, reward total was -21.000000. running mean: -19.961028\n",
            "resetting env. episode 709.000000, reward total was -15.000000. running mean: -19.911417\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -19.922303\n",
            "resetting env. episode 711.000000, reward total was -19.000000. running mean: -19.913080\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -19.913949\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -19.924810\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -19.925562\n",
            "resetting env. episode 715.000000, reward total was -19.000000. running mean: -19.916306\n",
            "resetting env. episode 716.000000, reward total was -20.000000. running mean: -19.917143\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -19.927972\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -19.928692\n",
            "resetting env. episode 719.000000, reward total was -20.000000. running mean: -19.929405\n",
            "resetting env. episode 720.000000, reward total was -18.000000. running mean: -19.910111\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -19.921010\n",
            "resetting env. episode 722.000000, reward total was -20.000000. running mean: -19.921800\n",
            "resetting env. episode 723.000000, reward total was -18.000000. running mean: -19.902582\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -19.913556\n",
            "resetting env. episode 725.000000, reward total was -20.000000. running mean: -19.914420\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -19.925276\n",
            "resetting env. episode 727.000000, reward total was -20.000000. running mean: -19.926023\n",
            "resetting env. episode 728.000000, reward total was -20.000000. running mean: -19.926763\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -19.937496\n",
            "resetting env. episode 730.000000, reward total was -19.000000. running mean: -19.928121\n",
            "resetting env. episode 731.000000, reward total was -20.000000. running mean: -19.928839\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -19.939551\n",
            "resetting env. episode 733.000000, reward total was -20.000000. running mean: -19.940155\n",
            "resetting env. episode 734.000000, reward total was -19.000000. running mean: -19.930754\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -19.941446\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -19.942032\n",
            "resetting env. episode 737.000000, reward total was -19.000000. running mean: -19.932612\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -19.943285\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -19.943853\n",
            "resetting env. episode 740.000000, reward total was -20.000000. running mean: -19.944414\n",
            "resetting env. episode 741.000000, reward total was -18.000000. running mean: -19.924970\n",
            "resetting env. episode 742.000000, reward total was -19.000000. running mean: -19.915720\n",
            "resetting env. episode 743.000000, reward total was -19.000000. running mean: -19.906563\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -19.917497\n",
            "resetting env. episode 745.000000, reward total was -17.000000. running mean: -19.888322\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -19.889439\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -19.900545\n",
            "resetting env. episode 748.000000, reward total was -19.000000. running mean: -19.891539\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -19.902624\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -19.913598\n",
            "resetting env. episode 751.000000, reward total was -20.000000. running mean: -19.914462\n",
            "resetting env. episode 752.000000, reward total was -20.000000. running mean: -19.915317\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -19.926164\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -19.936902\n",
            "resetting env. episode 755.000000, reward total was -20.000000. running mean: -19.937533\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -19.948158\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -19.958676\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -19.969090\n",
            "resetting env. episode 759.000000, reward total was -20.000000. running mean: -19.969399\n",
            "resetting env. episode 760.000000, reward total was -19.000000. running mean: -19.959705\n",
            "resetting env. episode 761.000000, reward total was -20.000000. running mean: -19.960108\n",
            "resetting env. episode 762.000000, reward total was -20.000000. running mean: -19.960507\n",
            "resetting env. episode 763.000000, reward total was -21.000000. running mean: -19.970902\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -19.971193\n",
            "resetting env. episode 765.000000, reward total was -20.000000. running mean: -19.971481\n",
            "resetting env. episode 766.000000, reward total was -19.000000. running mean: -19.961766\n",
            "resetting env. episode 767.000000, reward total was -20.000000. running mean: -19.962148\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -19.972527\n",
            "resetting env. episode 769.000000, reward total was -20.000000. running mean: -19.972801\n",
            "resetting env. episode 770.000000, reward total was -19.000000. running mean: -19.963073\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -19.973443\n",
            "resetting env. episode 772.000000, reward total was -18.000000. running mean: -19.953708\n",
            "resetting env. episode 773.000000, reward total was -17.000000. running mean: -19.924171\n",
            "resetting env. episode 774.000000, reward total was -19.000000. running mean: -19.914929\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -19.925780\n",
            "resetting env. episode 776.000000, reward total was -19.000000. running mean: -19.916522\n",
            "resetting env. episode 777.000000, reward total was -20.000000. running mean: -19.917357\n",
            "resetting env. episode 778.000000, reward total was -20.000000. running mean: -19.918184\n",
            "resetting env. episode 779.000000, reward total was -19.000000. running mean: -19.909002\n",
            "resetting env. episode 780.000000, reward total was -16.000000. running mean: -19.869912\n",
            "resetting env. episode 781.000000, reward total was -16.000000. running mean: -19.831213\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -19.832900\n",
            "resetting env. episode 783.000000, reward total was -20.000000. running mean: -19.834571\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -19.846226\n",
            "resetting env. episode 785.000000, reward total was -20.000000. running mean: -19.847763\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -19.859286\n",
            "resetting env. episode 787.000000, reward total was -20.000000. running mean: -19.860693\n",
            "resetting env. episode 788.000000, reward total was -19.000000. running mean: -19.852086\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -19.863565\n",
            "resetting env. episode 790.000000, reward total was -19.000000. running mean: -19.854930\n",
            "resetting env. episode 791.000000, reward total was -21.000000. running mean: -19.866380\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -19.877716\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -19.888939\n",
            "resetting env. episode 794.000000, reward total was -19.000000. running mean: -19.880050\n",
            "resetting env. episode 795.000000, reward total was -20.000000. running mean: -19.881249\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -19.892437\n",
            "resetting env. episode 797.000000, reward total was -20.000000. running mean: -19.893513\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -19.904577\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -19.915532\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -19.926376\n",
            "resetting env. episode 801.000000, reward total was -20.000000. running mean: -19.927113\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -19.937841\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -19.948463\n",
            "resetting env. episode 804.000000, reward total was -20.000000. running mean: -19.948978\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -19.959489\n",
            "resetting env. episode 806.000000, reward total was -18.000000. running mean: -19.939894\n",
            "resetting env. episode 807.000000, reward total was -19.000000. running mean: -19.930495\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -19.941190\n",
            "resetting env. episode 809.000000, reward total was -17.000000. running mean: -19.911778\n",
            "resetting env. episode 810.000000, reward total was -20.000000. running mean: -19.912660\n",
            "resetting env. episode 811.000000, reward total was -17.000000. running mean: -19.883534\n",
            "resetting env. episode 812.000000, reward total was -20.000000. running mean: -19.884698\n",
            "resetting env. episode 813.000000, reward total was -18.000000. running mean: -19.865851\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -19.877193\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -19.878421\n",
            "resetting env. episode 816.000000, reward total was -18.000000. running mean: -19.859637\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -19.871040\n",
            "resetting env. episode 818.000000, reward total was -18.000000. running mean: -19.852330\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -19.863807\n",
            "resetting env. episode 820.000000, reward total was -21.000000. running mean: -19.875168\n",
            "resetting env. episode 821.000000, reward total was -20.000000. running mean: -19.876417\n",
            "resetting env. episode 822.000000, reward total was -20.000000. running mean: -19.877653\n",
            "resetting env. episode 823.000000, reward total was -20.000000. running mean: -19.878876\n",
            "resetting env. episode 824.000000, reward total was -20.000000. running mean: -19.880087\n",
            "resetting env. episode 825.000000, reward total was -19.000000. running mean: -19.871286\n",
            "resetting env. episode 826.000000, reward total was -20.000000. running mean: -19.872574\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -19.873848\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -19.885109\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -19.896258\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -19.907296\n",
            "resetting env. episode 831.000000, reward total was -20.000000. running mean: -19.908223\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -19.919141\n",
            "resetting env. episode 833.000000, reward total was -19.000000. running mean: -19.909949\n",
            "resetting env. episode 834.000000, reward total was -21.000000. running mean: -19.920850\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -19.931641\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -19.942325\n",
            "resetting env. episode 837.000000, reward total was -19.000000. running mean: -19.932901\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -19.943572\n",
            "resetting env. episode 839.000000, reward total was -18.000000. running mean: -19.924137\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -19.934895\n",
            "resetting env. episode 841.000000, reward total was -19.000000. running mean: -19.925546\n",
            "resetting env. episode 842.000000, reward total was -20.000000. running mean: -19.926291\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -19.937028\n",
            "resetting env. episode 844.000000, reward total was -19.000000. running mean: -19.927658\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -19.938381\n",
            "resetting env. episode 846.000000, reward total was -19.000000. running mean: -19.928997\n",
            "resetting env. episode 847.000000, reward total was -19.000000. running mean: -19.919707\n",
            "resetting env. episode 848.000000, reward total was -19.000000. running mean: -19.910510\n",
            "resetting env. episode 849.000000, reward total was -20.000000. running mean: -19.911405\n",
            "resetting env. episode 850.000000, reward total was -19.000000. running mean: -19.902291\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -19.913268\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -19.924136\n",
            "resetting env. episode 853.000000, reward total was -19.000000. running mean: -19.914894\n",
            "resetting env. episode 854.000000, reward total was -20.000000. running mean: -19.915745\n",
            "resetting env. episode 855.000000, reward total was -18.000000. running mean: -19.896588\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -19.907622\n",
            "resetting env. episode 857.000000, reward total was -17.000000. running mean: -19.878546\n",
            "resetting env. episode 858.000000, reward total was -20.000000. running mean: -19.879760\n",
            "resetting env. episode 859.000000, reward total was -20.000000. running mean: -19.880963\n",
            "resetting env. episode 860.000000, reward total was -20.000000. running mean: -19.882153\n",
            "resetting env. episode 861.000000, reward total was -18.000000. running mean: -19.863331\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -19.874698\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -19.885951\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -19.897092\n",
            "resetting env. episode 865.000000, reward total was -20.000000. running mean: -19.898121\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -19.909140\n",
            "resetting env. episode 867.000000, reward total was -20.000000. running mean: -19.910048\n",
            "resetting env. episode 868.000000, reward total was -20.000000. running mean: -19.910948\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -19.921838\n",
            "resetting env. episode 870.000000, reward total was -20.000000. running mean: -19.922620\n",
            "resetting env. episode 871.000000, reward total was -17.000000. running mean: -19.893394\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -19.904460\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -19.915415\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -19.926261\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -19.926998\n",
            "resetting env. episode 876.000000, reward total was -20.000000. running mean: -19.927728\n",
            "resetting env. episode 877.000000, reward total was -18.000000. running mean: -19.908451\n",
            "resetting env. episode 878.000000, reward total was -21.000000. running mean: -19.919367\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -19.930173\n",
            "resetting env. episode 880.000000, reward total was -20.000000. running mean: -19.930871\n",
            "resetting env. episode 881.000000, reward total was -19.000000. running mean: -19.921562\n",
            "resetting env. episode 882.000000, reward total was -21.000000. running mean: -19.932347\n",
            "resetting env. episode 883.000000, reward total was -20.000000. running mean: -19.933023\n",
            "resetting env. episode 884.000000, reward total was -20.000000. running mean: -19.933693\n",
            "resetting env. episode 885.000000, reward total was -17.000000. running mean: -19.904356\n",
            "resetting env. episode 886.000000, reward total was -18.000000. running mean: -19.885313\n",
            "resetting env. episode 887.000000, reward total was -20.000000. running mean: -19.886460\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -19.897595\n",
            "resetting env. episode 889.000000, reward total was -20.000000. running mean: -19.898619\n",
            "resetting env. episode 890.000000, reward total was -20.000000. running mean: -19.899633\n",
            "resetting env. episode 891.000000, reward total was -17.000000. running mean: -19.870636\n",
            "resetting env. episode 892.000000, reward total was -20.000000. running mean: -19.871930\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -19.883211\n",
            "resetting env. episode 894.000000, reward total was -19.000000. running mean: -19.874379\n",
            "resetting env. episode 895.000000, reward total was -20.000000. running mean: -19.875635\n",
            "resetting env. episode 896.000000, reward total was -18.000000. running mean: -19.856879\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -19.868310\n",
            "resetting env. episode 898.000000, reward total was -20.000000. running mean: -19.869627\n",
            "resetting env. episode 899.000000, reward total was -20.000000. running mean: -19.870930\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -19.882221\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -19.893399\n",
            "resetting env. episode 902.000000, reward total was -20.000000. running mean: -19.894465\n",
            "resetting env. episode 903.000000, reward total was -19.000000. running mean: -19.885520\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -19.896665\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -19.907698\n",
            "resetting env. episode 906.000000, reward total was -16.000000. running mean: -19.868621\n",
            "resetting env. episode 907.000000, reward total was -18.000000. running mean: -19.849935\n",
            "resetting env. episode 908.000000, reward total was -20.000000. running mean: -19.851436\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -19.852921\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -19.864392\n",
            "resetting env. episode 911.000000, reward total was -21.000000. running mean: -19.875748\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -19.886991\n",
            "resetting env. episode 913.000000, reward total was -21.000000. running mean: -19.898121\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -19.909140\n",
            "resetting env. episode 915.000000, reward total was -21.000000. running mean: -19.920048\n",
            "resetting env. episode 916.000000, reward total was -20.000000. running mean: -19.920848\n",
            "resetting env. episode 917.000000, reward total was -20.000000. running mean: -19.921639\n",
            "resetting env. episode 918.000000, reward total was -19.000000. running mean: -19.912423\n",
            "resetting env. episode 919.000000, reward total was -20.000000. running mean: -19.913299\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -19.924166\n",
            "resetting env. episode 921.000000, reward total was -19.000000. running mean: -19.914924\n",
            "resetting env. episode 922.000000, reward total was -19.000000. running mean: -19.905775\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -19.916717\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -19.927550\n",
            "resetting env. episode 925.000000, reward total was -20.000000. running mean: -19.928274\n",
            "resetting env. episode 926.000000, reward total was -20.000000. running mean: -19.928992\n",
            "resetting env. episode 927.000000, reward total was -19.000000. running mean: -19.919702\n",
            "resetting env. episode 928.000000, reward total was -20.000000. running mean: -19.920505\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -19.931300\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -19.941987\n",
            "resetting env. episode 931.000000, reward total was -18.000000. running mean: -19.922567\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -19.933341\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -19.944008\n",
            "resetting env. episode 934.000000, reward total was -18.000000. running mean: -19.924568\n",
            "resetting env. episode 935.000000, reward total was -19.000000. running mean: -19.915322\n",
            "resetting env. episode 936.000000, reward total was -18.000000. running mean: -19.896169\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -19.897207\n",
            "resetting env. episode 938.000000, reward total was -21.000000. running mean: -19.908235\n",
            "resetting env. episode 939.000000, reward total was -21.000000. running mean: -19.919153\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -19.929961\n",
            "resetting env. episode 941.000000, reward total was -20.000000. running mean: -19.930662\n",
            "resetting env. episode 942.000000, reward total was -17.000000. running mean: -19.901355\n",
            "resetting env. episode 943.000000, reward total was -21.000000. running mean: -19.912341\n",
            "resetting env. episode 944.000000, reward total was -20.000000. running mean: -19.913218\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -19.924086\n",
            "resetting env. episode 946.000000, reward total was -20.000000. running mean: -19.924845\n",
            "resetting env. episode 947.000000, reward total was -21.000000. running mean: -19.935596\n",
            "resetting env. episode 948.000000, reward total was -20.000000. running mean: -19.936241\n",
            "resetting env. episode 949.000000, reward total was -19.000000. running mean: -19.926878\n",
            "resetting env. episode 950.000000, reward total was -20.000000. running mean: -19.927609\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -19.938333\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -19.948950\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -19.959460\n",
            "resetting env. episode 954.000000, reward total was -19.000000. running mean: -19.949866\n",
            "resetting env. episode 955.000000, reward total was -19.000000. running mean: -19.940367\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -19.950963\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -19.961454\n",
            "resetting env. episode 958.000000, reward total was -19.000000. running mean: -19.951839\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -19.962321\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -19.972698\n",
            "resetting env. episode 961.000000, reward total was -21.000000. running mean: -19.982971\n",
            "resetting env. episode 962.000000, reward total was -20.000000. running mean: -19.983141\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -19.993310\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -20.003377\n",
            "resetting env. episode 965.000000, reward total was -21.000000. running mean: -20.013343\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.023209\n",
            "resetting env. episode 967.000000, reward total was -18.000000. running mean: -20.002977\n",
            "resetting env. episode 968.000000, reward total was -19.000000. running mean: -19.992947\n",
            "resetting env. episode 969.000000, reward total was -19.000000. running mean: -19.983018\n",
            "resetting env. episode 970.000000, reward total was -19.000000. running mean: -19.973188\n",
            "resetting env. episode 971.000000, reward total was -21.000000. running mean: -19.983456\n",
            "resetting env. episode 972.000000, reward total was -20.000000. running mean: -19.983621\n",
            "resetting env. episode 973.000000, reward total was -17.000000. running mean: -19.953785\n",
            "resetting env. episode 974.000000, reward total was -20.000000. running mean: -19.954247\n",
            "resetting env. episode 975.000000, reward total was -20.000000. running mean: -19.954705\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -19.965158\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -19.975506\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -19.985751\n",
            "resetting env. episode 979.000000, reward total was -20.000000. running mean: -19.985894\n",
            "resetting env. episode 980.000000, reward total was -20.000000. running mean: -19.986035\n",
            "resetting env. episode 981.000000, reward total was -19.000000. running mean: -19.976174\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -19.986413\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -19.996548\n",
            "resetting env. episode 984.000000, reward total was -20.000000. running mean: -19.996583\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.006617\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -20.016551\n",
            "resetting env. episode 987.000000, reward total was -21.000000. running mean: -20.026385\n",
            "resetting env. episode 988.000000, reward total was -19.000000. running mean: -20.016122\n",
            "resetting env. episode 989.000000, reward total was -19.000000. running mean: -20.005960\n",
            "resetting env. episode 990.000000, reward total was -20.000000. running mean: -20.005901\n",
            "resetting env. episode 991.000000, reward total was -20.000000. running mean: -20.005842\n",
            "resetting env. episode 992.000000, reward total was -20.000000. running mean: -20.005783\n",
            "resetting env. episode 993.000000, reward total was -20.000000. running mean: -20.005726\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.015668\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.025512\n",
            "resetting env. episode 996.000000, reward total was -20.000000. running mean: -20.025256\n",
            "resetting env. episode 997.000000, reward total was -20.000000. running mean: -20.025004\n",
            "resetting env. episode 998.000000, reward total was -18.000000. running mean: -20.004754\n",
            "resetting env. episode 999.000000, reward total was -19.000000. running mean: -19.994706\n",
            "resetting env. episode 1000.000000, reward total was -18.000000. running mean: -19.974759\n",
            "resetting env. episode 1001.000000, reward total was -20.000000. running mean: -19.975012\n",
            "resetting env. episode 1002.000000, reward total was -21.000000. running mean: -19.985262\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -19.995409\n",
            "resetting env. episode 1004.000000, reward total was -19.000000. running mean: -19.985455\n",
            "resetting env. episode 1005.000000, reward total was -18.000000. running mean: -19.965600\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -19.975944\n",
            "resetting env. episode 1007.000000, reward total was -18.000000. running mean: -19.956185\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -19.966623\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -19.976957\n",
            "resetting env. episode 1010.000000, reward total was -18.000000. running mean: -19.957187\n",
            "resetting env. episode 1011.000000, reward total was -20.000000. running mean: -19.957615\n",
            "resetting env. episode 1012.000000, reward total was -21.000000. running mean: -19.968039\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -19.978359\n",
            "resetting env. episode 1014.000000, reward total was -19.000000. running mean: -19.968575\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -19.978889\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -19.979101\n",
            "resetting env. episode 1017.000000, reward total was -20.000000. running mean: -19.979310\n",
            "resetting env. episode 1018.000000, reward total was -19.000000. running mean: -19.969516\n",
            "resetting env. episode 1019.000000, reward total was -21.000000. running mean: -19.979821\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -19.990023\n",
            "resetting env. episode 1021.000000, reward total was -19.000000. running mean: -19.980123\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -19.990322\n",
            "resetting env. episode 1023.000000, reward total was -18.000000. running mean: -19.970418\n",
            "resetting env. episode 1024.000000, reward total was -14.000000. running mean: -19.910714\n",
            "resetting env. episode 1025.000000, reward total was -19.000000. running mean: -19.901607\n",
            "resetting env. episode 1026.000000, reward total was -20.000000. running mean: -19.902591\n",
            "resetting env. episode 1027.000000, reward total was -20.000000. running mean: -19.903565\n",
            "resetting env. episode 1028.000000, reward total was -18.000000. running mean: -19.884529\n",
            "resetting env. episode 1029.000000, reward total was -18.000000. running mean: -19.865684\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -19.867027\n",
            "resetting env. episode 1031.000000, reward total was -20.000000. running mean: -19.868357\n",
            "resetting env. episode 1032.000000, reward total was -21.000000. running mean: -19.879673\n",
            "resetting env. episode 1033.000000, reward total was -18.000000. running mean: -19.860877\n",
            "resetting env. episode 1034.000000, reward total was -19.000000. running mean: -19.852268\n",
            "resetting env. episode 1035.000000, reward total was -20.000000. running mean: -19.853745\n",
            "resetting env. episode 1036.000000, reward total was -19.000000. running mean: -19.845208\n",
            "resetting env. episode 1037.000000, reward total was -19.000000. running mean: -19.836756\n",
            "resetting env. episode 1038.000000, reward total was -19.000000. running mean: -19.828388\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -19.840104\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -19.851703\n",
            "resetting env. episode 1041.000000, reward total was -19.000000. running mean: -19.843186\n",
            "resetting env. episode 1042.000000, reward total was -20.000000. running mean: -19.844754\n",
            "resetting env. episode 1043.000000, reward total was -19.000000. running mean: -19.836307\n",
            "resetting env. episode 1044.000000, reward total was -21.000000. running mean: -19.847944\n",
            "resetting env. episode 1045.000000, reward total was -20.000000. running mean: -19.849464\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -19.860970\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -19.872360\n",
            "resetting env. episode 1048.000000, reward total was -20.000000. running mean: -19.873636\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -19.874900\n",
            "resetting env. episode 1050.000000, reward total was -20.000000. running mean: -19.876151\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -19.887390\n",
            "resetting env. episode 1052.000000, reward total was -18.000000. running mean: -19.868516\n",
            "resetting env. episode 1053.000000, reward total was -18.000000. running mean: -19.849830\n",
            "resetting env. episode 1054.000000, reward total was -19.000000. running mean: -19.841332\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -19.852919\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -19.864390\n",
            "resetting env. episode 1057.000000, reward total was -20.000000. running mean: -19.865746\n",
            "resetting env. episode 1058.000000, reward total was -18.000000. running mean: -19.847088\n",
            "resetting env. episode 1059.000000, reward total was -16.000000. running mean: -19.808617\n",
            "resetting env. episode 1060.000000, reward total was -20.000000. running mean: -19.810531\n",
            "resetting env. episode 1061.000000, reward total was -20.000000. running mean: -19.812426\n",
            "resetting env. episode 1062.000000, reward total was -19.000000. running mean: -19.804302\n",
            "resetting env. episode 1063.000000, reward total was -21.000000. running mean: -19.816259\n",
            "resetting env. episode 1064.000000, reward total was -19.000000. running mean: -19.808096\n",
            "resetting env. episode 1065.000000, reward total was -20.000000. running mean: -19.810015\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -19.811915\n",
            "resetting env. episode 1067.000000, reward total was -20.000000. running mean: -19.813796\n",
            "resetting env. episode 1068.000000, reward total was -17.000000. running mean: -19.785658\n",
            "resetting env. episode 1069.000000, reward total was -20.000000. running mean: -19.787801\n",
            "resetting env. episode 1070.000000, reward total was -20.000000. running mean: -19.789923\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -19.802024\n",
            "resetting env. episode 1072.000000, reward total was -21.000000. running mean: -19.814004\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -19.825864\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -19.837605\n",
            "resetting env. episode 1075.000000, reward total was -19.000000. running mean: -19.829229\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -19.840937\n",
            "resetting env. episode 1077.000000, reward total was -17.000000. running mean: -19.812527\n",
            "resetting env. episode 1078.000000, reward total was -20.000000. running mean: -19.814402\n",
            "resetting env. episode 1079.000000, reward total was -19.000000. running mean: -19.806258\n",
            "resetting env. episode 1080.000000, reward total was -19.000000. running mean: -19.798196\n",
            "resetting env. episode 1081.000000, reward total was -19.000000. running mean: -19.790214\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -19.802311\n",
            "resetting env. episode 1083.000000, reward total was -19.000000. running mean: -19.794288\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -19.806345\n",
            "resetting env. episode 1085.000000, reward total was -19.000000. running mean: -19.798282\n",
            "resetting env. episode 1086.000000, reward total was -19.000000. running mean: -19.790299\n",
            "resetting env. episode 1087.000000, reward total was -20.000000. running mean: -19.792396\n",
            "resetting env. episode 1088.000000, reward total was -20.000000. running mean: -19.794472\n",
            "resetting env. episode 1089.000000, reward total was -19.000000. running mean: -19.786527\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -19.798662\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -19.810676\n",
            "resetting env. episode 1092.000000, reward total was -20.000000. running mean: -19.812569\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -19.824443\n",
            "resetting env. episode 1094.000000, reward total was -19.000000. running mean: -19.816199\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -19.828037\n",
            "resetting env. episode 1096.000000, reward total was -18.000000. running mean: -19.809756\n",
            "resetting env. episode 1097.000000, reward total was -19.000000. running mean: -19.801659\n",
            "resetting env. episode 1098.000000, reward total was -19.000000. running mean: -19.793642\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -19.805706\n",
            "resetting env. episode 1100.000000, reward total was -21.000000. running mean: -19.817649\n",
            "resetting env. episode 1101.000000, reward total was -21.000000. running mean: -19.829472\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -19.841178\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -19.852766\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -19.864238\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -19.875596\n",
            "resetting env. episode 1106.000000, reward total was -20.000000. running mean: -19.876840\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -19.888071\n",
            "resetting env. episode 1108.000000, reward total was -19.000000. running mean: -19.879191\n",
            "resetting env. episode 1109.000000, reward total was -20.000000. running mean: -19.880399\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -19.891595\n",
            "resetting env. episode 1111.000000, reward total was -20.000000. running mean: -19.892679\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -19.893752\n",
            "resetting env. episode 1113.000000, reward total was -19.000000. running mean: -19.884814\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -19.885966\n",
            "resetting env. episode 1115.000000, reward total was -19.000000. running mean: -19.877107\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -19.888336\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -19.899452\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -19.910458\n",
            "resetting env. episode 1119.000000, reward total was -18.000000. running mean: -19.891353\n",
            "resetting env. episode 1120.000000, reward total was -20.000000. running mean: -19.892440\n",
            "resetting env. episode 1121.000000, reward total was -20.000000. running mean: -19.893515\n",
            "resetting env. episode 1122.000000, reward total was -20.000000. running mean: -19.894580\n",
            "resetting env. episode 1123.000000, reward total was -20.000000. running mean: -19.895634\n",
            "resetting env. episode 1124.000000, reward total was -17.000000. running mean: -19.866678\n",
            "resetting env. episode 1125.000000, reward total was -14.000000. running mean: -19.808011\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -19.819931\n",
            "resetting env. episode 1127.000000, reward total was -19.000000. running mean: -19.811732\n",
            "resetting env. episode 1128.000000, reward total was -21.000000. running mean: -19.823614\n",
            "resetting env. episode 1129.000000, reward total was -19.000000. running mean: -19.815378\n",
            "resetting env. episode 1130.000000, reward total was -21.000000. running mean: -19.827224\n",
            "resetting env. episode 1131.000000, reward total was -19.000000. running mean: -19.818952\n",
            "resetting env. episode 1132.000000, reward total was -20.000000. running mean: -19.820763\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -19.832555\n",
            "resetting env. episode 1134.000000, reward total was -19.000000. running mean: -19.824230\n",
            "resetting env. episode 1135.000000, reward total was -19.000000. running mean: -19.815987\n",
            "resetting env. episode 1136.000000, reward total was -21.000000. running mean: -19.827827\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -19.839549\n",
            "resetting env. episode 1138.000000, reward total was -20.000000. running mean: -19.841154\n",
            "resetting env. episode 1139.000000, reward total was -20.000000. running mean: -19.842742\n",
            "resetting env. episode 1140.000000, reward total was -20.000000. running mean: -19.844315\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -19.855872\n",
            "resetting env. episode 1142.000000, reward total was -19.000000. running mean: -19.847313\n",
            "resetting env. episode 1143.000000, reward total was -17.000000. running mean: -19.818840\n",
            "resetting env. episode 1144.000000, reward total was -20.000000. running mean: -19.820651\n",
            "resetting env. episode 1145.000000, reward total was -18.000000. running mean: -19.802445\n",
            "resetting env. episode 1146.000000, reward total was -18.000000. running mean: -19.784420\n",
            "resetting env. episode 1147.000000, reward total was -20.000000. running mean: -19.786576\n",
            "resetting env. episode 1148.000000, reward total was -19.000000. running mean: -19.778710\n",
            "resetting env. episode 1149.000000, reward total was -20.000000. running mean: -19.780923\n",
            "resetting env. episode 1150.000000, reward total was -19.000000. running mean: -19.773114\n",
            "resetting env. episode 1151.000000, reward total was -20.000000. running mean: -19.775383\n",
            "resetting env. episode 1152.000000, reward total was -20.000000. running mean: -19.777629\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -19.789853\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -19.801954\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -19.813935\n",
            "resetting env. episode 1156.000000, reward total was -18.000000. running mean: -19.795795\n",
            "resetting env. episode 1157.000000, reward total was -19.000000. running mean: -19.787837\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -19.799959\n",
            "resetting env. episode 1159.000000, reward total was -19.000000. running mean: -19.791959\n",
            "resetting env. episode 1160.000000, reward total was -20.000000. running mean: -19.794040\n",
            "resetting env. episode 1161.000000, reward total was -21.000000. running mean: -19.806099\n",
            "resetting env. episode 1162.000000, reward total was -19.000000. running mean: -19.798038\n",
            "resetting env. episode 1163.000000, reward total was -19.000000. running mean: -19.790058\n",
            "resetting env. episode 1164.000000, reward total was -20.000000. running mean: -19.792157\n",
            "resetting env. episode 1165.000000, reward total was -19.000000. running mean: -19.784236\n",
            "resetting env. episode 1166.000000, reward total was -20.000000. running mean: -19.786394\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -19.798530\n",
            "resetting env. episode 1168.000000, reward total was -21.000000. running mean: -19.810544\n",
            "resetting env. episode 1169.000000, reward total was -20.000000. running mean: -19.812439\n",
            "resetting env. episode 1170.000000, reward total was -20.000000. running mean: -19.814314\n",
            "resetting env. episode 1171.000000, reward total was -20.000000. running mean: -19.816171\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -19.818010\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -19.829830\n",
            "resetting env. episode 1174.000000, reward total was -18.000000. running mean: -19.811531\n",
            "resetting env. episode 1175.000000, reward total was -15.000000. running mean: -19.763416\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -19.765782\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -19.778124\n",
            "resetting env. episode 1178.000000, reward total was -20.000000. running mean: -19.780343\n",
            "resetting env. episode 1179.000000, reward total was -20.000000. running mean: -19.782539\n",
            "resetting env. episode 1180.000000, reward total was -19.000000. running mean: -19.774714\n",
            "resetting env. episode 1181.000000, reward total was -20.000000. running mean: -19.776967\n",
            "resetting env. episode 1182.000000, reward total was -19.000000. running mean: -19.769197\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -19.781505\n",
            "resetting env. episode 1184.000000, reward total was -20.000000. running mean: -19.783690\n",
            "resetting env. episode 1185.000000, reward total was -21.000000. running mean: -19.795853\n",
            "resetting env. episode 1186.000000, reward total was -18.000000. running mean: -19.777895\n",
            "resetting env. episode 1187.000000, reward total was -20.000000. running mean: -19.780116\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -19.792315\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -19.804391\n",
            "resetting env. episode 1190.000000, reward total was -19.000000. running mean: -19.796347\n",
            "resetting env. episode 1191.000000, reward total was -21.000000. running mean: -19.808384\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -19.820300\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -19.832097\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -19.843776\n",
            "resetting env. episode 1195.000000, reward total was -20.000000. running mean: -19.845338\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -19.846885\n",
            "resetting env. episode 1197.000000, reward total was -20.000000. running mean: -19.848416\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -19.859932\n",
            "resetting env. episode 1199.000000, reward total was -20.000000. running mean: -19.861333\n",
            "resetting env. episode 1200.000000, reward total was -20.000000. running mean: -19.862719\n",
            "resetting env. episode 1201.000000, reward total was -21.000000. running mean: -19.874092\n",
            "resetting env. episode 1202.000000, reward total was -18.000000. running mean: -19.855351\n",
            "resetting env. episode 1203.000000, reward total was -20.000000. running mean: -19.856798\n",
            "resetting env. episode 1204.000000, reward total was -20.000000. running mean: -19.858230\n",
            "resetting env. episode 1205.000000, reward total was -20.000000. running mean: -19.859647\n",
            "resetting env. episode 1206.000000, reward total was -21.000000. running mean: -19.871051\n",
            "resetting env. episode 1207.000000, reward total was -20.000000. running mean: -19.872340\n",
            "resetting env. episode 1208.000000, reward total was -20.000000. running mean: -19.873617\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -19.884881\n",
            "resetting env. episode 1210.000000, reward total was -20.000000. running mean: -19.886032\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -19.897172\n",
            "resetting env. episode 1212.000000, reward total was -20.000000. running mean: -19.898200\n",
            "resetting env. episode 1213.000000, reward total was -20.000000. running mean: -19.899218\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -19.900226\n",
            "resetting env. episode 1215.000000, reward total was -20.000000. running mean: -19.901224\n",
            "resetting env. episode 1216.000000, reward total was -21.000000. running mean: -19.912211\n",
            "resetting env. episode 1217.000000, reward total was -19.000000. running mean: -19.903089\n",
            "resetting env. episode 1218.000000, reward total was -20.000000. running mean: -19.904058\n",
            "resetting env. episode 1219.000000, reward total was -20.000000. running mean: -19.905018\n",
            "resetting env. episode 1220.000000, reward total was -20.000000. running mean: -19.905968\n",
            "resetting env. episode 1221.000000, reward total was -20.000000. running mean: -19.906908\n",
            "resetting env. episode 1222.000000, reward total was -18.000000. running mean: -19.887839\n",
            "resetting env. episode 1223.000000, reward total was -20.000000. running mean: -19.888960\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -19.900071\n",
            "resetting env. episode 1225.000000, reward total was -18.000000. running mean: -19.881070\n",
            "resetting env. episode 1226.000000, reward total was -20.000000. running mean: -19.882259\n",
            "resetting env. episode 1227.000000, reward total was -19.000000. running mean: -19.873437\n",
            "resetting env. episode 1228.000000, reward total was -19.000000. running mean: -19.864702\n",
            "resetting env. episode 1229.000000, reward total was -20.000000. running mean: -19.866055\n",
            "resetting env. episode 1230.000000, reward total was -19.000000. running mean: -19.857395\n",
            "resetting env. episode 1231.000000, reward total was -20.000000. running mean: -19.858821\n",
            "resetting env. episode 1232.000000, reward total was -20.000000. running mean: -19.860233\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -19.871630\n",
            "resetting env. episode 1234.000000, reward total was -19.000000. running mean: -19.862914\n",
            "resetting env. episode 1235.000000, reward total was -18.000000. running mean: -19.844285\n",
            "resetting env. episode 1236.000000, reward total was -20.000000. running mean: -19.845842\n",
            "resetting env. episode 1237.000000, reward total was -16.000000. running mean: -19.807384\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -19.819310\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -19.831117\n",
            "resetting env. episode 1240.000000, reward total was -21.000000. running mean: -19.842806\n",
            "resetting env. episode 1241.000000, reward total was -20.000000. running mean: -19.844378\n",
            "resetting env. episode 1242.000000, reward total was -19.000000. running mean: -19.835934\n",
            "resetting env. episode 1243.000000, reward total was -19.000000. running mean: -19.827574\n",
            "resetting env. episode 1244.000000, reward total was -20.000000. running mean: -19.829299\n",
            "resetting env. episode 1245.000000, reward total was -19.000000. running mean: -19.821006\n",
            "resetting env. episode 1246.000000, reward total was -20.000000. running mean: -19.822796\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -19.834568\n",
            "resetting env. episode 1248.000000, reward total was -19.000000. running mean: -19.826222\n",
            "resetting env. episode 1249.000000, reward total was -20.000000. running mean: -19.827960\n",
            "resetting env. episode 1250.000000, reward total was -18.000000. running mean: -19.809680\n",
            "resetting env. episode 1251.000000, reward total was -21.000000. running mean: -19.821583\n",
            "resetting env. episode 1252.000000, reward total was -15.000000. running mean: -19.773368\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -19.785634\n",
            "resetting env. episode 1254.000000, reward total was -19.000000. running mean: -19.777778\n",
            "resetting env. episode 1255.000000, reward total was -19.000000. running mean: -19.770000\n",
            "resetting env. episode 1256.000000, reward total was -19.000000. running mean: -19.762300\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -19.774677\n",
            "resetting env. episode 1258.000000, reward total was -20.000000. running mean: -19.776930\n",
            "resetting env. episode 1259.000000, reward total was -18.000000. running mean: -19.759161\n",
            "resetting env. episode 1260.000000, reward total was -19.000000. running mean: -19.751569\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -19.764053\n",
            "resetting env. episode 1262.000000, reward total was -19.000000. running mean: -19.756413\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -19.768849\n",
            "resetting env. episode 1264.000000, reward total was -20.000000. running mean: -19.771160\n",
            "resetting env. episode 1265.000000, reward total was -19.000000. running mean: -19.763449\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -19.775814\n",
            "resetting env. episode 1267.000000, reward total was -20.000000. running mean: -19.778056\n",
            "resetting env. episode 1268.000000, reward total was -19.000000. running mean: -19.770275\n",
            "resetting env. episode 1269.000000, reward total was -20.000000. running mean: -19.772573\n",
            "resetting env. episode 1270.000000, reward total was -20.000000. running mean: -19.774847\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -19.777099\n",
            "resetting env. episode 1272.000000, reward total was -18.000000. running mean: -19.759328\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -19.771734\n",
            "resetting env. episode 1274.000000, reward total was -20.000000. running mean: -19.774017\n",
            "resetting env. episode 1275.000000, reward total was -19.000000. running mean: -19.766277\n",
            "resetting env. episode 1276.000000, reward total was -20.000000. running mean: -19.768614\n",
            "resetting env. episode 1277.000000, reward total was -20.000000. running mean: -19.770928\n",
            "resetting env. episode 1278.000000, reward total was -20.000000. running mean: -19.773219\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -19.785486\n",
            "resetting env. episode 1280.000000, reward total was -17.000000. running mean: -19.757632\n",
            "resetting env. episode 1281.000000, reward total was -19.000000. running mean: -19.750055\n",
            "resetting env. episode 1282.000000, reward total was -20.000000. running mean: -19.752555\n",
            "resetting env. episode 1283.000000, reward total was -18.000000. running mean: -19.735029\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -19.747679\n",
            "resetting env. episode 1285.000000, reward total was -20.000000. running mean: -19.750202\n",
            "resetting env. episode 1286.000000, reward total was -19.000000. running mean: -19.742700\n",
            "resetting env. episode 1287.000000, reward total was -20.000000. running mean: -19.745273\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -19.747820\n",
            "resetting env. episode 1289.000000, reward total was -19.000000. running mean: -19.740342\n",
            "resetting env. episode 1290.000000, reward total was -20.000000. running mean: -19.742939\n",
            "resetting env. episode 1291.000000, reward total was -20.000000. running mean: -19.745509\n",
            "resetting env. episode 1292.000000, reward total was -19.000000. running mean: -19.738054\n",
            "resetting env. episode 1293.000000, reward total was -20.000000. running mean: -19.740674\n",
            "resetting env. episode 1294.000000, reward total was -20.000000. running mean: -19.743267\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -19.755834\n",
            "resetting env. episode 1296.000000, reward total was -18.000000. running mean: -19.738276\n",
            "resetting env. episode 1297.000000, reward total was -20.000000. running mean: -19.740893\n",
            "resetting env. episode 1298.000000, reward total was -19.000000. running mean: -19.733484\n",
            "resetting env. episode 1299.000000, reward total was -19.000000. running mean: -19.726149\n",
            "resetting env. episode 1300.000000, reward total was -19.000000. running mean: -19.718888\n",
            "resetting env. episode 1301.000000, reward total was -20.000000. running mean: -19.721699\n",
            "resetting env. episode 1302.000000, reward total was -20.000000. running mean: -19.724482\n",
            "resetting env. episode 1303.000000, reward total was -20.000000. running mean: -19.727237\n",
            "resetting env. episode 1304.000000, reward total was -19.000000. running mean: -19.719965\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -19.732765\n",
            "resetting env. episode 1306.000000, reward total was -20.000000. running mean: -19.735437\n",
            "resetting env. episode 1307.000000, reward total was -17.000000. running mean: -19.708083\n",
            "resetting env. episode 1308.000000, reward total was -17.000000. running mean: -19.681002\n",
            "resetting env. episode 1309.000000, reward total was -19.000000. running mean: -19.674192\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -19.687450\n",
            "resetting env. episode 1311.000000, reward total was -20.000000. running mean: -19.690576\n",
            "resetting env. episode 1312.000000, reward total was -18.000000. running mean: -19.673670\n",
            "resetting env. episode 1313.000000, reward total was -21.000000. running mean: -19.686933\n",
            "resetting env. episode 1314.000000, reward total was -17.000000. running mean: -19.660064\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -19.663463\n",
            "resetting env. episode 1316.000000, reward total was -20.000000. running mean: -19.666829\n",
            "resetting env. episode 1317.000000, reward total was -19.000000. running mean: -19.660160\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -19.673559\n",
            "resetting env. episode 1319.000000, reward total was -20.000000. running mean: -19.676823\n",
            "resetting env. episode 1320.000000, reward total was -17.000000. running mean: -19.650055\n",
            "resetting env. episode 1321.000000, reward total was -16.000000. running mean: -19.613555\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -19.617419\n",
            "resetting env. episode 1323.000000, reward total was -18.000000. running mean: -19.601245\n",
            "resetting env. episode 1324.000000, reward total was -18.000000. running mean: -19.585232\n",
            "resetting env. episode 1325.000000, reward total was -18.000000. running mean: -19.569380\n",
            "resetting env. episode 1326.000000, reward total was -18.000000. running mean: -19.553686\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -19.568149\n",
            "resetting env. episode 1328.000000, reward total was -19.000000. running mean: -19.562468\n",
            "resetting env. episode 1329.000000, reward total was -19.000000. running mean: -19.556843\n",
            "resetting env. episode 1330.000000, reward total was -19.000000. running mean: -19.551275\n",
            "resetting env. episode 1331.000000, reward total was -19.000000. running mean: -19.545762\n",
            "resetting env. episode 1332.000000, reward total was -20.000000. running mean: -19.550304\n",
            "resetting env. episode 1333.000000, reward total was -19.000000. running mean: -19.544801\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -19.559353\n",
            "resetting env. episode 1335.000000, reward total was -20.000000. running mean: -19.563760\n",
            "resetting env. episode 1336.000000, reward total was -17.000000. running mean: -19.538122\n",
            "resetting env. episode 1337.000000, reward total was -20.000000. running mean: -19.542741\n",
            "resetting env. episode 1338.000000, reward total was -20.000000. running mean: -19.547314\n",
            "resetting env. episode 1339.000000, reward total was -18.000000. running mean: -19.531840\n",
            "resetting env. episode 1340.000000, reward total was -19.000000. running mean: -19.526522\n",
            "resetting env. episode 1341.000000, reward total was -18.000000. running mean: -19.511257\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -19.516144\n",
            "resetting env. episode 1343.000000, reward total was -20.000000. running mean: -19.520983\n",
            "resetting env. episode 1344.000000, reward total was -19.000000. running mean: -19.515773\n",
            "resetting env. episode 1345.000000, reward total was -18.000000. running mean: -19.500615\n",
            "resetting env. episode 1346.000000, reward total was -19.000000. running mean: -19.495609\n",
            "resetting env. episode 1347.000000, reward total was -19.000000. running mean: -19.490653\n",
            "resetting env. episode 1348.000000, reward total was -20.000000. running mean: -19.495746\n",
            "resetting env. episode 1349.000000, reward total was -18.000000. running mean: -19.480789\n",
            "resetting env. episode 1350.000000, reward total was -19.000000. running mean: -19.475981\n",
            "resetting env. episode 1351.000000, reward total was -18.000000. running mean: -19.461221\n",
            "resetting env. episode 1352.000000, reward total was -21.000000. running mean: -19.476609\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -19.491843\n",
            "resetting env. episode 1354.000000, reward total was -20.000000. running mean: -19.496925\n",
            "resetting env. episode 1355.000000, reward total was -21.000000. running mean: -19.511955\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -19.526836\n",
            "resetting env. episode 1357.000000, reward total was -21.000000. running mean: -19.541567\n",
            "resetting env. episode 1358.000000, reward total was -19.000000. running mean: -19.536152\n",
            "resetting env. episode 1359.000000, reward total was -20.000000. running mean: -19.540790\n",
            "resetting env. episode 1360.000000, reward total was -19.000000. running mean: -19.535382\n",
            "resetting env. episode 1361.000000, reward total was -19.000000. running mean: -19.530028\n",
            "resetting env. episode 1362.000000, reward total was -19.000000. running mean: -19.524728\n",
            "resetting env. episode 1363.000000, reward total was -19.000000. running mean: -19.519481\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -19.534286\n",
            "resetting env. episode 1365.000000, reward total was -19.000000. running mean: -19.528943\n",
            "resetting env. episode 1366.000000, reward total was -18.000000. running mean: -19.513654\n",
            "resetting env. episode 1367.000000, reward total was -19.000000. running mean: -19.508517\n",
            "resetting env. episode 1368.000000, reward total was -21.000000. running mean: -19.523432\n",
            "resetting env. episode 1369.000000, reward total was -19.000000. running mean: -19.518198\n",
            "resetting env. episode 1370.000000, reward total was -20.000000. running mean: -19.523016\n",
            "resetting env. episode 1371.000000, reward total was -20.000000. running mean: -19.527786\n",
            "resetting env. episode 1372.000000, reward total was -18.000000. running mean: -19.512508\n",
            "resetting env. episode 1373.000000, reward total was -20.000000. running mean: -19.517383\n",
            "resetting env. episode 1374.000000, reward total was -20.000000. running mean: -19.522209\n",
            "resetting env. episode 1375.000000, reward total was -18.000000. running mean: -19.506987\n",
            "resetting env. episode 1376.000000, reward total was -17.000000. running mean: -19.481917\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -19.497098\n",
            "resetting env. episode 1378.000000, reward total was -19.000000. running mean: -19.492127\n",
            "resetting env. episode 1379.000000, reward total was -20.000000. running mean: -19.497206\n",
            "resetting env. episode 1380.000000, reward total was -20.000000. running mean: -19.502233\n",
            "resetting env. episode 1381.000000, reward total was -20.000000. running mean: -19.507211\n",
            "resetting env. episode 1382.000000, reward total was -20.000000. running mean: -19.512139\n",
            "resetting env. episode 1383.000000, reward total was -21.000000. running mean: -19.527018\n",
            "resetting env. episode 1384.000000, reward total was -19.000000. running mean: -19.521747\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -19.536530\n",
            "resetting env. episode 1386.000000, reward total was -20.000000. running mean: -19.541165\n",
            "resetting env. episode 1387.000000, reward total was -20.000000. running mean: -19.545753\n",
            "resetting env. episode 1388.000000, reward total was -19.000000. running mean: -19.540295\n",
            "resetting env. episode 1389.000000, reward total was -19.000000. running mean: -19.534893\n",
            "resetting env. episode 1390.000000, reward total was -21.000000. running mean: -19.549544\n",
            "resetting env. episode 1391.000000, reward total was -20.000000. running mean: -19.554048\n",
            "resetting env. episode 1392.000000, reward total was -21.000000. running mean: -19.568508\n",
            "resetting env. episode 1393.000000, reward total was -20.000000. running mean: -19.572823\n",
            "resetting env. episode 1394.000000, reward total was -19.000000. running mean: -19.567094\n",
            "resetting env. episode 1395.000000, reward total was -20.000000. running mean: -19.571423\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -19.585709\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -19.599852\n",
            "resetting env. episode 1398.000000, reward total was -21.000000. running mean: -19.613854\n",
            "resetting env. episode 1399.000000, reward total was -19.000000. running mean: -19.607715\n",
            "resetting env. episode 1400.000000, reward total was -19.000000. running mean: -19.601638\n",
            "resetting env. episode 1401.000000, reward total was -21.000000. running mean: -19.615622\n",
            "resetting env. episode 1402.000000, reward total was -20.000000. running mean: -19.619465\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -19.633271\n",
            "resetting env. episode 1404.000000, reward total was -20.000000. running mean: -19.636938\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -19.650569\n",
            "resetting env. episode 1406.000000, reward total was -20.000000. running mean: -19.654063\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -19.667522\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -19.670847\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -19.684139\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -19.697297\n",
            "resetting env. episode 1411.000000, reward total was -20.000000. running mean: -19.700324\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -19.713321\n",
            "resetting env. episode 1413.000000, reward total was -20.000000. running mean: -19.716188\n",
            "resetting env. episode 1414.000000, reward total was -18.000000. running mean: -19.699026\n",
            "resetting env. episode 1415.000000, reward total was -20.000000. running mean: -19.702036\n",
            "resetting env. episode 1416.000000, reward total was -20.000000. running mean: -19.705015\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -19.707965\n",
            "resetting env. episode 1418.000000, reward total was -17.000000. running mean: -19.680885\n",
            "resetting env. episode 1419.000000, reward total was -21.000000. running mean: -19.694077\n",
            "resetting env. episode 1420.000000, reward total was -21.000000. running mean: -19.707136\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -19.710064\n",
            "resetting env. episode 1422.000000, reward total was -19.000000. running mean: -19.702964\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -19.715934\n",
            "resetting env. episode 1424.000000, reward total was -20.000000. running mean: -19.718775\n",
            "resetting env. episode 1425.000000, reward total was -19.000000. running mean: -19.711587\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -19.724471\n",
            "resetting env. episode 1427.000000, reward total was -20.000000. running mean: -19.727227\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -19.739954\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -19.742555\n",
            "resetting env. episode 1430.000000, reward total was -20.000000. running mean: -19.745129\n",
            "resetting env. episode 1431.000000, reward total was -20.000000. running mean: -19.747678\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -19.760201\n",
            "resetting env. episode 1433.000000, reward total was -17.000000. running mean: -19.732599\n",
            "resetting env. episode 1434.000000, reward total was -20.000000. running mean: -19.735273\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -19.747920\n",
            "resetting env. episode 1436.000000, reward total was -18.000000. running mean: -19.730441\n",
            "resetting env. episode 1437.000000, reward total was -19.000000. running mean: -19.723137\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -19.735905\n",
            "resetting env. episode 1439.000000, reward total was -20.000000. running mean: -19.738546\n",
            "resetting env. episode 1440.000000, reward total was -19.000000. running mean: -19.731161\n",
            "resetting env. episode 1441.000000, reward total was -19.000000. running mean: -19.723849\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -19.736611\n",
            "resetting env. episode 1443.000000, reward total was -20.000000. running mean: -19.739245\n",
            "resetting env. episode 1444.000000, reward total was -20.000000. running mean: -19.741852\n",
            "resetting env. episode 1445.000000, reward total was -21.000000. running mean: -19.754434\n",
            "resetting env. episode 1446.000000, reward total was -20.000000. running mean: -19.756889\n",
            "resetting env. episode 1447.000000, reward total was -21.000000. running mean: -19.769320\n",
            "resetting env. episode 1448.000000, reward total was -18.000000. running mean: -19.751627\n",
            "resetting env. episode 1449.000000, reward total was -19.000000. running mean: -19.744111\n",
            "resetting env. episode 1450.000000, reward total was -18.000000. running mean: -19.726670\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -19.739403\n",
            "resetting env. episode 1452.000000, reward total was -20.000000. running mean: -19.742009\n",
            "resetting env. episode 1453.000000, reward total was -19.000000. running mean: -19.734589\n",
            "resetting env. episode 1454.000000, reward total was -20.000000. running mean: -19.737243\n",
            "resetting env. episode 1455.000000, reward total was -16.000000. running mean: -19.699871\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -19.712872\n",
            "resetting env. episode 1457.000000, reward total was -20.000000. running mean: -19.715743\n",
            "resetting env. episode 1458.000000, reward total was -20.000000. running mean: -19.718586\n",
            "resetting env. episode 1459.000000, reward total was -19.000000. running mean: -19.711400\n",
            "resetting env. episode 1460.000000, reward total was -19.000000. running mean: -19.704286\n",
            "resetting env. episode 1461.000000, reward total was -18.000000. running mean: -19.687243\n",
            "resetting env. episode 1462.000000, reward total was -16.000000. running mean: -19.650371\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -19.663867\n",
            "resetting env. episode 1464.000000, reward total was -19.000000. running mean: -19.657228\n",
            "resetting env. episode 1465.000000, reward total was -19.000000. running mean: -19.650656\n",
            "resetting env. episode 1466.000000, reward total was -20.000000. running mean: -19.654149\n",
            "resetting env. episode 1467.000000, reward total was -20.000000. running mean: -19.657608\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -19.671032\n",
            "resetting env. episode 1469.000000, reward total was -20.000000. running mean: -19.674322\n",
            "resetting env. episode 1470.000000, reward total was -19.000000. running mean: -19.667578\n",
            "resetting env. episode 1471.000000, reward total was -20.000000. running mean: -19.670903\n",
            "resetting env. episode 1472.000000, reward total was -21.000000. running mean: -19.684194\n",
            "resetting env. episode 1473.000000, reward total was -20.000000. running mean: -19.687352\n",
            "resetting env. episode 1474.000000, reward total was -18.000000. running mean: -19.670478\n",
            "resetting env. episode 1475.000000, reward total was -20.000000. running mean: -19.673773\n",
            "resetting env. episode 1476.000000, reward total was -20.000000. running mean: -19.677036\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -19.690265\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -19.703363\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -19.716329\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -19.729166\n",
            "resetting env. episode 1481.000000, reward total was -20.000000. running mean: -19.731874\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -19.744555\n",
            "resetting env. episode 1483.000000, reward total was -14.000000. running mean: -19.687110\n",
            "resetting env. episode 1484.000000, reward total was -20.000000. running mean: -19.690239\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -19.703336\n",
            "resetting env. episode 1486.000000, reward total was -17.000000. running mean: -19.676303\n",
            "resetting env. episode 1487.000000, reward total was -19.000000. running mean: -19.669540\n",
            "resetting env. episode 1488.000000, reward total was -21.000000. running mean: -19.682844\n",
            "resetting env. episode 1489.000000, reward total was -19.000000. running mean: -19.676016\n",
            "resetting env. episode 1490.000000, reward total was -21.000000. running mean: -19.689256\n",
            "resetting env. episode 1491.000000, reward total was -19.000000. running mean: -19.682363\n",
            "resetting env. episode 1492.000000, reward total was -21.000000. running mean: -19.695540\n",
            "resetting env. episode 1493.000000, reward total was -20.000000. running mean: -19.698584\n",
            "resetting env. episode 1494.000000, reward total was -20.000000. running mean: -19.701598\n",
            "resetting env. episode 1495.000000, reward total was -20.000000. running mean: -19.704582\n",
            "resetting env. episode 1496.000000, reward total was -18.000000. running mean: -19.687537\n",
            "resetting env. episode 1497.000000, reward total was -17.000000. running mean: -19.660661\n",
            "resetting env. episode 1498.000000, reward total was -20.000000. running mean: -19.664055\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -19.677414\n",
            "resetting env. episode 1500.000000, reward total was -18.000000. running mean: -19.660640\n",
            "CPU times: user 4h 5min 36s, sys: 49min 59s, total: 4h 55min 36s\n",
            "Wall time: 2h 32min 57s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "CteN7XKMVGqg",
        "outputId": "01410e70-06ab-4eee-c741-9e2320fe45ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZYA0HgMoO77a"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# pickle.dump(model, open('model.pkl', 'wb'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "pg_from_scratch_(h_600).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}