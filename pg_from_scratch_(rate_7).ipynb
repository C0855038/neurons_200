{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l9hHvTk6ec8"
      },
      "source": [
        "# Policy Gradient\n",
        "\n",
        "* http://karpathy.github.io/2016/05/31/rl/\n",
        "* https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
        "* https://github.com/gameofdimension/policy-gradient-pong\n",
        "* https://www.youtube.com/watch?v=tqrcjHuNdmQ\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqkOdLyN9Ylm"
      },
      "source": [
        "## Step 1: Installation for Colab - just execute these cells and do not worry too much\n",
        "\n",
        "* http://nbviewer.jupyter.org/github/patrickmineault/xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb \n",
        "* https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi\n",
        "* https://nyu-cds.github.io/python-mpi/setup/\n",
        "* https://medium.com/@kaleajit27/reinforcement-learning-on-google-colab-9cb2e1ef51e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF9MAVI16huj",
        "outputId": "d623a3e2-0963-40a1-d290-0ad1337470bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install python-opengl -y  >/dev/null\n",
        "!apt install xvfb -y >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "fSC11TfN6p69"
      },
      "outputs": [],
      "source": [
        "!pip install pyvirtualdisplay >/dev/null\n",
        "!pip install piglet >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "caiHE2hy6xrf"
      },
      "outputs": [],
      "source": [
        "# from pyvirtualdisplay import Display\n",
        "# display = Display(visible=0, size=(1400, 900))\n",
        "# display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "5c9e6886-5036-4ece-c5f1-340c50cca067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "935a2580-b3ae-4765-f15a-22234c60db10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "42e24282-06e1-4861-9b9a-2f3ff5dfa5fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "d1054ec3-a7cc-46c7-cc4e-cd3e38b3d274"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "f530d432-3f03-4f8f-8c0d-121aabafd11d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -20.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  env.close()\n",
        "  display_frames_as_gif(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 3 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-7\n",
        "learning_rate = 1e-7\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "486b4012-7c98-4900-8195-f9f187aab1f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -19.000000. running mean: -19.999900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.009901\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.019802\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.029604\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.039308\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.038915\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.038526\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -20.028140\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.037859\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.047480\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.057006\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.066436\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.075771\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.085014\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.094163\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.093222\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.102290\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.111267\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.120154\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.128952\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.127663\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.136386\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.145022\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.143572\n",
            "resetting env. episode 27.000000, reward total was -19.000000. running mean: -20.132136\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.140815\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.149407\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.147913\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.156434\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.154869\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.163321\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.161688\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.170071\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.178370\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.186586\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.194720\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.192773\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.200845\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.208837\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.216749\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.224581\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.232335\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.240012\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.237612\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.245236\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.252783\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.260256\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.267653\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.274976\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.282227\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.289404\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.296510\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.303545\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.310510\n",
            "resetting env. episode 57.000000, reward total was -19.000000. running mean: -20.297405\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.304431\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.311386\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.308273\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.315190\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.322038\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.328818\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.335529\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.342174\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.348752\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.355265\n",
            "resetting env. episode 68.000000, reward total was -19.000000. running mean: -20.341712\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.348295\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.344812\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.341364\n",
            "resetting env. episode 72.000000, reward total was -18.000000. running mean: -20.317950\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.314771\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.321623\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.328407\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.335123\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.341772\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.348354\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.354870\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.361322\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.367708\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.374031\n",
            "resetting env. episode 83.000000, reward total was -18.000000. running mean: -20.350291\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.356788\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.363220\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.359588\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.365992\n",
            "resetting env. episode 88.000000, reward total was -19.000000. running mean: -20.352332\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.358809\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.365221\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.361569\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.357953\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.364373\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.360730\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.367122\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.373451\n",
            "resetting env. episode 97.000000, reward total was -18.000000. running mean: -20.349717\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.356219\n",
            "resetting env. episode 99.000000, reward total was -18.000000. running mean: -20.332657\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.339331\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.345937\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.342478\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.329053\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.335763\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.342405\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.348981\n",
            "resetting env. episode 107.000000, reward total was -19.000000. running mean: -20.335491\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.342136\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.348715\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.355228\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.361675\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.368059\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.374378\n",
            "resetting env. episode 114.000000, reward total was -18.000000. running mean: -20.350634\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.347128\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.353657\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.360120\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.366519\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -20.362854\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.359225\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.355633\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.362077\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.368456\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.374771\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.381024\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.387213\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.393341\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.399408\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.405414\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.411360\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.407246\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.403174\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.409142\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.405050\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.411000\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.406890\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.402821\n",
            "resetting env. episode 138.000000, reward total was -18.000000. running mean: -20.378793\n",
            "resetting env. episode 139.000000, reward total was -19.000000. running mean: -20.365005\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.371355\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.377641\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.383865\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.380026\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.376226\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.382464\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.378639\n",
            "resetting env. episode 147.000000, reward total was -19.000000. running mean: -20.364853\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.371204\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.367492\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.363817\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.360179\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.366577\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.372911\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.369182\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.365490\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.371836\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.378117\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.384336\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.390493\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.396588\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.402622\n",
            "resetting env. episode 162.000000, reward total was -19.000000. running mean: -20.388596\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.394710\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.400763\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.406755\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.412687\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.418561\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.424375\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.420131\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.425930\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.431671\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.437354\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.442980\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.448551\n",
            "resetting env. episode 175.000000, reward total was -19.000000. running mean: -20.434065\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.429724\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.435427\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.441073\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.436662\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.442296\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.447873\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.453394\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.458860\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.464271\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.469629\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.464932\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.470283\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.475580\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.480824\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.476016\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.471256\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.476543\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.481778\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.486960\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.492091\n",
            "resetting env. episode 196.000000, reward total was -17.000000. running mean: -20.457170\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.452598\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.458072\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.453491\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.458956\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.454367\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.459823\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.465225\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.470573\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.455867\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.461308\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.466695\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.472028\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.477308\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.482535\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.487710\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.492832\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.487904\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -20.473025\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.478295\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.483512\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.478677\n",
            "resetting env. episode 218.000000, reward total was -19.000000. running mean: -20.463890\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.459251\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.464659\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.460012\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.465412\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.460758\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.466150\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.471489\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.476774\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.482006\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.477186\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.482414\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.487590\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.492714\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.487787\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.492909\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.497980\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.503000\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.507970\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.512890\n",
            "resetting env. episode 238.000000, reward total was -19.000000. running mean: -20.497762\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.502784\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.497756\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.492779\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.487851\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.492972\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.498043\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.503062\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.508031\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.512951\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.517822\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.522643\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.527417\n",
            "resetting env. episode 251.000000, reward total was -19.000000. running mean: -20.512143\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.517021\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.521851\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.526633\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.531366\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.536053\n",
            "resetting env. episode 257.000000, reward total was -18.000000. running mean: -20.510692\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.515585\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.520429\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.515225\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.520073\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.524872\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.529623\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.534327\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.538984\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.543594\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.538158\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.542777\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.547349\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.551875\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.546357\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.550893\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.555384\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.559830\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.564232\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.568590\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.572904\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.567175\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.571503\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -20.555788\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.550230\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.554728\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.549180\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.543689\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.548252\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.542769\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.547342\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.551868\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.556349\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.560786\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.565178\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.559526\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.563931\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.558292\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.552709\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.557182\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.551610\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.556094\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.560533\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.564928\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.569278\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.573585\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.577850\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.572071\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.576350\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.570587\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.564881\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.559232\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.553640\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.558103\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.562522\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.556897\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.561328\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.565715\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.560058\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.564457\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.568813\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.573125\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.577393\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.581619\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.585803\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.579945\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.574146\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.568404\n",
            "resetting env. episode 325.000000, reward total was -18.000000. running mean: -20.542720\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.537293\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.541920\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.546501\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -20.531036\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.525725\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.530468\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.525164\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.529912\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.534613\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.539267\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.533874\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.538535\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.523150\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.527918\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.532639\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.527313\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.532040\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.536719\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.541352\n",
            "resetting env. episode 345.000000, reward total was -18.000000. running mean: -20.515939\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.510779\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.515671\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.510515\n",
            "resetting env. episode 349.000000, reward total was -19.000000. running mean: -20.495410\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.500455\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.505451\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.510396\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.515292\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.510140\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.505038\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.509988\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.504888\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.509839\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.504741\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.499693\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.504696\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.509649\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.514553\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.509407\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.514313\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.509170\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.504078\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.509038\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.493947\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.499008\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.504018\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.498977\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.493988\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.499048\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.504057\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.499017\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.504027\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.498986\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.503996\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.508957\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.513867\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.518728\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.523541\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.528306\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.533023\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.537692\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.542315\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.546892\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.551423\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.535909\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -20.520550\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.525344\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.530091\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.534790\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.539442\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.544048\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.538607\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.543221\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.547789\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.542311\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.536888\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.531519\n",
            "resetting env. episode 403.000000, reward total was -18.000000. running mean: -20.506204\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.511142\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.516031\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.520870\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.525662\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.530405\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -20.515101\n",
            "resetting env. episode 410.000000, reward total was -19.000000. running mean: -20.499950\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.504950\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.509901\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.514802\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.519654\n",
            "resetting env. episode 415.000000, reward total was -19.000000. running mean: -20.504457\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.509413\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.514319\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.519175\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.503984\n",
            "resetting env. episode 420.000000, reward total was -19.000000. running mean: -20.488944\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -20.474054\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.479314\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.484521\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.489675\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.484779\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.489931\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -20.475032\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.470281\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.465578\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.470923\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.476213\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.481451\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.486637\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.491770\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.496853\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.491884\n",
            "resetting env. episode 437.000000, reward total was -19.000000. running mean: -20.476965\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.472196\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.477474\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.482699\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.477872\n",
            "resetting env. episode 442.000000, reward total was -18.000000. running mean: -20.453093\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.458562\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.463977\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -20.449337\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.454844\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.460295\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.465692\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.471035\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.476325\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.481562\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.486746\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.491879\n",
            "resetting env. episode 454.000000, reward total was -18.000000. running mean: -20.466960\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.472290\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.477567\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.482792\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.487964\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.483084\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.478253\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -20.463471\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.458836\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.464248\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.469605\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.464909\n",
            "resetting env. episode 466.000000, reward total was -19.000000. running mean: -20.450260\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.445757\n",
            "resetting env. episode 468.000000, reward total was -18.000000. running mean: -20.421300\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.427087\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.432816\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.428488\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.434203\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.439861\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.445462\n",
            "resetting env. episode 475.000000, reward total was -19.000000. running mean: -20.431008\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.436698\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.442331\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.447907\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.443428\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.448994\n",
            "resetting env. episode 481.000000, reward total was -17.000000. running mean: -20.414504\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.420359\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.416155\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.421994\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.417774\n",
            "resetting env. episode 486.000000, reward total was -18.000000. running mean: -20.393596\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.399660\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.405664\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.411607\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.417491\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.423316\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.409083\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.414992\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.420842\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.426634\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.432367\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.438044\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.443663\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.439227\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.444834\n",
            "CPU times: user 23min 31s, sys: 10min 34s, total: 34min 6s\n",
            "Wall time: 17min 49s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "NuSP4FE8QTA-",
        "outputId": "3f6594d8-36ab-4721-c117-4e40038a2f34"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCZR5OV-z-YJ",
        "outputId": "e49ace50-5e3d-42d3-833d-ecdfb091d8b3"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -19.000000. running mean: -19.999900\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -19.999901\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.009902\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.019803\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.029605\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.029309\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.039016\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.048626\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.048139\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.047658\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.047181\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.056710\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.056143\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.055581\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.065025\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.074375\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.083631\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.092795\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.101867\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.100848\n",
            "resetting env. episode 23.000000, reward total was -19.000000. running mean: -20.089840\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.098941\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.107952\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.116873\n",
            "resetting env. episode 27.000000, reward total was -19.000000. running mean: -20.105704\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.114647\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.123500\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.132265\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.140943\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.139533\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.148138\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.146656\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.145190\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -20.133738\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.132401\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.131077\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.139766\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.148368\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.146885\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.155416\n",
            "resetting env. episode 43.000000, reward total was -19.000000. running mean: -20.143862\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.142423\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.130999\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.139689\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.148292\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.156809\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.165241\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.173588\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.181853\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.190034\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.198134\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.206152\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.204091\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.212050\n",
            "resetting env. episode 57.000000, reward total was -19.000000. running mean: -20.199929\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.207930\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.215851\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.223692\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.231455\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.239141\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.246749\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.254282\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.251739\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.259222\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.266629\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.273963\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.281224\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.278411\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.285627\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.292771\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.289843\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.296945\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.303975\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.300936\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.307926\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.314847\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.321698\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.328482\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.325197\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.331945\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.328625\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.335339\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.341986\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.348566\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.345080\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.351629\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.348113\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.354632\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.361086\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.367475\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.373800\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.380062\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.386261\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.392399\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.398475\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.404490\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.390445\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.386541\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.382675\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.388849\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.374960\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.381210\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.387398\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.393524\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.399589\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.405593\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.411537\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.417422\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.413248\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.419115\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.424924\n",
            "resetting env. episode 114.000000, reward total was -19.000000. running mean: -20.410675\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.416568\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.422402\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.418178\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.413997\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.419857\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -20.405658\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.401601\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.407585\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.403510\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.409474\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.415380\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.421226\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.427014\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.432744\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.428416\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.434132\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.439791\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.435393\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.441039\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.446628\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.452162\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.447641\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.453164\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.458632\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.464046\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.469406\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.474712\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.469965\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.465265\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.470612\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.475906\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.481147\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.476336\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.471572\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.476856\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.482088\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.487267\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.492394\n",
            "resetting env. episode 153.000000, reward total was -19.000000. running mean: -20.477470\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.482696\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.487869\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.492990\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.488060\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.493180\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.498248\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.503265\n",
            "resetting env. episode 161.000000, reward total was -19.000000. running mean: -20.488233\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.483350\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.488517\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.483632\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.488795\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.493907\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.498968\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.503979\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.498939\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.493949\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.499010\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.504020\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.508980\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.503890\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.508851\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.513762\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.518625\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.523439\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.518204\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.513022\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.517892\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.522713\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.527486\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.522211\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.516989\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.521819\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.516601\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.521435\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.526220\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.530958\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.525649\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.520392\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.525188\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.529936\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.534637\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.529291\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.533998\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.538658\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.543271\n",
            "resetting env. episode 200.000000, reward total was -19.000000. running mean: -20.527838\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.532560\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.537234\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.531862\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.536544\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.541178\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.545766\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.550309\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.554806\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.559258\n",
            "resetting env. episode 210.000000, reward total was -18.000000. running mean: -20.533665\n",
            "resetting env. episode 211.000000, reward total was -18.000000. running mean: -20.508328\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.513245\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.508113\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.513031\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.517901\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.522722\n",
            "resetting env. episode 217.000000, reward total was -17.000000. running mean: -20.487495\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.492620\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.497694\n",
            "resetting env. episode 220.000000, reward total was -18.000000. running mean: -20.472717\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.477990\n",
            "resetting env. episode 222.000000, reward total was -18.000000. running mean: -20.453210\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.458678\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -20.444091\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.449650\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -20.435153\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.440802\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.446394\n",
            "resetting env. episode 229.000000, reward total was -18.000000. running mean: -20.421930\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.427711\n",
            "resetting env. episode 231.000000, reward total was -19.000000. running mean: -20.413434\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.419299\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.425106\n",
            "resetting env. episode 234.000000, reward total was -19.000000. running mean: -20.410855\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.416747\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.422579\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.428353\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.434070\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.439729\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.435332\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.440979\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.446569\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.442103\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.447682\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.453205\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.458673\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.464086\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.469446\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.474751\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.470004\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.475304\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.480551\n",
            "resetting env. episode 253.000000, reward total was -19.000000. running mean: -20.465745\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.471088\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.476377\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.481613\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.476797\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.482029\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.477209\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.472436\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.477712\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.482935\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.488106\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.483225\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.488392\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.493508\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.498573\n",
            "resetting env. episode 268.000000, reward total was -19.000000. running mean: -20.483588\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.478752\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -20.463964\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.469325\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.474631\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.479885\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.485086\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.480235\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.485433\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.490579\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.495673\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.490716\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.495809\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.490851\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.495942\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.500983\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.505973\n",
            "resetting env. episode 285.000000, reward total was -19.000000. running mean: -20.490913\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.496004\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.481044\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.486234\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.481371\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.486558\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.481692\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.486875\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.482006\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.487186\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.482314\n",
            "resetting env. episode 296.000000, reward total was -18.000000. running mean: -20.457491\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.462916\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.458287\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.463704\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.469067\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.474377\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.479633\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.484837\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.489988\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.495088\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.500137\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.505136\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.510085\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.504984\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.499934\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.504935\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.509885\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.514786\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.509639\n",
            "resetting env. episode 315.000000, reward total was -19.000000. running mean: -20.494542\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.499597\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.494601\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.499655\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.494658\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.499712\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.494715\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.499767\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.484770\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.479922\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.475123\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.480372\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.475568\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.480812\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.486004\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.481144\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.486333\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.491469\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.496555\n",
            "resetting env. episode 334.000000, reward total was -19.000000. running mean: -20.481589\n",
            "resetting env. episode 335.000000, reward total was -19.000000. running mean: -20.466773\n",
            "resetting env. episode 336.000000, reward total was -19.000000. running mean: -20.452105\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.447584\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.433109\n",
            "resetting env. episode 339.000000, reward total was -18.000000. running mean: -20.408777\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.414690\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.420543\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.426337\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.422074\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.427853\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.423575\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.419339\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.425146\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.430894\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.436585\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.442219\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.447797\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.443319\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -20.428886\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.434597\n",
            "resetting env. episode 355.000000, reward total was -19.000000. running mean: -20.420251\n",
            "resetting env. episode 356.000000, reward total was -18.000000. running mean: -20.396049\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.392088\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.398167\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.404186\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.410144\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.416042\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.411882\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.417763\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.413585\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.419450\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.425255\n",
            "resetting env. episode 367.000000, reward total was -19.000000. running mean: -20.411003\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.416892\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.422724\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.418496\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.414311\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.420168\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.415967\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.411807\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.417689\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.423512\n",
            "resetting env. episode 377.000000, reward total was -19.000000. running mean: -20.409277\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.415184\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.411032\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.416922\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.422753\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.428525\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.434240\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.429897\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.435599\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.441243\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.436830\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.432462\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.438137\n",
            "resetting env. episode 390.000000, reward total was -18.000000. running mean: -20.413756\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.419618\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.405422\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.411368\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.417254\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.423082\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.428851\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.434562\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.430217\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.425915\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.431655\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.427339\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.413065\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.418935\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.424745\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.420498\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.426293\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.432030\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.427710\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.433433\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.439098\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.434707\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.440360\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.445957\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.451497\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.456982\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.452412\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.457888\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.453309\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.458776\n",
            "resetting env. episode 420.000000, reward total was -18.000000. running mean: -20.434188\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.439847\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.435448\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.431094\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.426783\n",
            "resetting env. episode 425.000000, reward total was -18.000000. running mean: -20.402515\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.398490\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -20.384505\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.390660\n",
            "resetting env. episode 429.000000, reward total was -17.000000. running mean: -20.356753\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.353186\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.339654\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.336257\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.332895\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.319566\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.326370\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.333106\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.339775\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.336378\n",
            "resetting env. episode 439.000000, reward total was -19.000000. running mean: -20.323014\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.329784\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.336486\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.343121\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.329690\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.336393\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.343029\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.349599\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.346103\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.352642\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.359115\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.365524\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.371869\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.368150\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.374469\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.370724\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.367017\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.373347\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.369613\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.375917\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.382158\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.388336\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.394453\n",
            "resetting env. episode 462.000000, reward total was -19.000000. running mean: -20.380508\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.376703\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.372936\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.379207\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.385415\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.381561\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.387745\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.393868\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.399929\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.405930\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -20.391870\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.397952\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.393972\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.390032\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.386132\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -20.372271\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.368548\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.374863\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.371114\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.367403\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.373729\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.379991\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.386192\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.392330\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.398406\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.404422\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.410378\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.406274\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.412211\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.408089\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.414008\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.409868\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.415770\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.421612\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.427396\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.433122\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.438791\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.434403\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.430059\n",
            "CPU times: user 23min 53s, sys: 10min 42s, total: 34min 36s\n",
            "Wall time: 18min 1s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "1c39003a-9240-42b2-8899-0ee6f8811811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "28ffbcad-8938-4053-f873-9dd9188b1bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 8.000000, reward total was -19.000000. running mean: -20.970100\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.970399\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.970695\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.970988\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.961278\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.961665\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.952049\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.952528\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.943003\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.933573\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.924237\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.924995\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.925745\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.906487\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.907423\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.898348\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.899365\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.900371\n",
            "resetting env. episode 26.000000, reward total was -19.000000. running mean: -20.881368\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.882554\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.883728\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.884891\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.886042\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.887182\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.878310\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.879527\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.870731\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.862024\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.863404\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.854770\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.856222\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.847660\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.849183\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.840692\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.842285\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.843862\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.835423\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.827069\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.828798\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.820510\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.822305\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.814082\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.805941\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.807882\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.809803\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.811705\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.803588\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.805552\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.807497\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.799422\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.801427\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -20.783413\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.785579\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.787723\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.779846\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -20.762048\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.764427\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.766783\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.759115\n",
            "resetting env. episode 67.000000, reward total was -18.000000. running mean: -20.731524\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.724209\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.716966\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.709797\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.712699\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.715572\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.708416\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -20.691332\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.694419\n",
            "resetting env. episode 76.000000, reward total was -19.000000. running mean: -20.677474\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.680700\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.683893\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.687054\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.680183\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.683381\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.686548\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.689682\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.692785\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.695857\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.698899\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.691910\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.684991\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.678141\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.671359\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.674646\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.667899\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.671220\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.674508\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.667763\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.671086\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.654375\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.657831\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.651253\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.654740\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.658193\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.651611\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.635095\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.638744\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.632356\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.636033\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.629672\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.633376\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.627042\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.630771\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -20.614464\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.608319\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.612236\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.616114\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.619952\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.613753\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.617615\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.611439\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.615325\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.619172\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.622980\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.626750\n",
            "resetting env. episode 123.000000, reward total was -18.000000. running mean: -20.600483\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.604478\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.608433\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.612349\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.606225\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -20.590163\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.594261\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.598319\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.602335\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.596312\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.600349\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.594345\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.598402\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.602418\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.606394\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.600330\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.604327\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.608283\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.602201\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.606178\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.610117\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.604016\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.607975\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.601896\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.605877\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.609818\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.613720\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.617583\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.611407\n",
            "resetting env. episode 152.000000, reward total was -19.000000. running mean: -20.595293\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.589340\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.583446\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.577612\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.581836\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.586017\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.590157\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.584256\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.588413\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.582529\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.586704\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.590837\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.584928\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.589079\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.583188\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.577356\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.571583\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.575867\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.580108\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.584307\n",
            "resetting env. episode 172.000000, reward total was -19.000000. running mean: -20.568464\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.552779\n",
            "resetting env. episode 174.000000, reward total was -19.000000. running mean: -20.537252\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.541879\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.546460\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.550996\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.555486\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.559931\n",
            "resetting env. episode 180.000000, reward total was -19.000000. running mean: -20.544332\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.548888\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.553399\n",
            "resetting env. episode 183.000000, reward total was -19.000000. running mean: -20.537865\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.542487\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.547062\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.551591\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.556075\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.560515\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.564909\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.569260\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.563568\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.567932\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.572253\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.566530\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.570865\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.575156\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.579405\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.583611\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.587775\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.591897\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.585978\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.580118\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.584317\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.578474\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.582689\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.576862\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.571093\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.575383\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.569629\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.573932\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -20.558193\n",
            "resetting env. episode 212.000000, reward total was -19.000000. running mean: -20.542611\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.547185\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.551713\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.556196\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.550634\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.555128\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.559577\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.563981\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.568341\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.572658\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.566931\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.571262\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.575549\n",
            "resetting env. episode 225.000000, reward total was -19.000000. running mean: -20.559794\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.564196\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.568554\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.562868\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -20.547239\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.551767\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.556249\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.560687\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.565080\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.569429\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.573735\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.577998\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.572218\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.576495\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.580730\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.564923\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.549274\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.543781\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.538343\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.542960\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.547530\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.552055\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.546534\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.541069\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.545658\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.550202\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.554700\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.559153\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.563561\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.567926\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.572246\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.566524\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.570859\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.575150\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.579399\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.583605\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.577769\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.581991\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.586171\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.590309\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.594406\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.598462\n",
            "resetting env. episode 267.000000, reward total was -19.000000. running mean: -20.582478\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.576653\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.580886\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.585077\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.579227\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.573434\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.577700\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.581923\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.586104\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.590243\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.584340\n",
            "resetting env. episode 278.000000, reward total was -19.000000. running mean: -20.568497\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.562812\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.557184\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.561612\n",
            "resetting env. episode 282.000000, reward total was -19.000000. running mean: -20.545996\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.540536\n",
            "resetting env. episode 284.000000, reward total was -19.000000. running mean: -20.525131\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.529879\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.524580\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.529335\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.524041\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.518801\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.523613\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.518377\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.523193\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.517961\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.522781\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.527554\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.522278\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.517055\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.511885\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.516766\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.521598\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.516382\n",
            "resetting env. episode 302.000000, reward total was -18.000000. running mean: -20.491218\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.496306\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.481343\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.486530\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.491664\n",
            "resetting env. episode 307.000000, reward total was -19.000000. running mean: -20.476748\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.481980\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.477161\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.482389\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.487565\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -20.472689\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.477962\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.483183\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.478351\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.483568\n",
            "resetting env. episode 317.000000, reward total was -18.000000. running mean: -20.458732\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.454145\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.449603\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.455107\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -20.440556\n",
            "resetting env. episode 322.000000, reward total was -18.000000. running mean: -20.416150\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.401989\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.407969\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.413889\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.409750\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.415653\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.421496\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.427281\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.433009\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.438679\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.434292\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.439949\n",
            "resetting env. episode 334.000000, reward total was -19.000000. running mean: -20.425549\n",
            "resetting env. episode 335.000000, reward total was -19.000000. running mean: -20.411294\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.417181\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.423009\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.428779\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.434491\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.440146\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.435745\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.441387\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.436974\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.442604\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.448178\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.453696\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.459159\n",
            "resetting env. episode 348.000000, reward total was -19.000000. running mean: -20.444567\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.450122\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.455621\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.461064\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.466454\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -20.451789\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.447271\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.452799\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -20.438271\n",
            "resetting env. episode 357.000000, reward total was -18.000000. running mean: -20.413888\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.409749\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.405651\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.411595\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.417479\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -20.403304\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.409271\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.405178\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.411127\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.417015\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.422845\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.428617\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.414331\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.420187\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.425985\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.431726\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.437408\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.443034\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.448604\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.454118\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.459577\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.454981\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.450431\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.455927\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.461368\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.466754\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.462086\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.457465\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.462891\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.448262\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.453779\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.449242\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.454749\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.460202\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.465600\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.470944\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.476234\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.481472\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.486657\n",
            "resetting env. episode 396.000000, reward total was -18.000000. running mean: -20.461791\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.467173\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.472501\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.467776\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.473098\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.478367\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.463583\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.458948\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.454358\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.459815\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.465216\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.460564\n",
            "resetting env. episode 408.000000, reward total was -18.000000. running mean: -20.435959\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.441599\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.437183\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.442811\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.448383\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.453899\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.459360\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.454767\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.450219\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.445717\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.441260\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.436847\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.442479\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.448054\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.453573\n",
            "resetting env. episode 423.000000, reward total was -16.000000. running mean: -20.409038\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.414947\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.400798\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.406790\n",
            "resetting env. episode 427.000000, reward total was -18.000000. running mean: -20.382722\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.378895\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.385106\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.381255\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.377442\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.383668\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.389831\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.395933\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.401973\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.407954\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.413874\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.419735\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.415538\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.421383\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.427169\n",
            "resetting env. episode 442.000000, reward total was -18.000000. running mean: -20.402897\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.408868\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.414779\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.410632\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.416525\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.422360\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.428136\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.433855\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.439517\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.445121\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.450670\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.456163\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.461602\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.466986\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.472316\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.477593\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.472817\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.478089\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.483308\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.488475\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.493590\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.498654\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.493668\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.498731\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.503744\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.508706\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.513619\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.518483\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.523298\n",
            "resetting env. episode 471.000000, reward total was -18.000000. running mean: -20.498065\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.503084\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.498054\n",
            "resetting env. episode 474.000000, reward total was -19.000000. running mean: -20.483073\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.488242\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.493360\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.488426\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.483542\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -20.468707\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.464020\n",
            "resetting env. episode 481.000000, reward total was -18.000000. running mean: -20.439379\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.444986\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.450536\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.456030\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.461470\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.466855\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -20.452187\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.457665\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.463088\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.468457\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.473773\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.479035\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -20.464245\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.469602\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.474906\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.480157\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.485356\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.490502\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.495597\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.500641\n",
            "resetting env. episode 501.000000, reward total was -21.000000. running mean: -20.505635\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.510578\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.515473\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.520318\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.525115\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.529863\n",
            "resetting env. episode 507.000000, reward total was -21.000000. running mean: -20.534565\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -20.529219\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.533927\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.528588\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.533302\n",
            "resetting env. episode 512.000000, reward total was -20.000000. running mean: -20.527969\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.532689\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -20.537362\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.541989\n",
            "resetting env. episode 516.000000, reward total was -21.000000. running mean: -20.546569\n",
            "resetting env. episode 517.000000, reward total was -20.000000. running mean: -20.541103\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.545692\n",
            "resetting env. episode 519.000000, reward total was -20.000000. running mean: -20.540235\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.544833\n",
            "resetting env. episode 521.000000, reward total was -20.000000. running mean: -20.539384\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.543991\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -20.538551\n",
            "resetting env. episode 524.000000, reward total was -19.000000. running mean: -20.523165\n",
            "resetting env. episode 525.000000, reward total was -21.000000. running mean: -20.527934\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -20.532654\n",
            "resetting env. episode 527.000000, reward total was -20.000000. running mean: -20.527328\n",
            "resetting env. episode 528.000000, reward total was -20.000000. running mean: -20.522054\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.526834\n",
            "resetting env. episode 530.000000, reward total was -19.000000. running mean: -20.511565\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.516450\n",
            "resetting env. episode 532.000000, reward total was -20.000000. running mean: -20.511285\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.516172\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.521011\n",
            "resetting env. episode 535.000000, reward total was -18.000000. running mean: -20.495801\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.500843\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.505834\n",
            "resetting env. episode 538.000000, reward total was -20.000000. running mean: -20.500776\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -20.505768\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.510710\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.515603\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.520447\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.525243\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -20.529990\n",
            "resetting env. episode 545.000000, reward total was -19.000000. running mean: -20.514690\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.519544\n",
            "resetting env. episode 547.000000, reward total was -20.000000. running mean: -20.514348\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -20.519205\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.524013\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.528772\n",
            "resetting env. episode 551.000000, reward total was -20.000000. running mean: -20.523485\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.528250\n",
            "resetting env. episode 553.000000, reward total was -20.000000. running mean: -20.522967\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -20.527738\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.532460\n",
            "resetting env. episode 556.000000, reward total was -19.000000. running mean: -20.517136\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.521964\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.526745\n",
            "resetting env. episode 559.000000, reward total was -20.000000. running mean: -20.521477\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -20.526263\n",
            "resetting env. episode 561.000000, reward total was -20.000000. running mean: -20.521000\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.525790\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.530532\n",
            "resetting env. episode 564.000000, reward total was -20.000000. running mean: -20.525227\n",
            "resetting env. episode 565.000000, reward total was -20.000000. running mean: -20.519974\n",
            "resetting env. episode 566.000000, reward total was -20.000000. running mean: -20.514775\n",
            "resetting env. episode 567.000000, reward total was -20.000000. running mean: -20.509627\n",
            "resetting env. episode 568.000000, reward total was -20.000000. running mean: -20.504531\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -20.509485\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.514391\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -20.519247\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -20.524054\n",
            "resetting env. episode 573.000000, reward total was -21.000000. running mean: -20.528814\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.533525\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.538190\n",
            "resetting env. episode 576.000000, reward total was -19.000000. running mean: -20.522808\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -20.527580\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.532304\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -20.536981\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -20.531612\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.536295\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.540932\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -20.545523\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.550068\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.554567\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.559022\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.563431\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.567797\n",
            "resetting env. episode 589.000000, reward total was -19.000000. running mean: -20.552119\n",
            "resetting env. episode 590.000000, reward total was -20.000000. running mean: -20.546598\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.551132\n",
            "resetting env. episode 592.000000, reward total was -20.000000. running mean: -20.545621\n",
            "resetting env. episode 593.000000, reward total was -20.000000. running mean: -20.540164\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.544763\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.549315\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -20.543822\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.548384\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -20.552900\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.557371\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.561797\n",
            "resetting env. episode 601.000000, reward total was -19.000000. running mean: -20.546179\n",
            "resetting env. episode 602.000000, reward total was -20.000000. running mean: -20.540717\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.545310\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.549857\n",
            "resetting env. episode 605.000000, reward total was -17.000000. running mean: -20.514359\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -20.509215\n",
            "resetting env. episode 607.000000, reward total was -20.000000. running mean: -20.504123\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -20.509082\n",
            "resetting env. episode 609.000000, reward total was -21.000000. running mean: -20.513991\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.518851\n",
            "resetting env. episode 611.000000, reward total was -20.000000. running mean: -20.513662\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.518526\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -20.523341\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.528107\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.532826\n",
            "resetting env. episode 616.000000, reward total was -20.000000. running mean: -20.527498\n",
            "resetting env. episode 617.000000, reward total was -20.000000. running mean: -20.522223\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.527001\n",
            "resetting env. episode 619.000000, reward total was -21.000000. running mean: -20.531731\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.536413\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -20.531049\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.535739\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.540381\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.544977\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.549528\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.554032\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -20.558492\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.562907\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.567278\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.571605\n",
            "resetting env. episode 631.000000, reward total was -20.000000. running mean: -20.565889\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.570230\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -20.574528\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.578783\n",
            "resetting env. episode 635.000000, reward total was -19.000000. running mean: -20.562995\n",
            "resetting env. episode 636.000000, reward total was -20.000000. running mean: -20.557365\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -20.561791\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.566173\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.570512\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -20.564807\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.569158\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -20.573467\n",
            "resetting env. episode 643.000000, reward total was -21.000000. running mean: -20.577732\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.581955\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.586135\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -20.590274\n",
            "resetting env. episode 647.000000, reward total was -20.000000. running mean: -20.584371\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.588528\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -20.592642\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.596716\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.600749\n",
            "resetting env. episode 652.000000, reward total was -19.000000. running mean: -20.584741\n",
            "resetting env. episode 653.000000, reward total was -20.000000. running mean: -20.578894\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -20.583105\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.587274\n",
            "resetting env. episode 656.000000, reward total was -19.000000. running mean: -20.571401\n",
            "resetting env. episode 657.000000, reward total was -21.000000. running mean: -20.575687\n",
            "resetting env. episode 658.000000, reward total was -19.000000. running mean: -20.559930\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -20.564331\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -20.558688\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.563101\n",
            "resetting env. episode 662.000000, reward total was -21.000000. running mean: -20.567470\n",
            "resetting env. episode 663.000000, reward total was -20.000000. running mean: -20.561795\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.566177\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.570515\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.574810\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -20.579062\n",
            "resetting env. episode 668.000000, reward total was -20.000000. running mean: -20.573271\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.577539\n",
            "resetting env. episode 670.000000, reward total was -20.000000. running mean: -20.571763\n",
            "resetting env. episode 671.000000, reward total was -20.000000. running mean: -20.566046\n",
            "resetting env. episode 672.000000, reward total was -21.000000. running mean: -20.570385\n",
            "resetting env. episode 673.000000, reward total was -19.000000. running mean: -20.554681\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.559135\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -20.563543\n",
            "resetting env. episode 676.000000, reward total was -19.000000. running mean: -20.547908\n",
            "resetting env. episode 677.000000, reward total was -21.000000. running mean: -20.552429\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.556904\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -20.551335\n",
            "resetting env. episode 680.000000, reward total was -20.000000. running mean: -20.545822\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.550364\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -20.544860\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -20.539412\n",
            "resetting env. episode 684.000000, reward total was -21.000000. running mean: -20.544017\n",
            "resetting env. episode 685.000000, reward total was -20.000000. running mean: -20.538577\n",
            "resetting env. episode 686.000000, reward total was -19.000000. running mean: -20.523191\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -20.527960\n",
            "resetting env. episode 688.000000, reward total was -20.000000. running mean: -20.522680\n",
            "resetting env. episode 689.000000, reward total was -18.000000. running mean: -20.497453\n",
            "resetting env. episode 690.000000, reward total was -18.000000. running mean: -20.472479\n",
            "resetting env. episode 691.000000, reward total was -19.000000. running mean: -20.457754\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.463176\n",
            "resetting env. episode 693.000000, reward total was -20.000000. running mean: -20.458545\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.463959\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.469320\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.474626\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.479880\n",
            "resetting env. episode 698.000000, reward total was -20.000000. running mean: -20.475081\n",
            "resetting env. episode 699.000000, reward total was -20.000000. running mean: -20.470330\n",
            "resetting env. episode 700.000000, reward total was -21.000000. running mean: -20.475627\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.480871\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -20.486062\n",
            "resetting env. episode 703.000000, reward total was -18.000000. running mean: -20.461202\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -20.466590\n",
            "resetting env. episode 705.000000, reward total was -19.000000. running mean: -20.451924\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -20.457404\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -20.462830\n",
            "resetting env. episode 708.000000, reward total was -21.000000. running mean: -20.468202\n",
            "resetting env. episode 709.000000, reward total was -20.000000. running mean: -20.463520\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.468885\n",
            "resetting env. episode 711.000000, reward total was -21.000000. running mean: -20.474196\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -20.469454\n",
            "resetting env. episode 713.000000, reward total was -20.000000. running mean: -20.464759\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -20.460112\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.465511\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.470856\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -20.476147\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -20.481386\n",
            "resetting env. episode 719.000000, reward total was -20.000000. running mean: -20.476572\n",
            "resetting env. episode 720.000000, reward total was -18.000000. running mean: -20.451806\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.457288\n",
            "resetting env. episode 722.000000, reward total was -20.000000. running mean: -20.452715\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.458188\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -20.463606\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.468970\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -20.474280\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.479538\n",
            "resetting env. episode 728.000000, reward total was -20.000000. running mean: -20.474742\n",
            "resetting env. episode 729.000000, reward total was -20.000000. running mean: -20.469995\n",
            "resetting env. episode 730.000000, reward total was -19.000000. running mean: -20.455295\n",
            "resetting env. episode 731.000000, reward total was -20.000000. running mean: -20.450742\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.456234\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.461672\n",
            "resetting env. episode 734.000000, reward total was -21.000000. running mean: -20.467055\n",
            "resetting env. episode 735.000000, reward total was -20.000000. running mean: -20.462385\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -20.457761\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.463183\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.468552\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -20.463866\n",
            "resetting env. episode 740.000000, reward total was -20.000000. running mean: -20.459227\n",
            "resetting env. episode 741.000000, reward total was -20.000000. running mean: -20.454635\n",
            "resetting env. episode 742.000000, reward total was -18.000000. running mean: -20.430089\n",
            "resetting env. episode 743.000000, reward total was -20.000000. running mean: -20.425788\n",
            "resetting env. episode 744.000000, reward total was -20.000000. running mean: -20.421530\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -20.427315\n",
            "resetting env. episode 746.000000, reward total was -19.000000. running mean: -20.413041\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.418911\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.424722\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.430475\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.436170\n",
            "resetting env. episode 751.000000, reward total was -20.000000. running mean: -20.431808\n",
            "resetting env. episode 752.000000, reward total was -21.000000. running mean: -20.437490\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.443115\n",
            "resetting env. episode 754.000000, reward total was -20.000000. running mean: -20.438684\n",
            "resetting env. episode 755.000000, reward total was -20.000000. running mean: -20.434297\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -20.439954\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.445555\n",
            "resetting env. episode 758.000000, reward total was -20.000000. running mean: -20.441099\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.446688\n",
            "resetting env. episode 760.000000, reward total was -20.000000. running mean: -20.442221\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.447799\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.453321\n",
            "resetting env. episode 763.000000, reward total was -21.000000. running mean: -20.458788\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -20.454200\n",
            "resetting env. episode 765.000000, reward total was -21.000000. running mean: -20.459658\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.465062\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -20.470411\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.475707\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.480950\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.486140\n",
            "resetting env. episode 771.000000, reward total was -20.000000. running mean: -20.481279\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -20.486466\n",
            "resetting env. episode 773.000000, reward total was -20.000000. running mean: -20.481601\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.486785\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -20.491917\n",
            "resetting env. episode 776.000000, reward total was -21.000000. running mean: -20.496998\n",
            "resetting env. episode 777.000000, reward total was -20.000000. running mean: -20.492028\n",
            "resetting env. episode 778.000000, reward total was -20.000000. running mean: -20.487108\n",
            "resetting env. episode 779.000000, reward total was -21.000000. running mean: -20.492237\n",
            "resetting env. episode 780.000000, reward total was -20.000000. running mean: -20.487315\n",
            "resetting env. episode 781.000000, reward total was -19.000000. running mean: -20.472441\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -20.467717\n",
            "resetting env. episode 783.000000, reward total was -20.000000. running mean: -20.463040\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -20.468409\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.473725\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -20.478988\n",
            "resetting env. episode 787.000000, reward total was -20.000000. running mean: -20.474198\n",
            "resetting env. episode 788.000000, reward total was -20.000000. running mean: -20.469456\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -20.474762\n",
            "resetting env. episode 790.000000, reward total was -20.000000. running mean: -20.470014\n",
            "resetting env. episode 791.000000, reward total was -21.000000. running mean: -20.475314\n",
            "resetting env. episode 792.000000, reward total was -20.000000. running mean: -20.470561\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.475855\n",
            "resetting env. episode 794.000000, reward total was -20.000000. running mean: -20.471097\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.476386\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.481622\n",
            "resetting env. episode 797.000000, reward total was -20.000000. running mean: -20.476806\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -20.482038\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.487217\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.492345\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -20.497422\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.502447\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.507423\n",
            "resetting env. episode 804.000000, reward total was -20.000000. running mean: -20.502349\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -20.507325\n",
            "resetting env. episode 806.000000, reward total was -19.000000. running mean: -20.492252\n",
            "resetting env. episode 807.000000, reward total was -21.000000. running mean: -20.497329\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.502356\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.507333\n",
            "resetting env. episode 810.000000, reward total was -20.000000. running mean: -20.502259\n",
            "resetting env. episode 811.000000, reward total was -17.000000. running mean: -20.467237\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -20.472564\n",
            "resetting env. episode 813.000000, reward total was -20.000000. running mean: -20.467839\n",
            "resetting env. episode 814.000000, reward total was -19.000000. running mean: -20.453160\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.458629\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.464042\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.469402\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.474708\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -20.479961\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -20.475161\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.480410\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.485605\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.490749\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -20.495842\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -20.500884\n",
            "resetting env. episode 826.000000, reward total was -20.000000. running mean: -20.495875\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -20.490916\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.496007\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.501047\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -20.506036\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -20.510976\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -20.515866\n",
            "resetting env. episode 833.000000, reward total was -21.000000. running mean: -20.520707\n",
            "resetting env. episode 834.000000, reward total was -21.000000. running mean: -20.525500\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.530245\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -20.534943\n",
            "resetting env. episode 837.000000, reward total was -21.000000. running mean: -20.539593\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.544198\n",
            "resetting env. episode 839.000000, reward total was -20.000000. running mean: -20.538756\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -20.543368\n",
            "resetting env. episode 841.000000, reward total was -20.000000. running mean: -20.537934\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.542555\n",
            "resetting env. episode 843.000000, reward total was -20.000000. running mean: -20.537129\n",
            "resetting env. episode 844.000000, reward total was -20.000000. running mean: -20.531758\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -20.536441\n",
            "resetting env. episode 846.000000, reward total was -18.000000. running mean: -20.511076\n",
            "resetting env. episode 847.000000, reward total was -20.000000. running mean: -20.505965\n",
            "resetting env. episode 848.000000, reward total was -19.000000. running mean: -20.490906\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -20.495997\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.501037\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -20.506026\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.510966\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -20.515856\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -20.520698\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.525491\n",
            "resetting env. episode 856.000000, reward total was -20.000000. running mean: -20.520236\n",
            "resetting env. episode 857.000000, reward total was -20.000000. running mean: -20.515034\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -20.519883\n",
            "resetting env. episode 859.000000, reward total was -19.000000. running mean: -20.504684\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.509638\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -20.514541\n",
            "resetting env. episode 862.000000, reward total was -19.000000. running mean: -20.499396\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -20.504402\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.509358\n",
            "resetting env. episode 865.000000, reward total was -20.000000. running mean: -20.504264\n",
            "resetting env. episode 866.000000, reward total was -18.000000. running mean: -20.479222\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.484429\n",
            "resetting env. episode 868.000000, reward total was -21.000000. running mean: -20.489585\n",
            "resetting env. episode 869.000000, reward total was -18.000000. running mean: -20.464689\n",
            "resetting env. episode 870.000000, reward total was -21.000000. running mean: -20.470042\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -20.475342\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -20.480589\n",
            "resetting env. episode 873.000000, reward total was -20.000000. running mean: -20.475783\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.481025\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -20.476215\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -20.481452\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.486638\n",
            "resetting env. episode 878.000000, reward total was -20.000000. running mean: -20.481772\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -20.486954\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -20.492084\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.497163\n",
            "resetting env. episode 882.000000, reward total was -17.000000. running mean: -20.462192\n",
            "resetting env. episode 883.000000, reward total was -21.000000. running mean: -20.467570\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.472894\n",
            "resetting env. episode 885.000000, reward total was -21.000000. running mean: -20.478165\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -20.483384\n",
            "resetting env. episode 887.000000, reward total was -18.000000. running mean: -20.458550\n",
            "resetting env. episode 888.000000, reward total was -20.000000. running mean: -20.453964\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -20.459425\n",
            "resetting env. episode 890.000000, reward total was -20.000000. running mean: -20.454830\n",
            "resetting env. episode 891.000000, reward total was -20.000000. running mean: -20.450282\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -20.455779\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -20.451221\n",
            "resetting env. episode 894.000000, reward total was -20.000000. running mean: -20.446709\n",
            "resetting env. episode 895.000000, reward total was -21.000000. running mean: -20.452242\n",
            "resetting env. episode 896.000000, reward total was -20.000000. running mean: -20.447720\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.453243\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.458710\n",
            "resetting env. episode 899.000000, reward total was -19.000000. running mean: -20.444123\n",
            "resetting env. episode 900.000000, reward total was -20.000000. running mean: -20.439682\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.445285\n",
            "resetting env. episode 902.000000, reward total was -21.000000. running mean: -20.450832\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.456324\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -20.461761\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.467143\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.472472\n",
            "resetting env. episode 907.000000, reward total was -19.000000. running mean: -20.457747\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.463169\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -20.458538\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -20.463952\n",
            "resetting env. episode 911.000000, reward total was -21.000000. running mean: -20.469313\n",
            "resetting env. episode 912.000000, reward total was -20.000000. running mean: -20.464620\n",
            "resetting env. episode 913.000000, reward total was -21.000000. running mean: -20.469973\n",
            "resetting env. episode 914.000000, reward total was -19.000000. running mean: -20.455274\n",
            "resetting env. episode 915.000000, reward total was -17.000000. running mean: -20.420721\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.426514\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.432249\n",
            "resetting env. episode 918.000000, reward total was -20.000000. running mean: -20.427926\n",
            "resetting env. episode 919.000000, reward total was -20.000000. running mean: -20.423647\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.429410\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -20.435116\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -20.440765\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -20.446357\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.451894\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -20.457375\n",
            "resetting env. episode 926.000000, reward total was -19.000000. running mean: -20.442801\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -20.448373\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.453889\n",
            "resetting env. episode 929.000000, reward total was -20.000000. running mean: -20.449351\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.454857\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -20.460308\n",
            "resetting env. episode 932.000000, reward total was -20.000000. running mean: -20.455705\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -20.461148\n",
            "resetting env. episode 934.000000, reward total was -19.000000. running mean: -20.446537\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -20.452071\n",
            "resetting env. episode 936.000000, reward total was -21.000000. running mean: -20.457551\n",
            "resetting env. episode 937.000000, reward total was -21.000000. running mean: -20.462975\n",
            "resetting env. episode 938.000000, reward total was -19.000000. running mean: -20.448346\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -20.443862\n",
            "resetting env. episode 940.000000, reward total was -19.000000. running mean: -20.429423\n",
            "resetting env. episode 941.000000, reward total was -19.000000. running mean: -20.415129\n",
            "resetting env. episode 942.000000, reward total was -21.000000. running mean: -20.420978\n",
            "resetting env. episode 943.000000, reward total was -19.000000. running mean: -20.406768\n",
            "resetting env. episode 944.000000, reward total was -19.000000. running mean: -20.392700\n",
            "resetting env. episode 945.000000, reward total was -20.000000. running mean: -20.388773\n",
            "resetting env. episode 946.000000, reward total was -20.000000. running mean: -20.384886\n",
            "resetting env. episode 947.000000, reward total was -21.000000. running mean: -20.391037\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -20.397126\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -20.403155\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.409124\n",
            "resetting env. episode 951.000000, reward total was -20.000000. running mean: -20.405032\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -20.410982\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.416872\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.422704\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -20.428477\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -20.434192\n",
            "resetting env. episode 957.000000, reward total was -20.000000. running mean: -20.429850\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -20.435551\n",
            "resetting env. episode 959.000000, reward total was -20.000000. running mean: -20.431196\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -20.436884\n",
            "resetting env. episode 961.000000, reward total was -19.000000. running mean: -20.422515\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -20.428290\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.434007\n",
            "resetting env. episode 964.000000, reward total was -20.000000. running mean: -20.429667\n",
            "resetting env. episode 965.000000, reward total was -21.000000. running mean: -20.435370\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.441017\n",
            "resetting env. episode 967.000000, reward total was -19.000000. running mean: -20.426606\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -20.432340\n",
            "resetting env. episode 969.000000, reward total was -19.000000. running mean: -20.418017\n",
            "resetting env. episode 970.000000, reward total was -20.000000. running mean: -20.413837\n",
            "resetting env. episode 971.000000, reward total was -20.000000. running mean: -20.409698\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.415601\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.421445\n",
            "resetting env. episode 974.000000, reward total was -21.000000. running mean: -20.427231\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.432959\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.438629\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -20.444243\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.449800\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.455302\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -20.460749\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.466142\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.471480\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.476766\n",
            "resetting env. episode 984.000000, reward total was -21.000000. running mean: -20.481998\n",
            "resetting env. episode 985.000000, reward total was -20.000000. running mean: -20.477178\n",
            "resetting env. episode 986.000000, reward total was -19.000000. running mean: -20.462406\n",
            "resetting env. episode 987.000000, reward total was -21.000000. running mean: -20.467782\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -20.473104\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -20.478373\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.483589\n",
            "resetting env. episode 991.000000, reward total was -21.000000. running mean: -20.488754\n",
            "resetting env. episode 992.000000, reward total was -20.000000. running mean: -20.483866\n",
            "resetting env. episode 993.000000, reward total was -20.000000. running mean: -20.479027\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.484237\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.489395\n",
            "resetting env. episode 996.000000, reward total was -20.000000. running mean: -20.484501\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.489656\n",
            "resetting env. episode 998.000000, reward total was -21.000000. running mean: -20.494759\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -20.499812\n",
            "resetting env. episode 1000.000000, reward total was -20.000000. running mean: -20.494814\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -20.499865\n",
            "resetting env. episode 1002.000000, reward total was -21.000000. running mean: -20.504867\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.509818\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -20.514720\n",
            "resetting env. episode 1005.000000, reward total was -20.000000. running mean: -20.509573\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.514477\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -20.519332\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -20.524139\n",
            "resetting env. episode 1009.000000, reward total was -20.000000. running mean: -20.518897\n",
            "resetting env. episode 1010.000000, reward total was -19.000000. running mean: -20.503709\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.508671\n",
            "resetting env. episode 1012.000000, reward total was -17.000000. running mean: -20.473585\n",
            "resetting env. episode 1013.000000, reward total was -19.000000. running mean: -20.458849\n",
            "resetting env. episode 1014.000000, reward total was -19.000000. running mean: -20.444260\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.449818\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -20.445320\n",
            "resetting env. episode 1017.000000, reward total was -20.000000. running mean: -20.440866\n",
            "resetting env. episode 1018.000000, reward total was -21.000000. running mean: -20.446458\n",
            "resetting env. episode 1019.000000, reward total was -20.000000. running mean: -20.441993\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.447573\n",
            "resetting env. episode 1021.000000, reward total was -21.000000. running mean: -20.453098\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -20.458567\n",
            "resetting env. episode 1023.000000, reward total was -20.000000. running mean: -20.453981\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.459441\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -20.464847\n",
            "resetting env. episode 1026.000000, reward total was -19.000000. running mean: -20.450198\n",
            "resetting env. episode 1027.000000, reward total was -19.000000. running mean: -20.435696\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.441339\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.446926\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -20.452457\n",
            "resetting env. episode 1031.000000, reward total was -20.000000. running mean: -20.447932\n",
            "resetting env. episode 1032.000000, reward total was -20.000000. running mean: -20.443453\n",
            "resetting env. episode 1033.000000, reward total was -21.000000. running mean: -20.449018\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -20.454528\n",
            "resetting env. episode 1035.000000, reward total was -20.000000. running mean: -20.449983\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -20.455483\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.460928\n",
            "resetting env. episode 1038.000000, reward total was -21.000000. running mean: -20.466319\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.471656\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.476939\n",
            "resetting env. episode 1041.000000, reward total was -19.000000. running mean: -20.462170\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.467548\n",
            "resetting env. episode 1043.000000, reward total was -20.000000. running mean: -20.462872\n",
            "resetting env. episode 1044.000000, reward total was -20.000000. running mean: -20.458244\n",
            "resetting env. episode 1045.000000, reward total was -21.000000. running mean: -20.463661\n",
            "resetting env. episode 1046.000000, reward total was -20.000000. running mean: -20.459025\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -20.464434\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -20.469790\n",
            "resetting env. episode 1049.000000, reward total was -21.000000. running mean: -20.475092\n",
            "resetting env. episode 1050.000000, reward total was -20.000000. running mean: -20.470341\n",
            "resetting env. episode 1051.000000, reward total was -20.000000. running mean: -20.465638\n",
            "resetting env. episode 1052.000000, reward total was -21.000000. running mean: -20.470981\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -20.476272\n",
            "resetting env. episode 1054.000000, reward total was -21.000000. running mean: -20.481509\n",
            "resetting env. episode 1055.000000, reward total was -20.000000. running mean: -20.476694\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -20.481927\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -20.487108\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -20.492237\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -20.497314\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -20.502341\n",
            "resetting env. episode 1061.000000, reward total was -20.000000. running mean: -20.497318\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.502344\n",
            "resetting env. episode 1063.000000, reward total was -20.000000. running mean: -20.497321\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -20.502348\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.507324\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -20.502251\n",
            "resetting env. episode 1067.000000, reward total was -20.000000. running mean: -20.497229\n",
            "resetting env. episode 1068.000000, reward total was -21.000000. running mean: -20.502256\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -20.507234\n",
            "resetting env. episode 1070.000000, reward total was -18.000000. running mean: -20.482161\n",
            "resetting env. episode 1071.000000, reward total was -19.000000. running mean: -20.467340\n",
            "resetting env. episode 1072.000000, reward total was -20.000000. running mean: -20.462666\n",
            "resetting env. episode 1073.000000, reward total was -20.000000. running mean: -20.458040\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.463459\n",
            "resetting env. episode 1075.000000, reward total was -20.000000. running mean: -20.458825\n",
            "resetting env. episode 1076.000000, reward total was -20.000000. running mean: -20.454236\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.459694\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -20.465097\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.470446\n",
            "resetting env. episode 1080.000000, reward total was -21.000000. running mean: -20.475742\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -20.480984\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -20.486174\n",
            "resetting env. episode 1083.000000, reward total was -20.000000. running mean: -20.481313\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.486500\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -20.491635\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.496718\n",
            "resetting env. episode 1087.000000, reward total was -19.000000. running mean: -20.481751\n",
            "resetting env. episode 1088.000000, reward total was -19.000000. running mean: -20.466934\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -20.462264\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.467642\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.472965\n",
            "resetting env. episode 1092.000000, reward total was -21.000000. running mean: -20.478236\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -20.483453\n",
            "resetting env. episode 1094.000000, reward total was -21.000000. running mean: -20.488619\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -20.493732\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -20.498795\n",
            "resetting env. episode 1097.000000, reward total was -17.000000. running mean: -20.463807\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -20.469169\n",
            "resetting env. episode 1099.000000, reward total was -19.000000. running mean: -20.454477\n",
            "resetting env. episode 1100.000000, reward total was -21.000000. running mean: -20.459933\n",
            "resetting env. episode 1101.000000, reward total was -18.000000. running mean: -20.435333\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.440980\n",
            "resetting env. episode 1103.000000, reward total was -20.000000. running mean: -20.436570\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -20.442204\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.447782\n",
            "resetting env. episode 1106.000000, reward total was -20.000000. running mean: -20.443305\n",
            "resetting env. episode 1107.000000, reward total was -20.000000. running mean: -20.438872\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -20.444483\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -20.450038\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -20.455538\n",
            "resetting env. episode 1111.000000, reward total was -21.000000. running mean: -20.460982\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -20.466372\n",
            "resetting env. episode 1113.000000, reward total was -20.000000. running mean: -20.461709\n",
            "resetting env. episode 1114.000000, reward total was -18.000000. running mean: -20.437092\n",
            "resetting env. episode 1115.000000, reward total was -20.000000. running mean: -20.432721\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.438394\n",
            "resetting env. episode 1117.000000, reward total was -18.000000. running mean: -20.414010\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -20.419869\n",
            "resetting env. episode 1119.000000, reward total was -20.000000. running mean: -20.415671\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -20.421514\n",
            "resetting env. episode 1121.000000, reward total was -19.000000. running mean: -20.407299\n",
            "resetting env. episode 1122.000000, reward total was -20.000000. running mean: -20.403226\n",
            "resetting env. episode 1123.000000, reward total was -19.000000. running mean: -20.389194\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.395302\n",
            "resetting env. episode 1125.000000, reward total was -21.000000. running mean: -20.401349\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -20.407335\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.413262\n",
            "resetting env. episode 1128.000000, reward total was -21.000000. running mean: -20.419129\n",
            "resetting env. episode 1129.000000, reward total was -19.000000. running mean: -20.404938\n",
            "resetting env. episode 1130.000000, reward total was -21.000000. running mean: -20.410889\n",
            "resetting env. episode 1131.000000, reward total was -19.000000. running mean: -20.396780\n",
            "resetting env. episode 1132.000000, reward total was -19.000000. running mean: -20.382812\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -20.388984\n",
            "resetting env. episode 1134.000000, reward total was -19.000000. running mean: -20.375094\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -20.371343\n",
            "resetting env. episode 1136.000000, reward total was -20.000000. running mean: -20.367630\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.373953\n",
            "resetting env. episode 1138.000000, reward total was -20.000000. running mean: -20.370214\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -20.376512\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -20.382747\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -20.388919\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -20.395030\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -20.401080\n",
            "resetting env. episode 1144.000000, reward total was -21.000000. running mean: -20.407069\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -20.412998\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -20.418868\n",
            "resetting env. episode 1147.000000, reward total was -21.000000. running mean: -20.424679\n",
            "resetting env. episode 1148.000000, reward total was -19.000000. running mean: -20.410433\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.416328\n",
            "resetting env. episode 1150.000000, reward total was -21.000000. running mean: -20.422165\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -20.427943\n",
            "resetting env. episode 1152.000000, reward total was -20.000000. running mean: -20.423664\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -20.429427\n",
            "resetting env. episode 1154.000000, reward total was -18.000000. running mean: -20.405133\n",
            "resetting env. episode 1155.000000, reward total was -20.000000. running mean: -20.401082\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.407071\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -20.413000\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.418870\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -20.414681\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -20.420535\n",
            "resetting env. episode 1161.000000, reward total was -21.000000. running mean: -20.426329\n",
            "resetting env. episode 1162.000000, reward total was -20.000000. running mean: -20.422066\n",
            "resetting env. episode 1163.000000, reward total was -21.000000. running mean: -20.427845\n",
            "resetting env. episode 1164.000000, reward total was -21.000000. running mean: -20.433567\n",
            "resetting env. episode 1165.000000, reward total was -21.000000. running mean: -20.439231\n",
            "resetting env. episode 1166.000000, reward total was -20.000000. running mean: -20.434839\n",
            "resetting env. episode 1167.000000, reward total was -20.000000. running mean: -20.430491\n",
            "resetting env. episode 1168.000000, reward total was -21.000000. running mean: -20.436186\n",
            "resetting env. episode 1169.000000, reward total was -20.000000. running mean: -20.431824\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.437506\n",
            "resetting env. episode 1171.000000, reward total was -18.000000. running mean: -20.413130\n",
            "resetting env. episode 1172.000000, reward total was -21.000000. running mean: -20.418999\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -20.424809\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -20.430561\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -20.436255\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -20.431893\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -20.437574\n",
            "resetting env. episode 1178.000000, reward total was -21.000000. running mean: -20.443198\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -20.448766\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -20.454279\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -20.459736\n",
            "resetting env. episode 1182.000000, reward total was -19.000000. running mean: -20.445138\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -20.450687\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -20.456180\n",
            "resetting env. episode 1185.000000, reward total was -21.000000. running mean: -20.461618\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.467002\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -20.472332\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -20.477609\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.482833\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -20.488004\n",
            "resetting env. episode 1191.000000, reward total was -20.000000. running mean: -20.483124\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -20.488293\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.493410\n",
            "resetting env. episode 1194.000000, reward total was -20.000000. running mean: -20.488476\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.493591\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -20.488655\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -20.493769\n",
            "resetting env. episode 1198.000000, reward total was -20.000000. running mean: -20.488831\n",
            "resetting env. episode 1199.000000, reward total was -21.000000. running mean: -20.493943\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.499003\n",
            "resetting env. episode 1201.000000, reward total was -20.000000. running mean: -20.494013\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.499073\n",
            "resetting env. episode 1203.000000, reward total was -20.000000. running mean: -20.494083\n",
            "resetting env. episode 1204.000000, reward total was -19.000000. running mean: -20.479142\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -20.484350\n",
            "resetting env. episode 1206.000000, reward total was -21.000000. running mean: -20.489507\n",
            "resetting env. episode 1207.000000, reward total was -21.000000. running mean: -20.494612\n",
            "resetting env. episode 1208.000000, reward total was -19.000000. running mean: -20.479666\n",
            "resetting env. episode 1209.000000, reward total was -19.000000. running mean: -20.464869\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.470220\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -20.475518\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -20.480763\n",
            "resetting env. episode 1213.000000, reward total was -20.000000. running mean: -20.475955\n",
            "resetting env. episode 1214.000000, reward total was -21.000000. running mean: -20.481196\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -20.486384\n",
            "resetting env. episode 1216.000000, reward total was -21.000000. running mean: -20.491520\n",
            "resetting env. episode 1217.000000, reward total was -21.000000. running mean: -20.496605\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -20.501639\n",
            "resetting env. episode 1219.000000, reward total was -19.000000. running mean: -20.486622\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -20.491756\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -20.496839\n",
            "resetting env. episode 1222.000000, reward total was -18.000000. running mean: -20.471870\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -20.477151\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -20.482380\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -20.487556\n",
            "resetting env. episode 1226.000000, reward total was -20.000000. running mean: -20.482681\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -20.487854\n",
            "resetting env. episode 1228.000000, reward total was -19.000000. running mean: -20.472975\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -20.478245\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -20.483463\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -20.488628\n",
            "resetting env. episode 1232.000000, reward total was -19.000000. running mean: -20.473742\n",
            "resetting env. episode 1233.000000, reward total was -19.000000. running mean: -20.459005\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -20.464415\n",
            "resetting env. episode 1235.000000, reward total was -21.000000. running mean: -20.469770\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -20.475073\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -20.480322\n",
            "resetting env. episode 1238.000000, reward total was -19.000000. running mean: -20.465519\n",
            "resetting env. episode 1239.000000, reward total was -20.000000. running mean: -20.460864\n",
            "resetting env. episode 1240.000000, reward total was -21.000000. running mean: -20.466255\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -20.471592\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.476877\n",
            "resetting env. episode 1243.000000, reward total was -21.000000. running mean: -20.482108\n",
            "resetting env. episode 1244.000000, reward total was -21.000000. running mean: -20.487287\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -20.492414\n",
            "resetting env. episode 1246.000000, reward total was -20.000000. running mean: -20.487490\n",
            "resetting env. episode 1247.000000, reward total was -20.000000. running mean: -20.482615\n",
            "resetting env. episode 1248.000000, reward total was -19.000000. running mean: -20.467789\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.473111\n",
            "resetting env. episode 1250.000000, reward total was -20.000000. running mean: -20.468380\n",
            "resetting env. episode 1251.000000, reward total was -21.000000. running mean: -20.473696\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -20.478959\n",
            "resetting env. episode 1253.000000, reward total was -20.000000. running mean: -20.474169\n",
            "resetting env. episode 1254.000000, reward total was -20.000000. running mean: -20.469428\n",
            "resetting env. episode 1255.000000, reward total was -21.000000. running mean: -20.474733\n",
            "resetting env. episode 1256.000000, reward total was -21.000000. running mean: -20.479986\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -20.485186\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -20.490334\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -20.495431\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -20.500477\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.505472\n",
            "resetting env. episode 1262.000000, reward total was -21.000000. running mean: -20.510417\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -20.515313\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -20.520160\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -20.524958\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.529709\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -20.534412\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -20.539067\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.543677\n",
            "resetting env. episode 1270.000000, reward total was -20.000000. running mean: -20.538240\n",
            "resetting env. episode 1271.000000, reward total was -21.000000. running mean: -20.542858\n",
            "resetting env. episode 1272.000000, reward total was -21.000000. running mean: -20.547429\n",
            "resetting env. episode 1273.000000, reward total was -20.000000. running mean: -20.541955\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -20.546535\n",
            "resetting env. episode 1275.000000, reward total was -20.000000. running mean: -20.541070\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -20.545659\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -20.550203\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -20.554701\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -20.559154\n",
            "resetting env. episode 1280.000000, reward total was -21.000000. running mean: -20.563562\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -20.567926\n",
            "resetting env. episode 1282.000000, reward total was -20.000000. running mean: -20.562247\n",
            "resetting env. episode 1283.000000, reward total was -19.000000. running mean: -20.546625\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -20.551158\n",
            "resetting env. episode 1285.000000, reward total was -20.000000. running mean: -20.545647\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -20.550190\n",
            "resetting env. episode 1287.000000, reward total was -21.000000. running mean: -20.554688\n",
            "resetting env. episode 1288.000000, reward total was -21.000000. running mean: -20.559142\n",
            "resetting env. episode 1289.000000, reward total was -21.000000. running mean: -20.563550\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -20.567915\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -20.572235\n",
            "resetting env. episode 1292.000000, reward total was -21.000000. running mean: -20.576513\n",
            "resetting env. episode 1293.000000, reward total was -19.000000. running mean: -20.560748\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -20.565141\n",
            "resetting env. episode 1295.000000, reward total was -20.000000. running mean: -20.559489\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.563894\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -20.568255\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -20.572573\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -20.576847\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -20.581079\n",
            "resetting env. episode 1301.000000, reward total was -21.000000. running mean: -20.585268\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -20.589415\n",
            "resetting env. episode 1303.000000, reward total was -20.000000. running mean: -20.583521\n",
            "resetting env. episode 1304.000000, reward total was -21.000000. running mean: -20.587686\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.591809\n",
            "resetting env. episode 1306.000000, reward total was -20.000000. running mean: -20.585891\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -20.590032\n",
            "resetting env. episode 1308.000000, reward total was -20.000000. running mean: -20.584132\n",
            "resetting env. episode 1309.000000, reward total was -21.000000. running mean: -20.588290\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -20.592407\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.596483\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -20.600518\n",
            "resetting env. episode 1313.000000, reward total was -19.000000. running mean: -20.584513\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -20.588668\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -20.582781\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -20.586954\n",
            "resetting env. episode 1317.000000, reward total was -20.000000. running mean: -20.581084\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -20.585273\n",
            "resetting env. episode 1319.000000, reward total was -18.000000. running mean: -20.559420\n",
            "resetting env. episode 1320.000000, reward total was -18.000000. running mean: -20.533826\n",
            "resetting env. episode 1321.000000, reward total was -19.000000. running mean: -20.518488\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -20.523303\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -20.528070\n",
            "resetting env. episode 1324.000000, reward total was -21.000000. running mean: -20.532789\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -20.537462\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -20.542087\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.546666\n",
            "resetting env. episode 1328.000000, reward total was -19.000000. running mean: -20.531199\n",
            "resetting env. episode 1329.000000, reward total was -21.000000. running mean: -20.535887\n",
            "resetting env. episode 1330.000000, reward total was -21.000000. running mean: -20.540529\n",
            "resetting env. episode 1331.000000, reward total was -21.000000. running mean: -20.545123\n",
            "resetting env. episode 1332.000000, reward total was -20.000000. running mean: -20.539672\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -20.544275\n",
            "resetting env. episode 1334.000000, reward total was -20.000000. running mean: -20.538833\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -20.543444\n",
            "resetting env. episode 1336.000000, reward total was -20.000000. running mean: -20.538010\n",
            "resetting env. episode 1337.000000, reward total was -21.000000. running mean: -20.542630\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -20.547203\n",
            "resetting env. episode 1339.000000, reward total was -21.000000. running mean: -20.551731\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -20.556214\n",
            "resetting env. episode 1341.000000, reward total was -21.000000. running mean: -20.560652\n",
            "resetting env. episode 1342.000000, reward total was -21.000000. running mean: -20.565045\n",
            "resetting env. episode 1343.000000, reward total was -19.000000. running mean: -20.549395\n",
            "resetting env. episode 1344.000000, reward total was -19.000000. running mean: -20.533901\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.538562\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -20.543176\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -20.547745\n",
            "resetting env. episode 1348.000000, reward total was -21.000000. running mean: -20.552267\n",
            "resetting env. episode 1349.000000, reward total was -21.000000. running mean: -20.556744\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -20.561177\n",
            "resetting env. episode 1351.000000, reward total was -21.000000. running mean: -20.565565\n",
            "resetting env. episode 1352.000000, reward total was -21.000000. running mean: -20.569910\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.574210\n",
            "resetting env. episode 1354.000000, reward total was -20.000000. running mean: -20.568468\n",
            "resetting env. episode 1355.000000, reward total was -21.000000. running mean: -20.572784\n",
            "resetting env. episode 1356.000000, reward total was -20.000000. running mean: -20.567056\n",
            "resetting env. episode 1357.000000, reward total was -21.000000. running mean: -20.571385\n",
            "resetting env. episode 1358.000000, reward total was -20.000000. running mean: -20.565671\n",
            "resetting env. episode 1359.000000, reward total was -19.000000. running mean: -20.550015\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -20.554515\n",
            "resetting env. episode 1361.000000, reward total was -20.000000. running mean: -20.548969\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -20.553480\n",
            "resetting env. episode 1363.000000, reward total was -21.000000. running mean: -20.557945\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -20.562365\n",
            "resetting env. episode 1365.000000, reward total was -20.000000. running mean: -20.556742\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -20.561174\n",
            "resetting env. episode 1367.000000, reward total was -19.000000. running mean: -20.545563\n",
            "resetting env. episode 1368.000000, reward total was -18.000000. running mean: -20.520107\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -20.524906\n",
            "resetting env. episode 1370.000000, reward total was -20.000000. running mean: -20.519657\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.524460\n",
            "resetting env. episode 1372.000000, reward total was -20.000000. running mean: -20.519216\n",
            "resetting env. episode 1373.000000, reward total was -21.000000. running mean: -20.524024\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.528783\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -20.533496\n",
            "resetting env. episode 1376.000000, reward total was -21.000000. running mean: -20.538161\n",
            "resetting env. episode 1377.000000, reward total was -20.000000. running mean: -20.532779\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -20.537451\n",
            "resetting env. episode 1379.000000, reward total was -21.000000. running mean: -20.542077\n",
            "resetting env. episode 1380.000000, reward total was -19.000000. running mean: -20.526656\n",
            "resetting env. episode 1381.000000, reward total was -21.000000. running mean: -20.531389\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -20.536075\n",
            "resetting env. episode 1383.000000, reward total was -21.000000. running mean: -20.540715\n",
            "resetting env. episode 1384.000000, reward total was -21.000000. running mean: -20.545308\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -20.549854\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -20.554356\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -20.558812\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.563224\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -20.567592\n",
            "resetting env. episode 1390.000000, reward total was -19.000000. running mean: -20.551916\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -20.556397\n",
            "resetting env. episode 1392.000000, reward total was -21.000000. running mean: -20.560833\n",
            "resetting env. episode 1393.000000, reward total was -21.000000. running mean: -20.565225\n",
            "resetting env. episode 1394.000000, reward total was -21.000000. running mean: -20.569572\n",
            "resetting env. episode 1395.000000, reward total was -21.000000. running mean: -20.573877\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.578138\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -20.582356\n",
            "resetting env. episode 1398.000000, reward total was -20.000000. running mean: -20.576533\n",
            "resetting env. episode 1399.000000, reward total was -21.000000. running mean: -20.580768\n",
            "resetting env. episode 1400.000000, reward total was -19.000000. running mean: -20.564960\n",
            "resetting env. episode 1401.000000, reward total was -21.000000. running mean: -20.569310\n",
            "resetting env. episode 1402.000000, reward total was -20.000000. running mean: -20.563617\n",
            "resetting env. episode 1403.000000, reward total was -19.000000. running mean: -20.547981\n",
            "resetting env. episode 1404.000000, reward total was -20.000000. running mean: -20.542501\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -20.547076\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.551605\n",
            "resetting env. episode 1407.000000, reward total was -20.000000. running mean: -20.546089\n",
            "resetting env. episode 1408.000000, reward total was -19.000000. running mean: -20.530629\n",
            "resetting env. episode 1409.000000, reward total was -20.000000. running mean: -20.525322\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.530069\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -20.534768\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.539421\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.544026\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -20.548586\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.553100\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.557569\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -20.551994\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.556474\n",
            "resetting env. episode 1419.000000, reward total was -20.000000. running mean: -20.550909\n",
            "resetting env. episode 1420.000000, reward total was -20.000000. running mean: -20.545400\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -20.539946\n",
            "resetting env. episode 1422.000000, reward total was -21.000000. running mean: -20.544546\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -20.549101\n",
            "resetting env. episode 1424.000000, reward total was -20.000000. running mean: -20.543610\n",
            "resetting env. episode 1425.000000, reward total was -21.000000. running mean: -20.548174\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -20.552692\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -20.557165\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -20.561594\n",
            "resetting env. episode 1429.000000, reward total was -21.000000. running mean: -20.565978\n",
            "resetting env. episode 1430.000000, reward total was -21.000000. running mean: -20.570318\n",
            "resetting env. episode 1431.000000, reward total was -21.000000. running mean: -20.574615\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -20.578868\n",
            "resetting env. episode 1433.000000, reward total was -20.000000. running mean: -20.573080\n",
            "resetting env. episode 1434.000000, reward total was -20.000000. running mean: -20.567349\n",
            "resetting env. episode 1435.000000, reward total was -20.000000. running mean: -20.561675\n",
            "resetting env. episode 1436.000000, reward total was -21.000000. running mean: -20.566059\n",
            "resetting env. episode 1437.000000, reward total was -20.000000. running mean: -20.560398\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.564794\n",
            "resetting env. episode 1439.000000, reward total was -21.000000. running mean: -20.569146\n",
            "resetting env. episode 1440.000000, reward total was -20.000000. running mean: -20.563455\n",
            "resetting env. episode 1441.000000, reward total was -20.000000. running mean: -20.557820\n",
            "resetting env. episode 1442.000000, reward total was -20.000000. running mean: -20.552242\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.556720\n",
            "resetting env. episode 1444.000000, reward total was -19.000000. running mean: -20.541152\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -20.535741\n",
            "resetting env. episode 1446.000000, reward total was -20.000000. running mean: -20.530383\n",
            "resetting env. episode 1447.000000, reward total was -21.000000. running mean: -20.535080\n",
            "resetting env. episode 1448.000000, reward total was -21.000000. running mean: -20.539729\n",
            "resetting env. episode 1449.000000, reward total was -21.000000. running mean: -20.544332\n",
            "resetting env. episode 1450.000000, reward total was -21.000000. running mean: -20.548888\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -20.553399\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.557865\n",
            "resetting env. episode 1453.000000, reward total was -20.000000. running mean: -20.552287\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -20.556764\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -20.561196\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -20.565584\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -20.569928\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -20.574229\n",
            "resetting env. episode 1459.000000, reward total was -20.000000. running mean: -20.568487\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -20.572802\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -20.577074\n",
            "resetting env. episode 1462.000000, reward total was -20.000000. running mean: -20.571303\n",
            "resetting env. episode 1463.000000, reward total was -20.000000. running mean: -20.565590\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -20.569934\n",
            "resetting env. episode 1465.000000, reward total was -19.000000. running mean: -20.554235\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -20.558693\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -20.563106\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -20.567475\n",
            "resetting env. episode 1469.000000, reward total was -18.000000. running mean: -20.541800\n",
            "resetting env. episode 1470.000000, reward total was -21.000000. running mean: -20.546382\n",
            "resetting env. episode 1471.000000, reward total was -21.000000. running mean: -20.550918\n",
            "resetting env. episode 1472.000000, reward total was -20.000000. running mean: -20.545409\n",
            "resetting env. episode 1473.000000, reward total was -20.000000. running mean: -20.539955\n",
            "resetting env. episode 1474.000000, reward total was -20.000000. running mean: -20.534555\n",
            "resetting env. episode 1475.000000, reward total was -20.000000. running mean: -20.529210\n",
            "resetting env. episode 1476.000000, reward total was -19.000000. running mean: -20.513918\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -20.518778\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -20.523591\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -20.528355\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -20.533071\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -20.537740\n",
            "resetting env. episode 1482.000000, reward total was -20.000000. running mean: -20.532363\n",
            "resetting env. episode 1483.000000, reward total was -21.000000. running mean: -20.537039\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -20.541669\n",
            "resetting env. episode 1485.000000, reward total was -19.000000. running mean: -20.526252\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -20.530990\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.535680\n",
            "resetting env. episode 1488.000000, reward total was -21.000000. running mean: -20.540323\n",
            "resetting env. episode 1489.000000, reward total was -21.000000. running mean: -20.544920\n",
            "resetting env. episode 1490.000000, reward total was -21.000000. running mean: -20.549471\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -20.553976\n",
            "resetting env. episode 1492.000000, reward total was -21.000000. running mean: -20.558436\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -20.562852\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.567223\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -20.571551\n",
            "resetting env. episode 1496.000000, reward total was -21.000000. running mean: -20.575836\n",
            "resetting env. episode 1497.000000, reward total was -20.000000. running mean: -20.570077\n",
            "resetting env. episode 1498.000000, reward total was -20.000000. running mean: -20.564376\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -20.568733\n",
            "resetting env. episode 1500.000000, reward total was -19.000000. running mean: -20.553045\n",
            "CPU times: user 1h 11min 48s, sys: 32min 5s, total: 1h 43min 54s\n",
            "Wall time: 53min 55s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "CteN7XKMVGqg",
        "outputId": "261d2d6c-9f66-4dcb-f92b-1808f886fd7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "ZYA0HgMoO77a"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# pickle.dump(model, open('model.pkl', 'wb'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "pg_from_scratch_(rate_7).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}