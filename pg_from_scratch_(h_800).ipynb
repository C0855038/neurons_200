{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l9hHvTk6ec8"
      },
      "source": [
        "# Policy Gradient\n",
        "\n",
        "* http://karpathy.github.io/2016/05/31/rl/\n",
        "* https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
        "* https://github.com/gameofdimension/policy-gradient-pong\n",
        "* https://www.youtube.com/watch?v=tqrcjHuNdmQ\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqkOdLyN9Ylm"
      },
      "source": [
        "## Step 1: Installation for Colab - just execute these cells and do not worry too much\n",
        "\n",
        "* http://nbviewer.jupyter.org/github/patrickmineault/xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb \n",
        "* https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi\n",
        "* https://nyu-cds.github.io/python-mpi/setup/\n",
        "* https://medium.com/@kaleajit27/reinforcement-learning-on-google-colab-9cb2e1ef51e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF9MAVI16huj",
        "outputId": "5a533a72-5c4f-4638-f239-95838fbe65d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install python-opengl -y  >/dev/null\n",
        "!apt install xvfb -y >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fSC11TfN6p69"
      },
      "outputs": [],
      "source": [
        "!pip install pyvirtualdisplay >/dev/null\n",
        "!pip install piglet >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "caiHE2hy6xrf"
      },
      "outputs": [],
      "source": [
        "# from pyvirtualdisplay import Display\n",
        "# display = Display(visible=0, size=(1400, 900))\n",
        "# display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "a443411e-51ca-4865-c51d-c2bd1e970beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 33.9 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=f72e7585c9543494e35f58b8e2f22b1e66247d9203c9c56cbcd5175ea46f7069\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "44fbb829-3148-4b82-e15d-b641a155a308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "208220a2-6a96-48ae-c8f2-91a428d5b5aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "df89c947-5ad5-475c-cadf-3563fd2fef07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "6dc1598a-8d5d-44be-ac90-ce1ddb3d6144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -20.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  env.close()\n",
        "  display_frames_as_gif(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 800 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 3 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "6214a41e-c973-4cab-8eab-2d9811291381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -19.020000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -19.029800\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.049502\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -19.069007\n",
            "resetting env. episode 6.000000, reward total was -17.000000. running mean: -19.048317\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.067834\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -19.087155\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.106284\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -19.125221\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -19.143969\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -19.152529\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -19.171004\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -19.179294\n",
            "resetting env. episode 15.000000, reward total was -18.000000. running mean: -19.167501\n",
            "resetting env. episode 16.000000, reward total was -18.000000. running mean: -19.155826\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -19.164268\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -19.182625\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -19.200799\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -19.208791\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -19.226703\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -19.244436\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.261991\n",
            "resetting env. episode 24.000000, reward total was -19.000000. running mean: -19.259371\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.276778\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.294010\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -19.301070\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -19.308059\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.324979\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.341729\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -19.348312\n",
            "resetting env. episode 32.000000, reward total was -18.000000. running mean: -19.334828\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.351480\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -19.367965\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -19.364286\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -19.380643\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -19.396836\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -19.392868\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -19.398939\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -19.404950\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -19.410900\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -19.426791\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -19.432524\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.448198\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -19.463716\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -19.469079\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -19.474388\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.489644\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.504748\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -19.499701\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -19.504704\n",
            "resetting env. episode 52.000000, reward total was -19.000000. running mean: -19.499656\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -19.514660\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -19.529513\n",
            "resetting env. episode 55.000000, reward total was -18.000000. running mean: -19.514218\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -19.519076\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -19.533885\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -19.538546\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -19.533161\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.547829\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.562351\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -19.566728\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -19.571060\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -19.575350\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -19.589596\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.603700\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -19.607663\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.621587\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.635371\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -19.639017\n",
            "resetting env. episode 71.000000, reward total was -19.000000. running mean: -19.632627\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.646301\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.659838\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -19.673239\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -19.676507\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.689742\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.702844\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.715816\n",
            "resetting env. episode 79.000000, reward total was -18.000000. running mean: -19.698658\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -19.691671\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -19.704754\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.717707\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.730530\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.743224\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.755792\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -19.768234\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -19.770552\n",
            "resetting env. episode 88.000000, reward total was -19.000000. running mean: -19.762846\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.775218\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -19.777466\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -19.769691\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -19.771994\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -19.774274\n",
            "resetting env. episode 94.000000, reward total was -19.000000. running mean: -19.766532\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -19.768866\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -19.771178\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -19.763466\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -19.775831\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.788073\n",
            "resetting env. episode 100.000000, reward total was -19.000000. running mean: -19.780192\n",
            "resetting env. episode 101.000000, reward total was -18.000000. running mean: -19.762390\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -19.774766\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -19.767019\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -19.769348\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.781655\n",
            "resetting env. episode 106.000000, reward total was -16.000000. running mean: -19.743838\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.756400\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -19.758836\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.771248\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -19.773535\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -19.765800\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.778142\n",
            "resetting env. episode 113.000000, reward total was -19.000000. running mean: -19.770360\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.782657\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -19.794830\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.806882\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.818813\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -19.810625\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.812519\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -19.824394\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.836150\n",
            "resetting env. episode 122.000000, reward total was -18.000000. running mean: -19.817788\n",
            "resetting env. episode 123.000000, reward total was -18.000000. running mean: -19.799610\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -19.811614\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -19.823498\n",
            "resetting env. episode 126.000000, reward total was -19.000000. running mean: -19.815263\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -19.827110\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -19.828839\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -19.830551\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -19.842245\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -19.853823\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -19.865285\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -19.876632\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -19.887866\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -19.898987\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -19.899997\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -19.900997\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -19.911987\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -19.912867\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -19.923739\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -19.934501\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -19.935156\n",
            "resetting env. episode 143.000000, reward total was -17.000000. running mean: -19.905805\n",
            "resetting env. episode 144.000000, reward total was -19.000000. running mean: -19.896747\n",
            "resetting env. episode 145.000000, reward total was -19.000000. running mean: -19.887779\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -19.888901\n",
            "resetting env. episode 147.000000, reward total was -19.000000. running mean: -19.880012\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -19.891212\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -19.892300\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -19.903377\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -19.904343\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -19.915300\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -19.926147\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -19.936885\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -19.937516\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -19.938141\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -19.938760\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -19.949372\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -19.949879\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -19.960380\n",
            "resetting env. episode 161.000000, reward total was -19.000000. running mean: -19.950776\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -19.951268\n",
            "resetting env. episode 163.000000, reward total was -18.000000. running mean: -19.931756\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -19.932438\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -19.943114\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -19.943682\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -19.954246\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -19.964703\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -19.975056\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -19.985306\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -19.995453\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -19.995498\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.005543\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.005488\n",
            "resetting env. episode 175.000000, reward total was -18.000000. running mean: -19.985433\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -19.995578\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -19.995623\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.005666\n",
            "resetting env. episode 179.000000, reward total was -19.000000. running mean: -19.995610\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -19.995654\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.005697\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.005640\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.015584\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.025428\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -20.015174\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.015022\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.024872\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.034623\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.044277\n",
            "resetting env. episode 190.000000, reward total was -19.000000. running mean: -20.033834\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.043496\n",
            "resetting env. episode 192.000000, reward total was -18.000000. running mean: -20.023061\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.022830\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.012602\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.012476\n",
            "resetting env. episode 196.000000, reward total was -19.000000. running mean: -20.002351\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.012327\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.022204\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.031982\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.041662\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.041246\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.050833\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.060325\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.059722\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.069124\n",
            "resetting env. episode 206.000000, reward total was -19.000000. running mean: -20.058433\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.067849\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.057170\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.066599\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.075933\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.075173\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.084422\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.083577\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.092742\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.091814\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.100896\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.099887\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.108888\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.117799\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.126621\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.125355\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.124102\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.122861\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.131632\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.130316\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.139013\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.137622\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.146246\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.154784\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.163236\n",
            "resetting env. episode 231.000000, reward total was -19.000000. running mean: -20.151604\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.150087\n",
            "resetting env. episode 233.000000, reward total was -18.000000. running mean: -20.128587\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.137301\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -20.125928\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.134668\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.133322\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.141989\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.150569\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.159063\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.147472\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.155998\n",
            "resetting env. episode 243.000000, reward total was -19.000000. running mean: -20.144438\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.142993\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.141563\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.150148\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.158646\n",
            "resetting env. episode 248.000000, reward total was -19.000000. running mean: -20.147060\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.155589\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.164033\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.162393\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.170769\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.169061\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.167371\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.175697\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.183940\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.182101\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.180280\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.188477\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.196592\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.194626\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.202680\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.210653\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.218547\n",
            "resetting env. episode 265.000000, reward total was -19.000000. running mean: -20.206361\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.214297\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.212154\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.220033\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.227833\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.225554\n",
            "resetting env. episode 271.000000, reward total was -19.000000. running mean: -20.213299\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.221166\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.228954\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.216665\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.224498\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.232253\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.239930\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.247531\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.255056\n",
            "resetting env. episode 280.000000, reward total was -18.000000. running mean: -20.232505\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.230180\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.237878\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.245500\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.253045\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.250514\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.258009\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.245429\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.252975\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.260445\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.257840\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.255262\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.262709\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.270082\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.267381\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.274708\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.281961\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.289141\n",
            "resetting env. episode 298.000000, reward total was -18.000000. running mean: -20.266250\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.273587\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.270851\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.268143\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.265461\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.272807\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.270079\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.267378\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.264704\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.262057\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.269436\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.276742\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.283975\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.281135\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.278324\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.285540\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.282685\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.289858\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.286959\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.284090\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.291249\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.298336\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.295353\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.302400\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.309376\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.306282\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.313219\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.310087\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.296986\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.294016\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.301076\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.298065\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.295085\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.302134\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.309112\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.316021\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.322861\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.329632\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.326336\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.323073\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.309842\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.316744\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.313576\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.320440\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.327236\n",
            "resetting env. episode 343.000000, reward total was -19.000000. running mean: -20.313964\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.310824\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.317716\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.324539\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.311293\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.308180\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.315098\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.321947\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.318728\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.315541\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.322385\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.329161\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.335870\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.332511\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.339186\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.335794\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.332436\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.339112\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.335721\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.332364\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.329040\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.325750\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.332492\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.339167\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.345775\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.352318\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.358794\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.365207\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.361554\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.367939\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.374260\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.380517\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.376712\n",
            "resetting env. episode 376.000000, reward total was -19.000000. running mean: -20.362945\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.359315\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.365722\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.362065\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.368444\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.374760\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.381012\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.377202\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.383430\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.369596\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.375900\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.382141\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.388319\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.394436\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.400492\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.396487\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.392522\n",
            "resetting env. episode 393.000000, reward total was -19.000000. running mean: -20.378597\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.384811\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.390963\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.387053\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.383183\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.389351\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.395457\n",
            "resetting env. episode 400.000000, reward total was -18.000000. running mean: -20.371503\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.357788\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.364210\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.370568\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.376862\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.383093\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.389262\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.395370\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.391416\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.387502\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.393627\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.389691\n",
            "resetting env. episode 412.000000, reward total was -18.000000. running mean: -20.365794\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.372136\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.378414\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.374630\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.380884\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.387075\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.383204\n",
            "resetting env. episode 419.000000, reward total was -18.000000. running mean: -20.359372\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.355779\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.352221\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.348699\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.355212\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.361660\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.368043\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.374363\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.370619\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.366913\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.373244\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.379511\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.385716\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.381859\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.388040\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.394160\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.390218\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.386316\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.392453\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.388528\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.384643\n",
            "resetting env. episode 440.000000, reward total was -18.000000. running mean: -20.360797\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.357189\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.363617\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.369981\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.376281\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.382518\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.378693\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.384906\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.391057\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.387146\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.373275\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.359542\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.365947\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.362287\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.358664\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.355078\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.351527\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.348012\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.354532\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.360986\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -20.347376\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.343903\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.340464\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.347059\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.353588\n",
            "resetting env. episode 465.000000, reward total was -18.000000. running mean: -20.330052\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.336752\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -20.323384\n",
            "resetting env. episode 468.000000, reward total was -19.000000. running mean: -20.310151\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.307049\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.313979\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.310839\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.317730\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.314553\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.311408\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.308293\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.305211\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.312158\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.319037\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.325847\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.332588\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.339262\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.345870\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.342411\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.338987\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.345597\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.342141\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.338719\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.335332\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.341979\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.338559\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.335174\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.341822\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -20.328404\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.325120\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.331868\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.328550\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.335264\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.321912\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.328692\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.335406\n",
            "CPU times: user 1h 4min, sys: 12min 15s, total: 1h 16min 15s\n",
            "Wall time: 38min 54s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "3aa7bd50-12a0-4aa8-c35f-d5e2243abb0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "d4b0d3ce-f544-4124-8917-c1ed1932973d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -19.010000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.029900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.049601\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -19.049105\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -19.058614\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.078028\n",
            "resetting env. episode 8.000000, reward total was -19.000000. running mean: -19.077248\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.096475\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -19.105510\n",
            "resetting env. episode 11.000000, reward total was -19.000000. running mean: -19.104455\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -19.123411\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -19.142177\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -19.150755\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -19.169247\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -19.187555\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -19.195679\n",
            "resetting env. episode 18.000000, reward total was -19.000000. running mean: -19.193722\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -19.201785\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -19.219767\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -19.227570\n",
            "resetting env. episode 22.000000, reward total was -19.000000. running mean: -19.225294\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.243041\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -19.250611\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.268105\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -19.275423\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -19.282669\n",
            "resetting env. episode 28.000000, reward total was -19.000000. running mean: -19.279843\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.297044\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -19.304074\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -19.321033\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -19.317823\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -19.324644\n",
            "resetting env. episode 34.000000, reward total was -19.000000. running mean: -19.321398\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -19.328184\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -19.334902\n",
            "resetting env. episode 37.000000, reward total was -19.000000. running mean: -19.331553\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.348238\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -19.364755\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -19.371108\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -19.387397\n",
            "resetting env. episode 42.000000, reward total was -19.000000. running mean: -19.383523\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -19.389687\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.405790\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -19.421733\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -19.437515\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -19.453140\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.468609\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.483923\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -19.479083\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.494293\n",
            "resetting env. episode 52.000000, reward total was -19.000000. running mean: -19.489350\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -19.484456\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -19.499612\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -19.504615\n",
            "resetting env. episode 56.000000, reward total was -18.000000. running mean: -19.489569\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -19.504674\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -19.509627\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -19.514531\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.529385\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.544091\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -19.548651\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -19.543164\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.557732\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -19.572155\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.586434\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -19.590569\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -19.594663\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.608717\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -19.612630\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.626503\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.640238\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -19.643836\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -19.647398\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.660924\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.674314\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -19.677571\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.690796\n",
            "resetting env. episode 79.000000, reward total was -19.000000. running mean: -19.683888\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -19.697049\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -19.700078\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -19.703077\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.716047\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -19.718886\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.731697\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -19.744380\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.756937\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.769367\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.781674\n",
            "resetting env. episode 90.000000, reward total was -18.000000. running mean: -19.763857\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -19.756218\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.768656\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -19.770969\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -19.773260\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.785527\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.797672\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.809695\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -19.801598\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.813582\n",
            "resetting env. episode 100.000000, reward total was -18.000000. running mean: -19.795446\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -19.807492\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -19.809417\n",
            "resetting env. episode 103.000000, reward total was -18.000000. running mean: -19.791323\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -19.803410\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -19.805376\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -19.807322\n",
            "resetting env. episode 107.000000, reward total was -16.000000. running mean: -19.769249\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -19.771556\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.783841\n",
            "resetting env. episode 110.000000, reward total was -19.000000. running mean: -19.776002\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -19.788242\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.800360\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -19.802356\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -19.804333\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -19.816289\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.828126\n",
            "resetting env. episode 117.000000, reward total was -16.000000. running mean: -19.789845\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -19.781947\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.784127\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -19.796286\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.808323\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -19.810240\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -19.812137\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -19.814016\n",
            "resetting env. episode 125.000000, reward total was -17.000000. running mean: -19.785876\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -19.798017\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -19.800037\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -19.802037\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -19.814016\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -19.815876\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -19.817717\n",
            "resetting env. episode 132.000000, reward total was -19.000000. running mean: -19.809540\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -19.811445\n",
            "resetting env. episode 134.000000, reward total was -18.000000. running mean: -19.793330\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -19.805397\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -19.817343\n",
            "resetting env. episode 137.000000, reward total was -19.000000. running mean: -19.809170\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -19.821078\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -19.822867\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -19.834638\n",
            "resetting env. episode 141.000000, reward total was -19.000000. running mean: -19.826292\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -19.838029\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -19.849649\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -19.861152\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -19.872541\n",
            "resetting env. episode 146.000000, reward total was -19.000000. running mean: -19.863815\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -19.875177\n",
            "resetting env. episode 148.000000, reward total was -18.000000. running mean: -19.856425\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -19.867861\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -19.879183\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -19.880391\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -19.891587\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -19.892671\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -19.903744\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -19.914707\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -19.915560\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -19.926404\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -19.937140\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -19.937769\n",
            "resetting env. episode 160.000000, reward total was -19.000000. running mean: -19.928391\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -19.929107\n",
            "resetting env. episode 162.000000, reward total was -19.000000. running mean: -19.919816\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -19.920618\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -19.911412\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -19.922298\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -19.933075\n",
            "resetting env. episode 167.000000, reward total was -19.000000. running mean: -19.923744\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -19.934506\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -19.945161\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -19.955710\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -19.966153\n",
            "resetting env. episode 172.000000, reward total was -19.000000. running mean: -19.956491\n",
            "resetting env. episode 173.000000, reward total was -17.000000. running mean: -19.926926\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -19.927657\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -19.928380\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -19.939097\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -19.939706\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -19.940309\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -19.950905\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -19.961396\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -19.971782\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -19.972065\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -19.982344\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -19.982521\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -19.982695\n",
            "resetting env. episode 186.000000, reward total was -19.000000. running mean: -19.972868\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -19.963140\n",
            "resetting env. episode 188.000000, reward total was -19.000000. running mean: -19.953508\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -19.953973\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -19.964433\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -19.974789\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -19.975041\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -19.985291\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -19.995438\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.005484\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.015429\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.015274\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -20.005122\n",
            "resetting env. episode 199.000000, reward total was -19.000000. running mean: -19.995070\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.005120\n",
            "resetting env. episode 201.000000, reward total was -19.000000. running mean: -19.995069\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.005118\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.005067\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.005016\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.014966\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.024816\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.024568\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.024322\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.034079\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.043738\n",
            "resetting env. episode 211.000000, reward total was -18.000000. running mean: -20.023301\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.033068\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.042737\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -20.032310\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.031987\n",
            "resetting env. episode 216.000000, reward total was -19.000000. running mean: -20.021667\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.021450\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.021236\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.031023\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.030713\n",
            "resetting env. episode 221.000000, reward total was -17.000000. running mean: -20.000406\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.000402\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.010398\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -20.000294\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.010291\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.010188\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.020086\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.019885\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.029687\n",
            "resetting env. episode 230.000000, reward total was -18.000000. running mean: -20.009390\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.019296\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -20.009103\n",
            "resetting env. episode 233.000000, reward total was -19.000000. running mean: -19.999012\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -19.999022\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -19.999031\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.009041\n",
            "resetting env. episode 237.000000, reward total was -18.000000. running mean: -19.988951\n",
            "resetting env. episode 238.000000, reward total was -19.000000. running mean: -19.979061\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -19.989271\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -19.989378\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -19.999484\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.009489\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.009394\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.009300\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.009207\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.019115\n",
            "resetting env. episode 247.000000, reward total was -18.000000. running mean: -19.998924\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.008935\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.018846\n",
            "resetting env. episode 250.000000, reward total was -19.000000. running mean: -20.008657\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.008571\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.018485\n",
            "resetting env. episode 253.000000, reward total was -19.000000. running mean: -20.008300\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.008217\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.018135\n",
            "resetting env. episode 256.000000, reward total was -19.000000. running mean: -20.007954\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.017874\n",
            "resetting env. episode 258.000000, reward total was -19.000000. running mean: -20.007695\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.017618\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.017442\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.027268\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.036995\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.036625\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.046259\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.055796\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -20.045238\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.044786\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.054338\n",
            "resetting env. episode 269.000000, reward total was -19.000000. running mean: -20.043795\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -20.033357\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.043023\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.052593\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.062067\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.071446\n",
            "resetting env. episode 275.000000, reward total was -19.000000. running mean: -20.060732\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.070125\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.079423\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.078629\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.077843\n",
            "resetting env. episode 280.000000, reward total was -18.000000. running mean: -20.057064\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.056494\n",
            "resetting env. episode 282.000000, reward total was -17.000000. running mean: -20.025929\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.025669\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.035413\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.035059\n",
            "resetting env. episode 286.000000, reward total was -19.000000. running mean: -20.024708\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.034461\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.044116\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.043675\n",
            "resetting env. episode 290.000000, reward total was -19.000000. running mean: -20.033238\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.042906\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.042477\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.042052\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.051632\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.061115\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.050504\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.059999\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.059399\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.058805\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.058217\n",
            "resetting env. episode 301.000000, reward total was -19.000000. running mean: -20.047635\n",
            "resetting env. episode 302.000000, reward total was -18.000000. running mean: -20.027159\n",
            "resetting env. episode 303.000000, reward total was -17.000000. running mean: -19.996887\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.006918\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.016849\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.016681\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.016514\n",
            "resetting env. episode 308.000000, reward total was -17.000000. running mean: -19.986349\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -19.986485\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -19.996620\n",
            "resetting env. episode 311.000000, reward total was -19.000000. running mean: -19.986654\n",
            "resetting env. episode 312.000000, reward total was -18.000000. running mean: -19.966787\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -19.977120\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -19.987348\n",
            "resetting env. episode 315.000000, reward total was -18.000000. running mean: -19.967475\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -19.967800\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -19.968122\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -19.968441\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -19.968757\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -19.979069\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -19.979278\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -19.979486\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -19.989691\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -19.999794\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.009796\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.019698\n",
            "resetting env. episode 327.000000, reward total was -18.000000. running mean: -19.999501\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -19.999506\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.009511\n",
            "resetting env. episode 330.000000, reward total was -19.000000. running mean: -19.999416\n",
            "resetting env. episode 331.000000, reward total was -18.000000. running mean: -19.979422\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -19.989627\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -19.989731\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -19.999834\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.009835\n",
            "resetting env. episode 336.000000, reward total was -19.000000. running mean: -19.999737\n",
            "resetting env. episode 337.000000, reward total was -17.000000. running mean: -19.969740\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -19.980042\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -19.970242\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -19.970539\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -19.980834\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -19.991026\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -19.991115\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.001204\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.001192\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.001180\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.001169\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.011157\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.021045\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.020835\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.020626\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.030420\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.030116\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.039815\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.049417\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.058923\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.068333\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.067650\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.076973\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.086204\n",
            "resetting env. episode 361.000000, reward total was -19.000000. running mean: -20.075342\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.084588\n",
            "resetting env. episode 363.000000, reward total was -18.000000. running mean: -20.063742\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.073105\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.072374\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.071650\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.080934\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.090124\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.099223\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.108231\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -20.097149\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.106177\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.105115\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.104064\n",
            "resetting env. episode 375.000000, reward total was -19.000000. running mean: -20.093024\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.102093\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.111072\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -20.099962\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -20.088962\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.098072\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.107092\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.116021\n",
            "resetting env. episode 383.000000, reward total was -18.000000. running mean: -20.094861\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.093912\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.092973\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.102043\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.111023\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.119912\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.128713\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.137426\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.146052\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.154591\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.163045\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.161415\n",
            "resetting env. episode 395.000000, reward total was -18.000000. running mean: -20.139801\n",
            "resetting env. episode 396.000000, reward total was -18.000000. running mean: -20.118403\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.127219\n",
            "resetting env. episode 398.000000, reward total was -19.000000. running mean: -20.115947\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.124787\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.123539\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.132304\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.120981\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.119771\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.128573\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -20.117288\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.126115\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.124854\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -20.113605\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -20.102469\n",
            "resetting env. episode 410.000000, reward total was -19.000000. running mean: -20.091444\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.100530\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.099525\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.088529\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.077644\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.076868\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.086099\n",
            "resetting env. episode 417.000000, reward total was -19.000000. running mean: -20.075238\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.074486\n",
            "resetting env. episode 419.000000, reward total was -18.000000. running mean: -20.053741\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.063203\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.062571\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.071946\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.071226\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.080514\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.089709\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.088812\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.097923\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.096944\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.095975\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.095015\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.094065\n",
            "resetting env. episode 432.000000, reward total was -19.000000. running mean: -20.083124\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -20.072293\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.061570\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.070954\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.080245\n",
            "resetting env. episode 437.000000, reward total was -19.000000. running mean: -20.069442\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.068748\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.078060\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.087280\n",
            "resetting env. episode 441.000000, reward total was -19.000000. running mean: -20.076407\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.085643\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.094787\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.103839\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.112800\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.121672\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.120456\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.119251\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.128059\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.136778\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.125410\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.124156\n",
            "resetting env. episode 453.000000, reward total was -18.000000. running mean: -20.102915\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.101885\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.110867\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.099758\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.108760\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.117673\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.116496\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -20.105331\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.114278\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.123135\n",
            "resetting env. episode 463.000000, reward total was -17.000000. running mean: -20.091904\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.090985\n",
            "resetting env. episode 465.000000, reward total was -19.000000. running mean: -20.080075\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.079274\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.078481\n",
            "resetting env. episode 468.000000, reward total was -19.000000. running mean: -20.067696\n",
            "resetting env. episode 469.000000, reward total was -19.000000. running mean: -20.057019\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.056449\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.055885\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.065326\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.054673\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.054126\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.063585\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.072949\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.072219\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.071497\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.080782\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.089974\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.099075\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.098084\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.097103\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -20.086132\n",
            "resetting env. episode 485.000000, reward total was -19.000000. running mean: -20.075271\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.084518\n",
            "resetting env. episode 487.000000, reward total was -18.000000. running mean: -20.063673\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.063036\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.052406\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.051882\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.051363\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.050849\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.060341\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.069737\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.069040\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.068349\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.077666\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.076889\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.086120\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.095259\n",
            "resetting env. episode 501.000000, reward total was -17.000000. running mean: -20.064307\n",
            "resetting env. episode 502.000000, reward total was -18.000000. running mean: -20.043664\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.053227\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.062695\n",
            "resetting env. episode 505.000000, reward total was -19.000000. running mean: -20.052068\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.061547\n",
            "resetting env. episode 507.000000, reward total was -19.000000. running mean: -20.050932\n",
            "resetting env. episode 508.000000, reward total was -19.000000. running mean: -20.040422\n",
            "resetting env. episode 509.000000, reward total was -20.000000. running mean: -20.040018\n",
            "resetting env. episode 510.000000, reward total was -21.000000. running mean: -20.049618\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.059122\n",
            "resetting env. episode 512.000000, reward total was -20.000000. running mean: -20.058530\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.067945\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -20.077266\n",
            "resetting env. episode 515.000000, reward total was -19.000000. running mean: -20.066493\n",
            "resetting env. episode 516.000000, reward total was -19.000000. running mean: -20.055828\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.065270\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.074617\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.083871\n",
            "resetting env. episode 520.000000, reward total was -18.000000. running mean: -20.063032\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.072402\n",
            "resetting env. episode 522.000000, reward total was -18.000000. running mean: -20.051678\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -20.051161\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -20.050650\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.050143\n",
            "resetting env. episode 526.000000, reward total was -19.000000. running mean: -20.039642\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.049245\n",
            "resetting env. episode 528.000000, reward total was -20.000000. running mean: -20.048753\n",
            "resetting env. episode 529.000000, reward total was -20.000000. running mean: -20.048265\n",
            "resetting env. episode 530.000000, reward total was -19.000000. running mean: -20.037783\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.047405\n",
            "resetting env. episode 532.000000, reward total was -20.000000. running mean: -20.046931\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.056461\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.065897\n",
            "resetting env. episode 535.000000, reward total was -19.000000. running mean: -20.055238\n",
            "resetting env. episode 536.000000, reward total was -19.000000. running mean: -20.044685\n",
            "resetting env. episode 537.000000, reward total was -19.000000. running mean: -20.034239\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.043896\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -20.053457\n",
            "resetting env. episode 540.000000, reward total was -20.000000. running mean: -20.052923\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.062393\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.071769\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.081052\n",
            "resetting env. episode 544.000000, reward total was -19.000000. running mean: -20.070241\n",
            "resetting env. episode 545.000000, reward total was -18.000000. running mean: -20.049539\n",
            "resetting env. episode 546.000000, reward total was -18.000000. running mean: -20.029043\n",
            "resetting env. episode 547.000000, reward total was -20.000000. running mean: -20.028753\n",
            "resetting env. episode 548.000000, reward total was -17.000000. running mean: -19.998466\n",
            "resetting env. episode 549.000000, reward total was -20.000000. running mean: -19.998481\n",
            "resetting env. episode 550.000000, reward total was -19.000000. running mean: -19.988496\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -19.998611\n",
            "resetting env. episode 552.000000, reward total was -19.000000. running mean: -19.988625\n",
            "resetting env. episode 553.000000, reward total was -20.000000. running mean: -19.988739\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -19.998851\n",
            "resetting env. episode 555.000000, reward total was -19.000000. running mean: -19.988863\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -19.988974\n",
            "resetting env. episode 557.000000, reward total was -18.000000. running mean: -19.969084\n",
            "resetting env. episode 558.000000, reward total was -17.000000. running mean: -19.939394\n",
            "resetting env. episode 559.000000, reward total was -18.000000. running mean: -19.920000\n",
            "resetting env. episode 560.000000, reward total was -18.000000. running mean: -19.900800\n",
            "resetting env. episode 561.000000, reward total was -18.000000. running mean: -19.881792\n",
            "resetting env. episode 562.000000, reward total was -20.000000. running mean: -19.882974\n",
            "resetting env. episode 563.000000, reward total was -20.000000. running mean: -19.884144\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -19.895303\n",
            "resetting env. episode 565.000000, reward total was -17.000000. running mean: -19.866350\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -19.877686\n",
            "resetting env. episode 567.000000, reward total was -20.000000. running mean: -19.878909\n",
            "resetting env. episode 568.000000, reward total was -19.000000. running mean: -19.870120\n",
            "resetting env. episode 569.000000, reward total was -19.000000. running mean: -19.861419\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -19.872805\n",
            "resetting env. episode 571.000000, reward total was -19.000000. running mean: -19.864077\n",
            "resetting env. episode 572.000000, reward total was -19.000000. running mean: -19.855436\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -19.856882\n",
            "resetting env. episode 574.000000, reward total was -19.000000. running mean: -19.848313\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -19.859830\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -19.871231\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -19.872519\n",
            "resetting env. episode 578.000000, reward total was -20.000000. running mean: -19.873794\n",
            "resetting env. episode 579.000000, reward total was -20.000000. running mean: -19.875056\n",
            "resetting env. episode 580.000000, reward total was -19.000000. running mean: -19.866305\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -19.877642\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -19.888866\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -19.899977\n",
            "resetting env. episode 584.000000, reward total was -19.000000. running mean: -19.890977\n",
            "resetting env. episode 585.000000, reward total was -20.000000. running mean: -19.892068\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -19.903147\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -19.914115\n",
            "resetting env. episode 588.000000, reward total was -19.000000. running mean: -19.904974\n",
            "resetting env. episode 589.000000, reward total was -21.000000. running mean: -19.915925\n",
            "resetting env. episode 590.000000, reward total was -20.000000. running mean: -19.916765\n",
            "resetting env. episode 591.000000, reward total was -20.000000. running mean: -19.917598\n",
            "resetting env. episode 592.000000, reward total was -20.000000. running mean: -19.918422\n",
            "resetting env. episode 593.000000, reward total was -20.000000. running mean: -19.919237\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -19.930045\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -19.940745\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -19.941337\n",
            "resetting env. episode 597.000000, reward total was -19.000000. running mean: -19.931924\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -19.942605\n",
            "resetting env. episode 599.000000, reward total was -18.000000. running mean: -19.923179\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -19.933947\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -19.944607\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -19.955161\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -19.965610\n",
            "resetting env. episode 604.000000, reward total was -20.000000. running mean: -19.965954\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -19.976294\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -19.976531\n",
            "resetting env. episode 607.000000, reward total was -16.000000. running mean: -19.936766\n",
            "resetting env. episode 608.000000, reward total was -19.000000. running mean: -19.927398\n",
            "resetting env. episode 609.000000, reward total was -21.000000. running mean: -19.938124\n",
            "resetting env. episode 610.000000, reward total was -18.000000. running mean: -19.918743\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -19.929555\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -19.940260\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -19.950857\n",
            "resetting env. episode 614.000000, reward total was -19.000000. running mean: -19.941349\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -19.951935\n",
            "resetting env. episode 616.000000, reward total was -19.000000. running mean: -19.942416\n",
            "resetting env. episode 617.000000, reward total was -20.000000. running mean: -19.942992\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -19.953562\n",
            "resetting env. episode 619.000000, reward total was -21.000000. running mean: -19.964026\n",
            "resetting env. episode 620.000000, reward total was -18.000000. running mean: -19.944386\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -19.944942\n",
            "resetting env. episode 622.000000, reward total was -20.000000. running mean: -19.945493\n",
            "resetting env. episode 623.000000, reward total was -19.000000. running mean: -19.936038\n",
            "resetting env. episode 624.000000, reward total was -20.000000. running mean: -19.936677\n",
            "resetting env. episode 625.000000, reward total was -20.000000. running mean: -19.937311\n",
            "resetting env. episode 626.000000, reward total was -19.000000. running mean: -19.927937\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -19.938658\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -19.949271\n",
            "resetting env. episode 629.000000, reward total was -20.000000. running mean: -19.949779\n",
            "resetting env. episode 630.000000, reward total was -19.000000. running mean: -19.940281\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -19.950878\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -19.961369\n",
            "resetting env. episode 633.000000, reward total was -20.000000. running mean: -19.961756\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -19.972138\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -19.982417\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -19.992593\n",
            "resetting env. episode 637.000000, reward total was -20.000000. running mean: -19.992667\n",
            "resetting env. episode 638.000000, reward total was -20.000000. running mean: -19.992740\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.002813\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -20.002784\n",
            "resetting env. episode 641.000000, reward total was -19.000000. running mean: -19.992757\n",
            "resetting env. episode 642.000000, reward total was -18.000000. running mean: -19.972829\n",
            "resetting env. episode 643.000000, reward total was -21.000000. running mean: -19.983101\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -19.993270\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.003337\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -20.013304\n",
            "resetting env. episode 647.000000, reward total was -16.000000. running mean: -19.973171\n",
            "resetting env. episode 648.000000, reward total was -20.000000. running mean: -19.973439\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -19.983705\n",
            "resetting env. episode 650.000000, reward total was -20.000000. running mean: -19.983868\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -19.994029\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.004089\n",
            "resetting env. episode 653.000000, reward total was -20.000000. running mean: -20.004048\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -20.014007\n",
            "resetting env. episode 655.000000, reward total was -20.000000. running mean: -20.013867\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -20.013728\n",
            "resetting env. episode 657.000000, reward total was -20.000000. running mean: -20.013591\n",
            "resetting env. episode 658.000000, reward total was -20.000000. running mean: -20.013455\n",
            "resetting env. episode 659.000000, reward total was -20.000000. running mean: -20.013321\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.023187\n",
            "resetting env. episode 661.000000, reward total was -20.000000. running mean: -20.022956\n",
            "resetting env. episode 662.000000, reward total was -21.000000. running mean: -20.032726\n",
            "resetting env. episode 663.000000, reward total was -19.000000. running mean: -20.022399\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.032175\n",
            "resetting env. episode 665.000000, reward total was -19.000000. running mean: -20.021853\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.031635\n",
            "resetting env. episode 667.000000, reward total was -20.000000. running mean: -20.031318\n",
            "resetting env. episode 668.000000, reward total was -19.000000. running mean: -20.021005\n",
            "resetting env. episode 669.000000, reward total was -19.000000. running mean: -20.010795\n",
            "resetting env. episode 670.000000, reward total was -19.000000. running mean: -20.000687\n",
            "resetting env. episode 671.000000, reward total was -19.000000. running mean: -19.990680\n",
            "resetting env. episode 672.000000, reward total was -18.000000. running mean: -19.970773\n",
            "resetting env. episode 673.000000, reward total was -19.000000. running mean: -19.961066\n",
            "resetting env. episode 674.000000, reward total was -19.000000. running mean: -19.951455\n",
            "resetting env. episode 675.000000, reward total was -19.000000. running mean: -19.941940\n",
            "resetting env. episode 676.000000, reward total was -18.000000. running mean: -19.922521\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -19.923296\n",
            "resetting env. episode 678.000000, reward total was -19.000000. running mean: -19.914063\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -19.914922\n",
            "resetting env. episode 680.000000, reward total was -20.000000. running mean: -19.915773\n",
            "resetting env. episode 681.000000, reward total was -19.000000. running mean: -19.906615\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -19.907549\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -19.908474\n",
            "resetting env. episode 684.000000, reward total was -19.000000. running mean: -19.899389\n",
            "resetting env. episode 685.000000, reward total was -19.000000. running mean: -19.890395\n",
            "resetting env. episode 686.000000, reward total was -19.000000. running mean: -19.881491\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -19.882676\n",
            "resetting env. episode 688.000000, reward total was -18.000000. running mean: -19.863849\n",
            "resetting env. episode 689.000000, reward total was -19.000000. running mean: -19.855211\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -19.856659\n",
            "resetting env. episode 691.000000, reward total was -20.000000. running mean: -19.858092\n",
            "resetting env. episode 692.000000, reward total was -20.000000. running mean: -19.859511\n",
            "resetting env. episode 693.000000, reward total was -20.000000. running mean: -19.860916\n",
            "resetting env. episode 694.000000, reward total was -20.000000. running mean: -19.862307\n",
            "resetting env. episode 695.000000, reward total was -18.000000. running mean: -19.843684\n",
            "resetting env. episode 696.000000, reward total was -19.000000. running mean: -19.835247\n",
            "resetting env. episode 697.000000, reward total was -20.000000. running mean: -19.836895\n",
            "resetting env. episode 698.000000, reward total was -20.000000. running mean: -19.838526\n",
            "resetting env. episode 699.000000, reward total was -20.000000. running mean: -19.840140\n",
            "resetting env. episode 700.000000, reward total was -21.000000. running mean: -19.851739\n",
            "resetting env. episode 701.000000, reward total was -20.000000. running mean: -19.853222\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -19.864689\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -19.876042\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -19.877282\n",
            "resetting env. episode 705.000000, reward total was -17.000000. running mean: -19.848509\n",
            "resetting env. episode 706.000000, reward total was -20.000000. running mean: -19.850024\n",
            "resetting env. episode 707.000000, reward total was -20.000000. running mean: -19.851524\n",
            "resetting env. episode 708.000000, reward total was -19.000000. running mean: -19.843009\n",
            "resetting env. episode 709.000000, reward total was -18.000000. running mean: -19.824579\n",
            "resetting env. episode 710.000000, reward total was -20.000000. running mean: -19.826333\n",
            "resetting env. episode 711.000000, reward total was -20.000000. running mean: -19.828069\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -19.829789\n",
            "resetting env. episode 713.000000, reward total was -20.000000. running mean: -19.831491\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -19.843176\n",
            "resetting env. episode 715.000000, reward total was -20.000000. running mean: -19.844744\n",
            "resetting env. episode 716.000000, reward total was -19.000000. running mean: -19.836297\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -19.847934\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -19.849454\n",
            "resetting env. episode 719.000000, reward total was -20.000000. running mean: -19.850960\n",
            "resetting env. episode 720.000000, reward total was -20.000000. running mean: -19.852450\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -19.863926\n",
            "resetting env. episode 722.000000, reward total was -20.000000. running mean: -19.865287\n",
            "resetting env. episode 723.000000, reward total was -20.000000. running mean: -19.866634\n",
            "resetting env. episode 724.000000, reward total was -20.000000. running mean: -19.867967\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -19.879288\n",
            "resetting env. episode 726.000000, reward total was -19.000000. running mean: -19.870495\n",
            "resetting env. episode 727.000000, reward total was -19.000000. running mean: -19.861790\n",
            "resetting env. episode 728.000000, reward total was -19.000000. running mean: -19.853172\n",
            "resetting env. episode 729.000000, reward total was -17.000000. running mean: -19.824640\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -19.836394\n",
            "resetting env. episode 731.000000, reward total was -17.000000. running mean: -19.808030\n",
            "resetting env. episode 732.000000, reward total was -20.000000. running mean: -19.809950\n",
            "resetting env. episode 733.000000, reward total was -18.000000. running mean: -19.791850\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -19.793932\n",
            "resetting env. episode 735.000000, reward total was -20.000000. running mean: -19.795992\n",
            "resetting env. episode 736.000000, reward total was -21.000000. running mean: -19.808032\n",
            "resetting env. episode 737.000000, reward total was -20.000000. running mean: -19.809952\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -19.821853\n",
            "resetting env. episode 739.000000, reward total was -21.000000. running mean: -19.833634\n",
            "resetting env. episode 740.000000, reward total was -19.000000. running mean: -19.825298\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -19.837045\n",
            "resetting env. episode 742.000000, reward total was -19.000000. running mean: -19.828674\n",
            "resetting env. episode 743.000000, reward total was -20.000000. running mean: -19.830387\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -19.842084\n",
            "resetting env. episode 745.000000, reward total was -18.000000. running mean: -19.823663\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -19.825426\n",
            "resetting env. episode 747.000000, reward total was -17.000000. running mean: -19.797172\n",
            "resetting env. episode 748.000000, reward total was -18.000000. running mean: -19.779200\n",
            "resetting env. episode 749.000000, reward total was -19.000000. running mean: -19.771408\n",
            "resetting env. episode 750.000000, reward total was -20.000000. running mean: -19.773694\n",
            "resetting env. episode 751.000000, reward total was -18.000000. running mean: -19.755957\n",
            "resetting env. episode 752.000000, reward total was -20.000000. running mean: -19.758398\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -19.770814\n",
            "resetting env. episode 754.000000, reward total was -19.000000. running mean: -19.763105\n",
            "resetting env. episode 755.000000, reward total was -21.000000. running mean: -19.775474\n",
            "resetting env. episode 756.000000, reward total was -18.000000. running mean: -19.757720\n",
            "resetting env. episode 757.000000, reward total was -20.000000. running mean: -19.760142\n",
            "resetting env. episode 758.000000, reward total was -18.000000. running mean: -19.742541\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -19.755116\n",
            "resetting env. episode 760.000000, reward total was -19.000000. running mean: -19.747564\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -19.760089\n",
            "resetting env. episode 762.000000, reward total was -20.000000. running mean: -19.762488\n",
            "resetting env. episode 763.000000, reward total was -17.000000. running mean: -19.734863\n",
            "resetting env. episode 764.000000, reward total was -18.000000. running mean: -19.717514\n",
            "resetting env. episode 765.000000, reward total was -19.000000. running mean: -19.710339\n",
            "resetting env. episode 766.000000, reward total was -20.000000. running mean: -19.713236\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -19.726104\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -19.738843\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -19.751454\n",
            "resetting env. episode 770.000000, reward total was -18.000000. running mean: -19.733940\n",
            "resetting env. episode 771.000000, reward total was -20.000000. running mean: -19.736600\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -19.749234\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -19.761742\n",
            "resetting env. episode 774.000000, reward total was -19.000000. running mean: -19.754124\n",
            "resetting env. episode 775.000000, reward total was -19.000000. running mean: -19.746583\n",
            "resetting env. episode 776.000000, reward total was -21.000000. running mean: -19.759117\n",
            "resetting env. episode 777.000000, reward total was -19.000000. running mean: -19.751526\n",
            "resetting env. episode 778.000000, reward total was -21.000000. running mean: -19.764011\n",
            "resetting env. episode 779.000000, reward total was -20.000000. running mean: -19.766371\n",
            "resetting env. episode 780.000000, reward total was -20.000000. running mean: -19.768707\n",
            "resetting env. episode 781.000000, reward total was -20.000000. running mean: -19.771020\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -19.783310\n",
            "resetting env. episode 783.000000, reward total was -20.000000. running mean: -19.785477\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -19.797622\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -19.809646\n",
            "resetting env. episode 786.000000, reward total was -17.000000. running mean: -19.781549\n",
            "resetting env. episode 787.000000, reward total was -20.000000. running mean: -19.783734\n",
            "resetting env. episode 788.000000, reward total was -20.000000. running mean: -19.785896\n",
            "resetting env. episode 789.000000, reward total was -20.000000. running mean: -19.788037\n",
            "resetting env. episode 790.000000, reward total was -19.000000. running mean: -19.780157\n",
            "resetting env. episode 791.000000, reward total was -19.000000. running mean: -19.772356\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -19.784632\n",
            "resetting env. episode 793.000000, reward total was -17.000000. running mean: -19.756786\n",
            "resetting env. episode 794.000000, reward total was -20.000000. running mean: -19.759218\n",
            "resetting env. episode 795.000000, reward total was -20.000000. running mean: -19.761626\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -19.774009\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -19.786269\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -19.798407\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -19.810422\n",
            "resetting env. episode 800.000000, reward total was -20.000000. running mean: -19.812318\n",
            "resetting env. episode 801.000000, reward total was -20.000000. running mean: -19.814195\n",
            "resetting env. episode 802.000000, reward total was -19.000000. running mean: -19.806053\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -19.817993\n",
            "resetting env. episode 804.000000, reward total was -20.000000. running mean: -19.819813\n",
            "resetting env. episode 805.000000, reward total was -18.000000. running mean: -19.801615\n",
            "resetting env. episode 806.000000, reward total was -20.000000. running mean: -19.803598\n",
            "resetting env. episode 807.000000, reward total was -16.000000. running mean: -19.765562\n",
            "resetting env. episode 808.000000, reward total was -19.000000. running mean: -19.757907\n",
            "resetting env. episode 809.000000, reward total was -20.000000. running mean: -19.760328\n",
            "resetting env. episode 810.000000, reward total was -20.000000. running mean: -19.762724\n",
            "resetting env. episode 811.000000, reward total was -17.000000. running mean: -19.735097\n",
            "resetting env. episode 812.000000, reward total was -19.000000. running mean: -19.727746\n",
            "resetting env. episode 813.000000, reward total was -19.000000. running mean: -19.720469\n",
            "resetting env. episode 814.000000, reward total was -20.000000. running mean: -19.723264\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -19.736031\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -19.748671\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -19.761184\n",
            "resetting env. episode 818.000000, reward total was -19.000000. running mean: -19.753573\n",
            "resetting env. episode 819.000000, reward total was -20.000000. running mean: -19.756037\n",
            "resetting env. episode 820.000000, reward total was -21.000000. running mean: -19.768476\n",
            "resetting env. episode 821.000000, reward total was -19.000000. running mean: -19.760792\n",
            "resetting env. episode 822.000000, reward total was -17.000000. running mean: -19.733184\n",
            "resetting env. episode 823.000000, reward total was -18.000000. running mean: -19.715852\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -19.728693\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -19.731407\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -19.744092\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -19.746652\n",
            "resetting env. episode 828.000000, reward total was -20.000000. running mean: -19.749185\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -19.761693\n",
            "resetting env. episode 830.000000, reward total was -20.000000. running mean: -19.764076\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -19.776435\n",
            "resetting env. episode 832.000000, reward total was -19.000000. running mean: -19.768671\n",
            "resetting env. episode 833.000000, reward total was -20.000000. running mean: -19.770984\n",
            "resetting env. episode 834.000000, reward total was -19.000000. running mean: -19.763275\n",
            "resetting env. episode 835.000000, reward total was -20.000000. running mean: -19.765642\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -19.767985\n",
            "resetting env. episode 837.000000, reward total was -21.000000. running mean: -19.780306\n",
            "resetting env. episode 838.000000, reward total was -19.000000. running mean: -19.772502\n",
            "resetting env. episode 839.000000, reward total was -20.000000. running mean: -19.774777\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -19.777030\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -19.789259\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -19.801367\n",
            "resetting env. episode 843.000000, reward total was -20.000000. running mean: -19.803353\n",
            "resetting env. episode 844.000000, reward total was -17.000000. running mean: -19.775320\n",
            "resetting env. episode 845.000000, reward total was -18.000000. running mean: -19.757566\n",
            "resetting env. episode 846.000000, reward total was -20.000000. running mean: -19.759991\n",
            "resetting env. episode 847.000000, reward total was -21.000000. running mean: -19.772391\n",
            "resetting env. episode 848.000000, reward total was -20.000000. running mean: -19.774667\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -19.786920\n",
            "resetting env. episode 850.000000, reward total was -19.000000. running mean: -19.779051\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -19.791261\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -19.803348\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -19.815314\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -19.827161\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -19.838890\n",
            "resetting env. episode 856.000000, reward total was -20.000000. running mean: -19.840501\n",
            "resetting env. episode 857.000000, reward total was -20.000000. running mean: -19.842096\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -19.853675\n",
            "resetting env. episode 859.000000, reward total was -18.000000. running mean: -19.835138\n",
            "resetting env. episode 860.000000, reward total was -19.000000. running mean: -19.826787\n",
            "resetting env. episode 861.000000, reward total was -20.000000. running mean: -19.828519\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -19.840234\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -19.851831\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -19.863313\n",
            "resetting env. episode 865.000000, reward total was -18.000000. running mean: -19.844680\n",
            "resetting env. episode 866.000000, reward total was -19.000000. running mean: -19.836233\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -19.847871\n",
            "resetting env. episode 868.000000, reward total was -20.000000. running mean: -19.849392\n",
            "resetting env. episode 869.000000, reward total was -19.000000. running mean: -19.840898\n",
            "resetting env. episode 870.000000, reward total was -20.000000. running mean: -19.842489\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -19.854064\n",
            "resetting env. episode 872.000000, reward total was -20.000000. running mean: -19.855524\n",
            "resetting env. episode 873.000000, reward total was -19.000000. running mean: -19.846968\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -19.858499\n",
            "resetting env. episode 875.000000, reward total was -18.000000. running mean: -19.839914\n",
            "resetting env. episode 876.000000, reward total was -19.000000. running mean: -19.831515\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -19.843199\n",
            "resetting env. episode 878.000000, reward total was -20.000000. running mean: -19.844767\n",
            "resetting env. episode 879.000000, reward total was -19.000000. running mean: -19.836320\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -19.847957\n",
            "resetting env. episode 881.000000, reward total was -18.000000. running mean: -19.829477\n",
            "resetting env. episode 882.000000, reward total was -21.000000. running mean: -19.841182\n",
            "resetting env. episode 883.000000, reward total was -19.000000. running mean: -19.832770\n",
            "resetting env. episode 884.000000, reward total was -16.000000. running mean: -19.794443\n",
            "resetting env. episode 885.000000, reward total was -17.000000. running mean: -19.766498\n",
            "resetting env. episode 886.000000, reward total was -19.000000. running mean: -19.758833\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -19.771245\n",
            "resetting env. episode 888.000000, reward total was -20.000000. running mean: -19.773532\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -19.785797\n",
            "resetting env. episode 890.000000, reward total was -20.000000. running mean: -19.787939\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -19.800060\n",
            "resetting env. episode 892.000000, reward total was -20.000000. running mean: -19.802059\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -19.814039\n",
            "resetting env. episode 894.000000, reward total was -20.000000. running mean: -19.815898\n",
            "resetting env. episode 895.000000, reward total was -20.000000. running mean: -19.817739\n",
            "resetting env. episode 896.000000, reward total was -20.000000. running mean: -19.819562\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -19.831366\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -19.843053\n",
            "resetting env. episode 899.000000, reward total was -20.000000. running mean: -19.844622\n",
            "resetting env. episode 900.000000, reward total was -20.000000. running mean: -19.846176\n",
            "resetting env. episode 901.000000, reward total was -20.000000. running mean: -19.847714\n",
            "resetting env. episode 902.000000, reward total was -20.000000. running mean: -19.849237\n",
            "resetting env. episode 903.000000, reward total was -18.000000. running mean: -19.830745\n",
            "resetting env. episode 904.000000, reward total was -19.000000. running mean: -19.822437\n",
            "resetting env. episode 905.000000, reward total was -20.000000. running mean: -19.824213\n",
            "resetting env. episode 906.000000, reward total was -19.000000. running mean: -19.815971\n",
            "resetting env. episode 907.000000, reward total was -20.000000. running mean: -19.817811\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -19.829633\n",
            "resetting env. episode 909.000000, reward total was -21.000000. running mean: -19.841336\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -19.852923\n",
            "resetting env. episode 911.000000, reward total was -20.000000. running mean: -19.854394\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -19.865850\n",
            "resetting env. episode 913.000000, reward total was -19.000000. running mean: -19.857191\n",
            "resetting env. episode 914.000000, reward total was -19.000000. running mean: -19.848620\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -19.850133\n",
            "resetting env. episode 916.000000, reward total was -19.000000. running mean: -19.841632\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -19.853216\n",
            "resetting env. episode 918.000000, reward total was -18.000000. running mean: -19.834684\n",
            "resetting env. episode 919.000000, reward total was -21.000000. running mean: -19.846337\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -19.857873\n",
            "resetting env. episode 921.000000, reward total was -20.000000. running mean: -19.859295\n",
            "resetting env. episode 922.000000, reward total was -20.000000. running mean: -19.860702\n",
            "resetting env. episode 923.000000, reward total was -20.000000. running mean: -19.862095\n",
            "resetting env. episode 924.000000, reward total was -20.000000. running mean: -19.863474\n",
            "resetting env. episode 925.000000, reward total was -18.000000. running mean: -19.844839\n",
            "resetting env. episode 926.000000, reward total was -20.000000. running mean: -19.846391\n",
            "resetting env. episode 927.000000, reward total was -18.000000. running mean: -19.827927\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -19.839647\n",
            "resetting env. episode 929.000000, reward total was -19.000000. running mean: -19.831251\n",
            "resetting env. episode 930.000000, reward total was -19.000000. running mean: -19.822938\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -19.834709\n",
            "resetting env. episode 932.000000, reward total was -19.000000. running mean: -19.826362\n",
            "resetting env. episode 933.000000, reward total was -18.000000. running mean: -19.808098\n",
            "resetting env. episode 934.000000, reward total was -19.000000. running mean: -19.800017\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -19.812017\n",
            "resetting env. episode 936.000000, reward total was -20.000000. running mean: -19.813897\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -19.815758\n",
            "resetting env. episode 938.000000, reward total was -20.000000. running mean: -19.817600\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -19.819424\n",
            "resetting env. episode 940.000000, reward total was -20.000000. running mean: -19.821230\n",
            "resetting env. episode 941.000000, reward total was -19.000000. running mean: -19.813018\n",
            "resetting env. episode 942.000000, reward total was -20.000000. running mean: -19.814888\n",
            "resetting env. episode 943.000000, reward total was -20.000000. running mean: -19.816739\n",
            "resetting env. episode 944.000000, reward total was -18.000000. running mean: -19.798571\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -19.810586\n",
            "resetting env. episode 946.000000, reward total was -19.000000. running mean: -19.802480\n",
            "resetting env. episode 947.000000, reward total was -19.000000. running mean: -19.794455\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -19.806511\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -19.818445\n",
            "resetting env. episode 950.000000, reward total was -19.000000. running mean: -19.810261\n",
            "resetting env. episode 951.000000, reward total was -20.000000. running mean: -19.812158\n",
            "resetting env. episode 952.000000, reward total was -20.000000. running mean: -19.814037\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -19.825896\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -19.837637\n",
            "resetting env. episode 955.000000, reward total was -18.000000. running mean: -19.819261\n",
            "resetting env. episode 956.000000, reward total was -19.000000. running mean: -19.811068\n",
            "resetting env. episode 957.000000, reward total was -17.000000. running mean: -19.782958\n",
            "resetting env. episode 958.000000, reward total was -19.000000. running mean: -19.775128\n",
            "resetting env. episode 959.000000, reward total was -20.000000. running mean: -19.777377\n",
            "resetting env. episode 960.000000, reward total was -19.000000. running mean: -19.769603\n",
            "resetting env. episode 961.000000, reward total was -19.000000. running mean: -19.761907\n",
            "resetting env. episode 962.000000, reward total was -20.000000. running mean: -19.764288\n",
            "resetting env. episode 963.000000, reward total was -20.000000. running mean: -19.766645\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -19.778979\n",
            "resetting env. episode 965.000000, reward total was -20.000000. running mean: -19.781189\n",
            "resetting env. episode 966.000000, reward total was -19.000000. running mean: -19.773377\n",
            "resetting env. episode 967.000000, reward total was -21.000000. running mean: -19.785643\n",
            "resetting env. episode 968.000000, reward total was -19.000000. running mean: -19.777787\n",
            "resetting env. episode 969.000000, reward total was -20.000000. running mean: -19.780009\n",
            "resetting env. episode 970.000000, reward total was -19.000000. running mean: -19.772209\n",
            "resetting env. episode 971.000000, reward total was -19.000000. running mean: -19.764487\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -19.776842\n",
            "resetting env. episode 973.000000, reward total was -19.000000. running mean: -19.769073\n",
            "resetting env. episode 974.000000, reward total was -16.000000. running mean: -19.731383\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -19.744069\n",
            "resetting env. episode 976.000000, reward total was -20.000000. running mean: -19.746628\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -19.759162\n",
            "resetting env. episode 978.000000, reward total was -20.000000. running mean: -19.761570\n",
            "resetting env. episode 979.000000, reward total was -19.000000. running mean: -19.753955\n",
            "resetting env. episode 980.000000, reward total was -20.000000. running mean: -19.756415\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -19.768851\n",
            "resetting env. episode 982.000000, reward total was -20.000000. running mean: -19.771162\n",
            "resetting env. episode 983.000000, reward total was -17.000000. running mean: -19.743451\n",
            "resetting env. episode 984.000000, reward total was -18.000000. running mean: -19.726016\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -19.738756\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -19.751369\n",
            "resetting env. episode 987.000000, reward total was -20.000000. running mean: -19.753855\n",
            "resetting env. episode 988.000000, reward total was -20.000000. running mean: -19.756316\n",
            "resetting env. episode 989.000000, reward total was -19.000000. running mean: -19.748753\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -19.761266\n",
            "resetting env. episode 991.000000, reward total was -21.000000. running mean: -19.773653\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -19.785916\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -19.798057\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -19.810077\n",
            "resetting env. episode 995.000000, reward total was -19.000000. running mean: -19.801976\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -19.813956\n",
            "resetting env. episode 997.000000, reward total was -17.000000. running mean: -19.785817\n",
            "resetting env. episode 998.000000, reward total was -21.000000. running mean: -19.797958\n",
            "resetting env. episode 999.000000, reward total was -20.000000. running mean: -19.799979\n",
            "resetting env. episode 1000.000000, reward total was -20.000000. running mean: -19.801979\n",
            "resetting env. episode 1001.000000, reward total was -20.000000. running mean: -19.803959\n",
            "resetting env. episode 1002.000000, reward total was -20.000000. running mean: -19.805920\n",
            "resetting env. episode 1003.000000, reward total was -16.000000. running mean: -19.767860\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -19.780182\n",
            "resetting env. episode 1005.000000, reward total was -19.000000. running mean: -19.772380\n",
            "resetting env. episode 1006.000000, reward total was -20.000000. running mean: -19.774656\n",
            "resetting env. episode 1007.000000, reward total was -19.000000. running mean: -19.766910\n",
            "resetting env. episode 1008.000000, reward total was -20.000000. running mean: -19.769241\n",
            "resetting env. episode 1009.000000, reward total was -17.000000. running mean: -19.741548\n",
            "resetting env. episode 1010.000000, reward total was -20.000000. running mean: -19.744133\n",
            "resetting env. episode 1011.000000, reward total was -19.000000. running mean: -19.736691\n",
            "resetting env. episode 1012.000000, reward total was -20.000000. running mean: -19.739324\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -19.751931\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -19.764412\n",
            "resetting env. episode 1015.000000, reward total was -20.000000. running mean: -19.766768\n",
            "resetting env. episode 1016.000000, reward total was -19.000000. running mean: -19.759100\n",
            "resetting env. episode 1017.000000, reward total was -20.000000. running mean: -19.761509\n",
            "resetting env. episode 1018.000000, reward total was -19.000000. running mean: -19.753894\n",
            "resetting env. episode 1019.000000, reward total was -19.000000. running mean: -19.746355\n",
            "resetting env. episode 1020.000000, reward total was -20.000000. running mean: -19.748892\n",
            "resetting env. episode 1021.000000, reward total was -18.000000. running mean: -19.731403\n",
            "resetting env. episode 1022.000000, reward total was -19.000000. running mean: -19.724089\n",
            "resetting env. episode 1023.000000, reward total was -20.000000. running mean: -19.726848\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -19.739579\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -19.752183\n",
            "resetting env. episode 1026.000000, reward total was -18.000000. running mean: -19.734662\n",
            "resetting env. episode 1027.000000, reward total was -20.000000. running mean: -19.737315\n",
            "resetting env. episode 1028.000000, reward total was -19.000000. running mean: -19.729942\n",
            "resetting env. episode 1029.000000, reward total was -19.000000. running mean: -19.722642\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -19.735416\n",
            "resetting env. episode 1031.000000, reward total was -19.000000. running mean: -19.728062\n",
            "resetting env. episode 1032.000000, reward total was -20.000000. running mean: -19.730781\n",
            "resetting env. episode 1033.000000, reward total was -21.000000. running mean: -19.743473\n",
            "resetting env. episode 1034.000000, reward total was -19.000000. running mean: -19.736039\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -19.748678\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -19.761192\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -19.773580\n",
            "resetting env. episode 1038.000000, reward total was -19.000000. running mean: -19.765844\n",
            "resetting env. episode 1039.000000, reward total was -20.000000. running mean: -19.768185\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -19.780504\n",
            "resetting env. episode 1041.000000, reward total was -18.000000. running mean: -19.762698\n",
            "resetting env. episode 1042.000000, reward total was -18.000000. running mean: -19.745071\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -19.757621\n",
            "resetting env. episode 1044.000000, reward total was -20.000000. running mean: -19.760045\n",
            "resetting env. episode 1045.000000, reward total was -18.000000. running mean: -19.742444\n",
            "resetting env. episode 1046.000000, reward total was -19.000000. running mean: -19.735020\n",
            "resetting env. episode 1047.000000, reward total was -20.000000. running mean: -19.737669\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -19.750293\n",
            "resetting env. episode 1049.000000, reward total was -18.000000. running mean: -19.732790\n",
            "resetting env. episode 1050.000000, reward total was -18.000000. running mean: -19.715462\n",
            "resetting env. episode 1051.000000, reward total was -20.000000. running mean: -19.718307\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -19.721124\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -19.733913\n",
            "resetting env. episode 1054.000000, reward total was -19.000000. running mean: -19.726574\n",
            "resetting env. episode 1055.000000, reward total was -20.000000. running mean: -19.729308\n",
            "resetting env. episode 1056.000000, reward total was -20.000000. running mean: -19.732015\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -19.744695\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -19.757248\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -19.769675\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -19.781979\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -19.794159\n",
            "resetting env. episode 1062.000000, reward total was -19.000000. running mean: -19.786217\n",
            "resetting env. episode 1063.000000, reward total was -21.000000. running mean: -19.798355\n",
            "resetting env. episode 1064.000000, reward total was -17.000000. running mean: -19.770372\n",
            "resetting env. episode 1065.000000, reward total was -19.000000. running mean: -19.762668\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -19.765041\n",
            "resetting env. episode 1067.000000, reward total was -20.000000. running mean: -19.767391\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -19.769717\n",
            "resetting env. episode 1069.000000, reward total was -19.000000. running mean: -19.762020\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -19.774400\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -19.786656\n",
            "resetting env. episode 1072.000000, reward total was -21.000000. running mean: -19.798789\n",
            "resetting env. episode 1073.000000, reward total was -18.000000. running mean: -19.780801\n",
            "resetting env. episode 1074.000000, reward total was -20.000000. running mean: -19.782993\n",
            "resetting env. episode 1075.000000, reward total was -19.000000. running mean: -19.775163\n",
            "resetting env. episode 1076.000000, reward total was -18.000000. running mean: -19.757412\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -19.769837\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -19.782139\n",
            "resetting env. episode 1079.000000, reward total was -19.000000. running mean: -19.774318\n",
            "resetting env. episode 1080.000000, reward total was -20.000000. running mean: -19.776574\n",
            "resetting env. episode 1081.000000, reward total was -19.000000. running mean: -19.768809\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -19.781121\n",
            "resetting env. episode 1083.000000, reward total was -20.000000. running mean: -19.783309\n",
            "resetting env. episode 1084.000000, reward total was -19.000000. running mean: -19.775476\n",
            "resetting env. episode 1085.000000, reward total was -19.000000. running mean: -19.767722\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -19.780044\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -19.792244\n",
            "resetting env. episode 1088.000000, reward total was -20.000000. running mean: -19.794321\n",
            "resetting env. episode 1089.000000, reward total was -19.000000. running mean: -19.786378\n",
            "resetting env. episode 1090.000000, reward total was -20.000000. running mean: -19.788514\n",
            "resetting env. episode 1091.000000, reward total was -19.000000. running mean: -19.780629\n",
            "resetting env. episode 1092.000000, reward total was -19.000000. running mean: -19.772823\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -19.785095\n",
            "resetting env. episode 1094.000000, reward total was -18.000000. running mean: -19.767244\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -19.779571\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -19.791776\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -19.803858\n",
            "resetting env. episode 1098.000000, reward total was -20.000000. running mean: -19.805819\n",
            "resetting env. episode 1099.000000, reward total was -20.000000. running mean: -19.807761\n",
            "resetting env. episode 1100.000000, reward total was -21.000000. running mean: -19.819684\n",
            "resetting env. episode 1101.000000, reward total was -20.000000. running mean: -19.821487\n",
            "resetting env. episode 1102.000000, reward total was -19.000000. running mean: -19.813272\n",
            "resetting env. episode 1103.000000, reward total was -19.000000. running mean: -19.805139\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -19.817088\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -19.828917\n",
            "resetting env. episode 1106.000000, reward total was -19.000000. running mean: -19.820628\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -19.832421\n",
            "resetting env. episode 1108.000000, reward total was -19.000000. running mean: -19.824097\n",
            "resetting env. episode 1109.000000, reward total was -20.000000. running mean: -19.825856\n",
            "resetting env. episode 1110.000000, reward total was -20.000000. running mean: -19.827598\n",
            "resetting env. episode 1111.000000, reward total was -20.000000. running mean: -19.829322\n",
            "resetting env. episode 1112.000000, reward total was -19.000000. running mean: -19.821028\n",
            "resetting env. episode 1113.000000, reward total was -18.000000. running mean: -19.802818\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -19.804790\n",
            "resetting env. episode 1115.000000, reward total was -17.000000. running mean: -19.776742\n",
            "resetting env. episode 1116.000000, reward total was -18.000000. running mean: -19.758975\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -19.771385\n",
            "resetting env. episode 1118.000000, reward total was -20.000000. running mean: -19.773671\n",
            "resetting env. episode 1119.000000, reward total was -18.000000. running mean: -19.755934\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -19.768375\n",
            "resetting env. episode 1121.000000, reward total was -18.000000. running mean: -19.750691\n",
            "resetting env. episode 1122.000000, reward total was -17.000000. running mean: -19.723184\n",
            "resetting env. episode 1123.000000, reward total was -18.000000. running mean: -19.705953\n",
            "resetting env. episode 1124.000000, reward total was -19.000000. running mean: -19.698893\n",
            "resetting env. episode 1125.000000, reward total was -20.000000. running mean: -19.701904\n",
            "resetting env. episode 1126.000000, reward total was -14.000000. running mean: -19.644885\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -19.658436\n",
            "resetting env. episode 1128.000000, reward total was -17.000000. running mean: -19.631852\n",
            "resetting env. episode 1129.000000, reward total was -20.000000. running mean: -19.635533\n",
            "resetting env. episode 1130.000000, reward total was -20.000000. running mean: -19.639178\n",
            "resetting env. episode 1131.000000, reward total was -20.000000. running mean: -19.642786\n",
            "resetting env. episode 1132.000000, reward total was -20.000000. running mean: -19.646358\n",
            "resetting env. episode 1133.000000, reward total was -20.000000. running mean: -19.649895\n",
            "resetting env. episode 1134.000000, reward total was -19.000000. running mean: -19.643396\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -19.646962\n",
            "resetting env. episode 1136.000000, reward total was -20.000000. running mean: -19.650492\n",
            "resetting env. episode 1137.000000, reward total was -17.000000. running mean: -19.623987\n",
            "resetting env. episode 1138.000000, reward total was -21.000000. running mean: -19.637747\n",
            "resetting env. episode 1139.000000, reward total was -20.000000. running mean: -19.641370\n",
            "resetting env. episode 1140.000000, reward total was -20.000000. running mean: -19.644956\n",
            "resetting env. episode 1141.000000, reward total was -20.000000. running mean: -19.648507\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -19.662022\n",
            "resetting env. episode 1143.000000, reward total was -20.000000. running mean: -19.665401\n",
            "resetting env. episode 1144.000000, reward total was -21.000000. running mean: -19.678747\n",
            "resetting env. episode 1145.000000, reward total was -20.000000. running mean: -19.681960\n",
            "resetting env. episode 1146.000000, reward total was -20.000000. running mean: -19.685140\n",
            "resetting env. episode 1147.000000, reward total was -20.000000. running mean: -19.688289\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -19.691406\n",
            "resetting env. episode 1149.000000, reward total was -19.000000. running mean: -19.684492\n",
            "resetting env. episode 1150.000000, reward total was -21.000000. running mean: -19.697647\n",
            "resetting env. episode 1151.000000, reward total was -18.000000. running mean: -19.680671\n",
            "resetting env. episode 1152.000000, reward total was -16.000000. running mean: -19.643864\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -19.657425\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -19.670851\n",
            "resetting env. episode 1155.000000, reward total was -19.000000. running mean: -19.664142\n",
            "resetting env. episode 1156.000000, reward total was -17.000000. running mean: -19.637501\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -19.651126\n",
            "resetting env. episode 1158.000000, reward total was -19.000000. running mean: -19.644615\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -19.648169\n",
            "resetting env. episode 1160.000000, reward total was -17.000000. running mean: -19.621687\n",
            "resetting env. episode 1161.000000, reward total was -21.000000. running mean: -19.635470\n",
            "resetting env. episode 1162.000000, reward total was -19.000000. running mean: -19.629115\n",
            "resetting env. episode 1163.000000, reward total was -20.000000. running mean: -19.632824\n",
            "resetting env. episode 1164.000000, reward total was -19.000000. running mean: -19.626496\n",
            "resetting env. episode 1165.000000, reward total was -17.000000. running mean: -19.600231\n",
            "resetting env. episode 1166.000000, reward total was -18.000000. running mean: -19.584229\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -19.598386\n",
            "resetting env. episode 1168.000000, reward total was -17.000000. running mean: -19.572403\n",
            "resetting env. episode 1169.000000, reward total was -20.000000. running mean: -19.576679\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -19.590912\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -19.605003\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -19.608953\n",
            "resetting env. episode 1173.000000, reward total was -19.000000. running mean: -19.602863\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -19.616834\n",
            "resetting env. episode 1175.000000, reward total was -18.000000. running mean: -19.600666\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -19.614659\n",
            "resetting env. episode 1177.000000, reward total was -20.000000. running mean: -19.618513\n",
            "resetting env. episode 1178.000000, reward total was -18.000000. running mean: -19.602328\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -19.616304\n",
            "resetting env. episode 1180.000000, reward total was -20.000000. running mean: -19.620141\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -19.633940\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -19.637601\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -19.651225\n",
            "resetting env. episode 1184.000000, reward total was -20.000000. running mean: -19.654712\n",
            "resetting env. episode 1185.000000, reward total was -19.000000. running mean: -19.648165\n",
            "resetting env. episode 1186.000000, reward total was -20.000000. running mean: -19.651684\n",
            "resetting env. episode 1187.000000, reward total was -18.000000. running mean: -19.635167\n",
            "resetting env. episode 1188.000000, reward total was -19.000000. running mean: -19.628815\n",
            "resetting env. episode 1189.000000, reward total was -19.000000. running mean: -19.622527\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -19.636302\n",
            "resetting env. episode 1191.000000, reward total was -18.000000. running mean: -19.619939\n",
            "resetting env. episode 1192.000000, reward total was -17.000000. running mean: -19.593739\n",
            "resetting env. episode 1193.000000, reward total was -19.000000. running mean: -19.587802\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -19.601924\n",
            "resetting env. episode 1195.000000, reward total was -18.000000. running mean: -19.585905\n",
            "resetting env. episode 1196.000000, reward total was -21.000000. running mean: -19.600046\n",
            "resetting env. episode 1197.000000, reward total was -20.000000. running mean: -19.604045\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -19.618005\n",
            "resetting env. episode 1199.000000, reward total was -19.000000. running mean: -19.611825\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -19.625706\n",
            "resetting env. episode 1201.000000, reward total was -20.000000. running mean: -19.629449\n",
            "resetting env. episode 1202.000000, reward total was -20.000000. running mean: -19.633155\n",
            "resetting env. episode 1203.000000, reward total was -19.000000. running mean: -19.626823\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -19.640555\n",
            "resetting env. episode 1205.000000, reward total was -18.000000. running mean: -19.624149\n",
            "resetting env. episode 1206.000000, reward total was -21.000000. running mean: -19.637908\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -19.631529\n",
            "resetting env. episode 1208.000000, reward total was -20.000000. running mean: -19.635214\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -19.648861\n",
            "resetting env. episode 1210.000000, reward total was -20.000000. running mean: -19.652373\n",
            "resetting env. episode 1211.000000, reward total was -19.000000. running mean: -19.645849\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -19.659391\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -19.672797\n",
            "resetting env. episode 1214.000000, reward total was -21.000000. running mean: -19.686069\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -19.699208\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -19.702216\n",
            "resetting env. episode 1217.000000, reward total was -20.000000. running mean: -19.705194\n",
            "resetting env. episode 1218.000000, reward total was -18.000000. running mean: -19.688142\n",
            "resetting env. episode 1219.000000, reward total was -18.000000. running mean: -19.671260\n",
            "resetting env. episode 1220.000000, reward total was -20.000000. running mean: -19.674548\n",
            "resetting env. episode 1221.000000, reward total was -16.000000. running mean: -19.637802\n",
            "resetting env. episode 1222.000000, reward total was -19.000000. running mean: -19.631424\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -19.645110\n",
            "resetting env. episode 1224.000000, reward total was -19.000000. running mean: -19.638659\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -19.652272\n",
            "resetting env. episode 1226.000000, reward total was -19.000000. running mean: -19.645750\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -19.659292\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -19.672699\n",
            "resetting env. episode 1229.000000, reward total was -20.000000. running mean: -19.675972\n",
            "resetting env. episode 1230.000000, reward total was -20.000000. running mean: -19.679213\n",
            "resetting env. episode 1231.000000, reward total was -19.000000. running mean: -19.672420\n",
            "resetting env. episode 1232.000000, reward total was -20.000000. running mean: -19.675696\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -19.688939\n",
            "resetting env. episode 1234.000000, reward total was -20.000000. running mean: -19.692050\n",
            "resetting env. episode 1235.000000, reward total was -16.000000. running mean: -19.655129\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -19.668578\n",
            "resetting env. episode 1237.000000, reward total was -19.000000. running mean: -19.661892\n",
            "resetting env. episode 1238.000000, reward total was -20.000000. running mean: -19.665273\n",
            "resetting env. episode 1239.000000, reward total was -19.000000. running mean: -19.658621\n",
            "resetting env. episode 1240.000000, reward total was -18.000000. running mean: -19.642034\n",
            "resetting env. episode 1241.000000, reward total was -20.000000. running mean: -19.645614\n",
            "resetting env. episode 1242.000000, reward total was -19.000000. running mean: -19.639158\n",
            "resetting env. episode 1243.000000, reward total was -20.000000. running mean: -19.642766\n",
            "resetting env. episode 1244.000000, reward total was -19.000000. running mean: -19.636339\n",
            "resetting env. episode 1245.000000, reward total was -20.000000. running mean: -19.639975\n",
            "resetting env. episode 1246.000000, reward total was -18.000000. running mean: -19.623576\n",
            "resetting env. episode 1247.000000, reward total was -18.000000. running mean: -19.607340\n",
            "resetting env. episode 1248.000000, reward total was -19.000000. running mean: -19.601266\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -19.615254\n",
            "resetting env. episode 1250.000000, reward total was -18.000000. running mean: -19.599101\n",
            "resetting env. episode 1251.000000, reward total was -21.000000. running mean: -19.613110\n",
            "resetting env. episode 1252.000000, reward total was -19.000000. running mean: -19.606979\n",
            "resetting env. episode 1253.000000, reward total was -17.000000. running mean: -19.580909\n",
            "resetting env. episode 1254.000000, reward total was -16.000000. running mean: -19.545100\n",
            "resetting env. episode 1255.000000, reward total was -21.000000. running mean: -19.559649\n",
            "resetting env. episode 1256.000000, reward total was -21.000000. running mean: -19.574053\n",
            "resetting env. episode 1257.000000, reward total was -19.000000. running mean: -19.568312\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -19.582629\n",
            "resetting env. episode 1259.000000, reward total was -20.000000. running mean: -19.586803\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -19.590935\n",
            "resetting env. episode 1261.000000, reward total was -18.000000. running mean: -19.575025\n",
            "resetting env. episode 1262.000000, reward total was -20.000000. running mean: -19.579275\n",
            "resetting env. episode 1263.000000, reward total was -17.000000. running mean: -19.553482\n",
            "resetting env. episode 1264.000000, reward total was -20.000000. running mean: -19.557948\n",
            "resetting env. episode 1265.000000, reward total was -18.000000. running mean: -19.542368\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -19.556944\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -19.571375\n",
            "resetting env. episode 1268.000000, reward total was -18.000000. running mean: -19.555661\n",
            "resetting env. episode 1269.000000, reward total was -17.000000. running mean: -19.530105\n",
            "resetting env. episode 1270.000000, reward total was -20.000000. running mean: -19.534804\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -19.539456\n",
            "resetting env. episode 1272.000000, reward total was -20.000000. running mean: -19.544061\n",
            "resetting env. episode 1273.000000, reward total was -18.000000. running mean: -19.528620\n",
            "resetting env. episode 1274.000000, reward total was -19.000000. running mean: -19.523334\n",
            "resetting env. episode 1275.000000, reward total was -21.000000. running mean: -19.538101\n",
            "resetting env. episode 1276.000000, reward total was -19.000000. running mean: -19.532720\n",
            "resetting env. episode 1277.000000, reward total was -18.000000. running mean: -19.517393\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -19.532219\n",
            "resetting env. episode 1279.000000, reward total was -20.000000. running mean: -19.536897\n",
            "resetting env. episode 1280.000000, reward total was -20.000000. running mean: -19.541528\n",
            "resetting env. episode 1281.000000, reward total was -20.000000. running mean: -19.546112\n",
            "resetting env. episode 1282.000000, reward total was -18.000000. running mean: -19.530651\n",
            "resetting env. episode 1283.000000, reward total was -19.000000. running mean: -19.525345\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -19.540091\n",
            "resetting env. episode 1285.000000, reward total was -18.000000. running mean: -19.524690\n",
            "resetting env. episode 1286.000000, reward total was -19.000000. running mean: -19.519443\n",
            "resetting env. episode 1287.000000, reward total was -17.000000. running mean: -19.494249\n",
            "resetting env. episode 1288.000000, reward total was -19.000000. running mean: -19.489306\n",
            "resetting env. episode 1289.000000, reward total was -20.000000. running mean: -19.494413\n",
            "resetting env. episode 1290.000000, reward total was -19.000000. running mean: -19.489469\n",
            "resetting env. episode 1291.000000, reward total was -18.000000. running mean: -19.474575\n",
            "resetting env. episode 1292.000000, reward total was -21.000000. running mean: -19.489829\n",
            "resetting env. episode 1293.000000, reward total was -18.000000. running mean: -19.474931\n",
            "resetting env. episode 1294.000000, reward total was -19.000000. running mean: -19.470181\n",
            "resetting env. episode 1295.000000, reward total was -19.000000. running mean: -19.465479\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -19.480825\n",
            "resetting env. episode 1297.000000, reward total was -20.000000. running mean: -19.486016\n",
            "resetting env. episode 1298.000000, reward total was -20.000000. running mean: -19.491156\n",
            "resetting env. episode 1299.000000, reward total was -17.000000. running mean: -19.466245\n",
            "resetting env. episode 1300.000000, reward total was -20.000000. running mean: -19.471582\n",
            "resetting env. episode 1301.000000, reward total was -21.000000. running mean: -19.486866\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -19.501998\n",
            "resetting env. episode 1303.000000, reward total was -19.000000. running mean: -19.496978\n",
            "resetting env. episode 1304.000000, reward total was -17.000000. running mean: -19.472008\n",
            "resetting env. episode 1305.000000, reward total was -20.000000. running mean: -19.477288\n",
            "resetting env. episode 1306.000000, reward total was -20.000000. running mean: -19.482515\n",
            "resetting env. episode 1307.000000, reward total was -19.000000. running mean: -19.477690\n",
            "resetting env. episode 1308.000000, reward total was -18.000000. running mean: -19.462913\n",
            "resetting env. episode 1309.000000, reward total was -21.000000. running mean: -19.478284\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -19.493501\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -19.508566\n",
            "resetting env. episode 1312.000000, reward total was -19.000000. running mean: -19.503480\n",
            "resetting env. episode 1313.000000, reward total was -19.000000. running mean: -19.498446\n",
            "resetting env. episode 1314.000000, reward total was -17.000000. running mean: -19.473461\n",
            "resetting env. episode 1315.000000, reward total was -19.000000. running mean: -19.468726\n",
            "resetting env. episode 1316.000000, reward total was -20.000000. running mean: -19.474039\n",
            "resetting env. episode 1317.000000, reward total was -18.000000. running mean: -19.459299\n",
            "resetting env. episode 1318.000000, reward total was -20.000000. running mean: -19.464706\n",
            "resetting env. episode 1319.000000, reward total was -19.000000. running mean: -19.460059\n",
            "resetting env. episode 1320.000000, reward total was -19.000000. running mean: -19.455458\n",
            "resetting env. episode 1321.000000, reward total was -19.000000. running mean: -19.450904\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -19.466395\n",
            "resetting env. episode 1323.000000, reward total was -19.000000. running mean: -19.461731\n",
            "resetting env. episode 1324.000000, reward total was -20.000000. running mean: -19.467113\n",
            "resetting env. episode 1325.000000, reward total was -18.000000. running mean: -19.452442\n",
            "resetting env. episode 1326.000000, reward total was -20.000000. running mean: -19.457918\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -19.473339\n",
            "resetting env. episode 1328.000000, reward total was -20.000000. running mean: -19.478605\n",
            "resetting env. episode 1329.000000, reward total was -18.000000. running mean: -19.463819\n",
            "resetting env. episode 1330.000000, reward total was -21.000000. running mean: -19.479181\n",
            "resetting env. episode 1331.000000, reward total was -19.000000. running mean: -19.474389\n",
            "resetting env. episode 1332.000000, reward total was -18.000000. running mean: -19.459645\n",
            "resetting env. episode 1333.000000, reward total was -18.000000. running mean: -19.445049\n",
            "resetting env. episode 1334.000000, reward total was -19.000000. running mean: -19.440598\n",
            "resetting env. episode 1335.000000, reward total was -19.000000. running mean: -19.436192\n",
            "resetting env. episode 1336.000000, reward total was -19.000000. running mean: -19.431830\n",
            "resetting env. episode 1337.000000, reward total was -20.000000. running mean: -19.437512\n",
            "resetting env. episode 1338.000000, reward total was -19.000000. running mean: -19.433137\n",
            "resetting env. episode 1339.000000, reward total was -18.000000. running mean: -19.418806\n",
            "resetting env. episode 1340.000000, reward total was -19.000000. running mean: -19.414618\n",
            "resetting env. episode 1341.000000, reward total was -19.000000. running mean: -19.410471\n",
            "resetting env. episode 1342.000000, reward total was -19.000000. running mean: -19.406367\n",
            "resetting env. episode 1343.000000, reward total was -18.000000. running mean: -19.392303\n",
            "resetting env. episode 1344.000000, reward total was -21.000000. running mean: -19.408380\n",
            "resetting env. episode 1345.000000, reward total was -20.000000. running mean: -19.414296\n",
            "resetting env. episode 1346.000000, reward total was -20.000000. running mean: -19.420153\n",
            "resetting env. episode 1347.000000, reward total was -19.000000. running mean: -19.415952\n",
            "resetting env. episode 1348.000000, reward total was -20.000000. running mean: -19.421792\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -19.427574\n",
            "resetting env. episode 1350.000000, reward total was -17.000000. running mean: -19.403298\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -19.409265\n",
            "resetting env. episode 1352.000000, reward total was -21.000000. running mean: -19.425173\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -19.440921\n",
            "resetting env. episode 1354.000000, reward total was -20.000000. running mean: -19.446512\n",
            "resetting env. episode 1355.000000, reward total was -20.000000. running mean: -19.452047\n",
            "resetting env. episode 1356.000000, reward total was -20.000000. running mean: -19.457526\n",
            "resetting env. episode 1357.000000, reward total was -18.000000. running mean: -19.442951\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -19.458522\n",
            "resetting env. episode 1359.000000, reward total was -20.000000. running mean: -19.463936\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -19.479297\n",
            "resetting env. episode 1361.000000, reward total was -20.000000. running mean: -19.484504\n",
            "resetting env. episode 1362.000000, reward total was -19.000000. running mean: -19.479659\n",
            "resetting env. episode 1363.000000, reward total was -17.000000. running mean: -19.454862\n",
            "resetting env. episode 1364.000000, reward total was -20.000000. running mean: -19.460314\n",
            "resetting env. episode 1365.000000, reward total was -19.000000. running mean: -19.455711\n",
            "resetting env. episode 1366.000000, reward total was -19.000000. running mean: -19.451153\n",
            "resetting env. episode 1367.000000, reward total was -20.000000. running mean: -19.456642\n",
            "resetting env. episode 1368.000000, reward total was -21.000000. running mean: -19.472076\n",
            "resetting env. episode 1369.000000, reward total was -19.000000. running mean: -19.467355\n",
            "resetting env. episode 1370.000000, reward total was -20.000000. running mean: -19.472681\n",
            "resetting env. episode 1371.000000, reward total was -19.000000. running mean: -19.467954\n",
            "resetting env. episode 1372.000000, reward total was -17.000000. running mean: -19.443275\n",
            "resetting env. episode 1373.000000, reward total was -19.000000. running mean: -19.438842\n",
            "resetting env. episode 1374.000000, reward total was -18.000000. running mean: -19.424454\n",
            "resetting env. episode 1375.000000, reward total was -19.000000. running mean: -19.420209\n",
            "resetting env. episode 1376.000000, reward total was -18.000000. running mean: -19.406007\n",
            "resetting env. episode 1377.000000, reward total was -19.000000. running mean: -19.401947\n",
            "resetting env. episode 1378.000000, reward total was -18.000000. running mean: -19.387928\n",
            "resetting env. episode 1379.000000, reward total was -19.000000. running mean: -19.384048\n",
            "resetting env. episode 1380.000000, reward total was -17.000000. running mean: -19.360208\n",
            "resetting env. episode 1381.000000, reward total was -18.000000. running mean: -19.346606\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -19.363140\n",
            "resetting env. episode 1383.000000, reward total was -19.000000. running mean: -19.359508\n",
            "resetting env. episode 1384.000000, reward total was -21.000000. running mean: -19.375913\n",
            "resetting env. episode 1385.000000, reward total was -19.000000. running mean: -19.372154\n",
            "resetting env. episode 1386.000000, reward total was -19.000000. running mean: -19.368432\n",
            "resetting env. episode 1387.000000, reward total was -18.000000. running mean: -19.354748\n",
            "resetting env. episode 1388.000000, reward total was -19.000000. running mean: -19.351201\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -19.367689\n",
            "resetting env. episode 1390.000000, reward total was -20.000000. running mean: -19.374012\n",
            "resetting env. episode 1391.000000, reward total was -20.000000. running mean: -19.380272\n",
            "resetting env. episode 1392.000000, reward total was -21.000000. running mean: -19.396469\n",
            "resetting env. episode 1393.000000, reward total was -20.000000. running mean: -19.402504\n",
            "resetting env. episode 1394.000000, reward total was -20.000000. running mean: -19.408479\n",
            "resetting env. episode 1395.000000, reward total was -16.000000. running mean: -19.374394\n",
            "resetting env. episode 1396.000000, reward total was -17.000000. running mean: -19.350650\n",
            "resetting env. episode 1397.000000, reward total was -16.000000. running mean: -19.317144\n",
            "resetting env. episode 1398.000000, reward total was -19.000000. running mean: -19.313973\n",
            "resetting env. episode 1399.000000, reward total was -18.000000. running mean: -19.300833\n",
            "resetting env. episode 1400.000000, reward total was -19.000000. running mean: -19.297824\n",
            "resetting env. episode 1401.000000, reward total was -19.000000. running mean: -19.294846\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -19.311898\n",
            "resetting env. episode 1403.000000, reward total was -18.000000. running mean: -19.298779\n",
            "resetting env. episode 1404.000000, reward total was -20.000000. running mean: -19.305791\n",
            "resetting env. episode 1405.000000, reward total was -16.000000. running mean: -19.272733\n",
            "resetting env. episode 1406.000000, reward total was -19.000000. running mean: -19.270006\n",
            "resetting env. episode 1407.000000, reward total was -18.000000. running mean: -19.257306\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -19.264733\n",
            "resetting env. episode 1409.000000, reward total was -20.000000. running mean: -19.272085\n",
            "resetting env. episode 1410.000000, reward total was -19.000000. running mean: -19.269364\n",
            "resetting env. episode 1411.000000, reward total was -15.000000. running mean: -19.226671\n",
            "resetting env. episode 1412.000000, reward total was -18.000000. running mean: -19.214404\n",
            "resetting env. episode 1413.000000, reward total was -17.000000. running mean: -19.192260\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -19.210337\n",
            "resetting env. episode 1415.000000, reward total was -20.000000. running mean: -19.218234\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -19.236052\n",
            "resetting env. episode 1417.000000, reward total was -21.000000. running mean: -19.253691\n",
            "resetting env. episode 1418.000000, reward total was -16.000000. running mean: -19.221154\n",
            "resetting env. episode 1419.000000, reward total was -20.000000. running mean: -19.228943\n",
            "resetting env. episode 1420.000000, reward total was -20.000000. running mean: -19.236653\n",
            "resetting env. episode 1421.000000, reward total was -18.000000. running mean: -19.224287\n",
            "resetting env. episode 1422.000000, reward total was -20.000000. running mean: -19.232044\n",
            "resetting env. episode 1423.000000, reward total was -18.000000. running mean: -19.219724\n",
            "resetting env. episode 1424.000000, reward total was -19.000000. running mean: -19.217526\n",
            "resetting env. episode 1425.000000, reward total was -21.000000. running mean: -19.235351\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -19.252998\n",
            "resetting env. episode 1427.000000, reward total was -19.000000. running mean: -19.250468\n",
            "resetting env. episode 1428.000000, reward total was -18.000000. running mean: -19.237963\n",
            "resetting env. episode 1429.000000, reward total was -18.000000. running mean: -19.225583\n",
            "resetting env. episode 1430.000000, reward total was -17.000000. running mean: -19.203327\n",
            "resetting env. episode 1431.000000, reward total was -16.000000. running mean: -19.171294\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -19.189581\n",
            "resetting env. episode 1433.000000, reward total was -20.000000. running mean: -19.197685\n",
            "resetting env. episode 1434.000000, reward total was -17.000000. running mean: -19.175709\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -19.193951\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -19.202012\n",
            "resetting env. episode 1437.000000, reward total was -19.000000. running mean: -19.199992\n",
            "resetting env. episode 1438.000000, reward total was -18.000000. running mean: -19.187992\n",
            "resetting env. episode 1439.000000, reward total was -19.000000. running mean: -19.186112\n",
            "resetting env. episode 1440.000000, reward total was -18.000000. running mean: -19.174251\n",
            "resetting env. episode 1441.000000, reward total was -21.000000. running mean: -19.192508\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -19.210583\n",
            "resetting env. episode 1443.000000, reward total was -18.000000. running mean: -19.198477\n",
            "resetting env. episode 1444.000000, reward total was -21.000000. running mean: -19.216493\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -19.224328\n",
            "resetting env. episode 1446.000000, reward total was -19.000000. running mean: -19.222084\n",
            "resetting env. episode 1447.000000, reward total was -20.000000. running mean: -19.229864\n",
            "resetting env. episode 1448.000000, reward total was -20.000000. running mean: -19.237565\n",
            "resetting env. episode 1449.000000, reward total was -19.000000. running mean: -19.235189\n",
            "resetting env. episode 1450.000000, reward total was -18.000000. running mean: -19.222837\n",
            "resetting env. episode 1451.000000, reward total was -17.000000. running mean: -19.200609\n",
            "resetting env. episode 1452.000000, reward total was -19.000000. running mean: -19.198603\n",
            "resetting env. episode 1453.000000, reward total was -17.000000. running mean: -19.176617\n",
            "resetting env. episode 1454.000000, reward total was -19.000000. running mean: -19.174851\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -19.193102\n",
            "resetting env. episode 1456.000000, reward total was -19.000000. running mean: -19.191171\n",
            "resetting env. episode 1457.000000, reward total was -19.000000. running mean: -19.189260\n",
            "resetting env. episode 1458.000000, reward total was -20.000000. running mean: -19.197367\n",
            "resetting env. episode 1459.000000, reward total was -18.000000. running mean: -19.185393\n",
            "resetting env. episode 1460.000000, reward total was -19.000000. running mean: -19.183539\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -19.201704\n",
            "resetting env. episode 1462.000000, reward total was -20.000000. running mean: -19.209687\n",
            "resetting env. episode 1463.000000, reward total was -19.000000. running mean: -19.207590\n",
            "resetting env. episode 1464.000000, reward total was -19.000000. running mean: -19.205514\n",
            "resetting env. episode 1465.000000, reward total was -20.000000. running mean: -19.213459\n",
            "resetting env. episode 1466.000000, reward total was -19.000000. running mean: -19.211324\n",
            "resetting env. episode 1467.000000, reward total was -18.000000. running mean: -19.199211\n",
            "resetting env. episode 1468.000000, reward total was -19.000000. running mean: -19.197219\n",
            "resetting env. episode 1469.000000, reward total was -20.000000. running mean: -19.205247\n",
            "resetting env. episode 1470.000000, reward total was -15.000000. running mean: -19.163194\n",
            "resetting env. episode 1471.000000, reward total was -20.000000. running mean: -19.171562\n",
            "resetting env. episode 1472.000000, reward total was -20.000000. running mean: -19.179847\n",
            "resetting env. episode 1473.000000, reward total was -21.000000. running mean: -19.198048\n",
            "resetting env. episode 1474.000000, reward total was -20.000000. running mean: -19.206068\n",
            "resetting env. episode 1475.000000, reward total was -20.000000. running mean: -19.214007\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -19.231867\n",
            "resetting env. episode 1477.000000, reward total was -20.000000. running mean: -19.239548\n",
            "resetting env. episode 1478.000000, reward total was -18.000000. running mean: -19.227153\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -19.244881\n",
            "resetting env. episode 1480.000000, reward total was -20.000000. running mean: -19.252433\n",
            "resetting env. episode 1481.000000, reward total was -20.000000. running mean: -19.259908\n",
            "resetting env. episode 1482.000000, reward total was -20.000000. running mean: -19.267309\n",
            "resetting env. episode 1483.000000, reward total was -21.000000. running mean: -19.284636\n",
            "resetting env. episode 1484.000000, reward total was -19.000000. running mean: -19.281790\n",
            "resetting env. episode 1485.000000, reward total was -16.000000. running mean: -19.248972\n",
            "resetting env. episode 1486.000000, reward total was -20.000000. running mean: -19.256482\n",
            "resetting env. episode 1487.000000, reward total was -17.000000. running mean: -19.233917\n",
            "resetting env. episode 1488.000000, reward total was -17.000000. running mean: -19.211578\n",
            "resetting env. episode 1489.000000, reward total was -19.000000. running mean: -19.209462\n",
            "resetting env. episode 1490.000000, reward total was -16.000000. running mean: -19.177368\n",
            "resetting env. episode 1491.000000, reward total was -20.000000. running mean: -19.185594\n",
            "resetting env. episode 1492.000000, reward total was -19.000000. running mean: -19.183738\n",
            "resetting env. episode 1493.000000, reward total was -18.000000. running mean: -19.171901\n",
            "resetting env. episode 1494.000000, reward total was -20.000000. running mean: -19.180182\n",
            "resetting env. episode 1495.000000, reward total was -18.000000. running mean: -19.168380\n",
            "resetting env. episode 1496.000000, reward total was -20.000000. running mean: -19.176696\n",
            "resetting env. episode 1497.000000, reward total was -19.000000. running mean: -19.174929\n",
            "resetting env. episode 1498.000000, reward total was -18.000000. running mean: -19.163180\n",
            "resetting env. episode 1499.000000, reward total was -20.000000. running mean: -19.171548\n",
            "resetting env. episode 1500.000000, reward total was -21.000000. running mean: -19.189833\n",
            "CPU times: user 3h 51min 58s, sys: 43min 53s, total: 4h 35min 52s\n",
            "Wall time: 2h 21min 32s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "CteN7XKMVGqg",
        "outputId": "01e14ef7-8876-4b1c-94ce-c4f36513c3db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZYA0HgMoO77a"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# pickle.dump(model, open('model.pkl', 'wb'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "pg_from_scratch_(h_800).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}