{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l9hHvTk6ec8"
      },
      "source": [
        "# Policy Gradient\n",
        "\n",
        "* http://karpathy.github.io/2016/05/31/rl/\n",
        "* https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
        "* https://github.com/gameofdimension/policy-gradient-pong\n",
        "* https://www.youtube.com/watch?v=tqrcjHuNdmQ\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqkOdLyN9Ylm"
      },
      "source": [
        "## Step 1: Installation for Colab - just execute these cells and do not worry too much\n",
        "\n",
        "* http://nbviewer.jupyter.org/github/patrickmineault/xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb \n",
        "* https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi\n",
        "* https://nyu-cds.github.io/python-mpi/setup/\n",
        "* https://medium.com/@kaleajit27/reinforcement-learning-on-google-colab-9cb2e1ef51e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF9MAVI16huj",
        "outputId": "30a14371-5bab-4aef-ac85-9f8eab2e19d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install python-opengl -y  >/dev/null\n",
        "!apt install xvfb -y >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fSC11TfN6p69"
      },
      "outputs": [],
      "source": [
        "!pip install pyvirtualdisplay >/dev/null\n",
        "!pip install piglet >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "caiHE2hy6xrf"
      },
      "outputs": [],
      "source": [
        "# from pyvirtualdisplay import Display\n",
        "# display = Display(visible=0, size=(1400, 900))\n",
        "# display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "7ec0a298-ae99-4d5d-e172-af50a53754b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 29.3 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=e71a91f28bf0f46d4d1f05bde9bee2c376c85938daaac706b29ce3581f5184f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "4d8d8337-97cd-4633-cd75-e91600393731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "c05114f5-e07b-4c01-f426-6543711154aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "b66c4f5f-4282-4326-a967-b62de861a527"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "c7363a5f-a7bc-4c23-91e1-92566ebf33da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -16.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  env.close()\n",
        "  display_frames_as_gif(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 3 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-6\n",
        "learning_rate = 1e-6\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "612448e0-d38d-4435-8c6f-9d879d5f2c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -19.020000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.039800\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.059402\n",
            "resetting env. episode 5.000000, reward total was -18.000000. running mean: -19.048808\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -19.068320\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.087637\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -19.096760\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.115793\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -19.134635\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -19.153288\n",
            "resetting env. episode 12.000000, reward total was -19.000000. running mean: -19.151756\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -19.150238\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -19.158736\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -19.177148\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -19.185377\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -19.203523\n",
            "resetting env. episode 18.000000, reward total was -18.000000. running mean: -19.191488\n",
            "resetting env. episode 19.000000, reward total was -19.000000. running mean: -19.189573\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -19.197677\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -19.215700\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -19.223543\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -19.231308\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -19.238995\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.256605\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -19.264039\n",
            "resetting env. episode 27.000000, reward total was -19.000000. running mean: -19.261399\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -19.278785\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.295997\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.313037\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -19.329906\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -19.346607\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.363141\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -19.379510\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -19.385715\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -19.391858\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -19.407939\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -19.413860\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -19.419721\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -19.425524\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -19.441269\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -19.446856\n",
            "resetting env. episode 43.000000, reward total was -18.000000. running mean: -19.432387\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.448063\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -19.453583\n",
            "resetting env. episode 46.000000, reward total was -19.000000. running mean: -19.449047\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -19.454556\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -19.460011\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -19.465411\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.480757\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.495949\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -19.500990\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -19.515980\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -19.520820\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -19.515612\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -19.520456\n",
            "resetting env. episode 57.000000, reward total was -19.000000. running mean: -19.515251\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -19.520099\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.534898\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.549549\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.564053\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -19.568413\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.582728\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -19.576901\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -19.591132\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.605221\n",
            "resetting env. episode 67.000000, reward total was -19.000000. running mean: -19.599169\n",
            "resetting env. episode 68.000000, reward total was -16.000000. running mean: -19.563177\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.577545\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.591770\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -19.595852\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.609894\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.623795\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -19.627557\n",
            "resetting env. episode 75.000000, reward total was -19.000000. running mean: -19.621281\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.635068\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.648718\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.662230\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -19.675608\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -19.688852\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -19.701963\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.714944\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.727794\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.740516\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.753111\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -19.765580\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.777924\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.790145\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.802244\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -19.804221\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.816179\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.828017\n",
            "resetting env. episode 93.000000, reward total was -19.000000. running mean: -19.819737\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -19.831540\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -19.833224\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.844892\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -19.846443\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -19.837979\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.849599\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.861103\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -19.872492\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -19.873767\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -19.885029\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -19.886179\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -19.887317\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -19.888444\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.899560\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.910564\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.921458\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.932244\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -19.942921\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.953492\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -19.953957\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.964418\n",
            "resetting env. episode 115.000000, reward total was -18.000000. running mean: -19.944773\n",
            "resetting env. episode 116.000000, reward total was -19.000000. running mean: -19.935326\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.945972\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.956513\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.956948\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -19.957378\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.967804\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -19.968126\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -19.978445\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -19.978661\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -19.978874\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -19.979085\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -19.979294\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -19.989501\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -19.999606\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.009610\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.009514\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.009419\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.019325\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.029132\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.038840\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.048452\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.057967\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.057388\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.066814\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.066146\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.075484\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.074729\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.073982\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.083242\n",
            "resetting env. episode 145.000000, reward total was -19.000000. running mean: -20.072410\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.081686\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.080869\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.080060\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.079260\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.068467\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.077782\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.077005\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.086235\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.085372\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.084518\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.093673\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.102737\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.101709\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.110692\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.119585\n",
            "resetting env. episode 161.000000, reward total was -19.000000. running mean: -20.108389\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.117305\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.126132\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -20.114871\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.123722\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.132485\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.141160\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.139749\n",
            "resetting env. episode 169.000000, reward total was -18.000000. running mean: -20.118351\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.127168\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.125896\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.134637\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.143291\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.141858\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.150439\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.158935\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.157345\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.165772\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.174114\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.182373\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.180549\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.188744\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.196856\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.204888\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.202839\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.200811\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.208803\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.206715\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.214647\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.222501\n",
            "resetting env. episode 191.000000, reward total was -19.000000. running mean: -20.210276\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.218173\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.225991\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.233731\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.241394\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.248980\n",
            "resetting env. episode 197.000000, reward total was -19.000000. running mean: -20.236490\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -20.224126\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.231884\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.239565\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.247170\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.254698\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.262151\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.269530\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.256834\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.264266\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.271623\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.278907\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.276118\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.273357\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.280623\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.277817\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.285039\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.292188\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.289267\n",
            "resetting env. episode 216.000000, reward total was -19.000000. running mean: -20.276374\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.283610\n",
            "resetting env. episode 218.000000, reward total was -19.000000. running mean: -20.270774\n",
            "resetting env. episode 219.000000, reward total was -19.000000. running mean: -20.258066\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -20.245486\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.253031\n",
            "resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.240500\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.238095\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.245715\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.243257\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.240825\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.248417\n",
            "resetting env. episode 228.000000, reward total was -18.000000. running mean: -20.225932\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.233673\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.241336\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.238923\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.246534\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.254068\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.251528\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.259012\n",
            "resetting env. episode 236.000000, reward total was -18.000000. running mean: -20.236422\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.244058\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.241618\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.249201\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.236709\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.244342\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.251899\n",
            "resetting env. episode 243.000000, reward total was -19.000000. running mean: -20.239380\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.236986\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.244616\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.252170\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.259648\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.267052\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.264381\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.271737\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.269020\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.276330\n",
            "resetting env. episode 253.000000, reward total was -19.000000. running mean: -20.263567\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.260931\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.268322\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.275638\n",
            "resetting env. episode 257.000000, reward total was -19.000000. running mean: -20.262882\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.260253\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.267651\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.274974\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.282224\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.279402\n",
            "resetting env. episode 263.000000, reward total was -19.000000. running mean: -20.266608\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.273942\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.271203\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.278491\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.285706\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.292849\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.299920\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.306921\n",
            "resetting env. episode 271.000000, reward total was -19.000000. running mean: -20.293852\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.300913\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.307904\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.314825\n",
            "resetting env. episode 275.000000, reward total was -19.000000. running mean: -20.301677\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.308660\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.315573\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.322418\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.319194\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.326002\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.322742\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.329514\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.336219\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.342857\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.339428\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.346034\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.342574\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.349148\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.345656\n",
            "resetting env. episode 290.000000, reward total was -19.000000. running mean: -20.332200\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.328878\n",
            "resetting env. episode 292.000000, reward total was -19.000000. running mean: -20.315589\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.322433\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.319209\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.326017\n",
            "resetting env. episode 296.000000, reward total was -18.000000. running mean: -20.302757\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.309729\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.316632\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.323465\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.320231\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.327028\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.333758\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.330421\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.337116\n",
            "resetting env. episode 305.000000, reward total was -16.000000. running mean: -20.293745\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.300808\n",
            "resetting env. episode 307.000000, reward total was -19.000000. running mean: -20.287800\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.294922\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.301973\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.298953\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.295963\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.303004\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.309974\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.316874\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.323705\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.320468\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.327263\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.333991\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.330651\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.327344\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.334071\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.340730\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.347323\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.353850\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.360311\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.366708\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.373041\n",
            "resetting env. episode 328.000000, reward total was -17.000000. running mean: -20.339311\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.345917\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.352458\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.358934\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.355344\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.361791\n",
            "resetting env. episode 334.000000, reward total was -18.000000. running mean: -20.338173\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.344791\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.351343\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.357830\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.364252\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.370609\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.376903\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.383134\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.389303\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.385410\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.391556\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.397640\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.393664\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.399727\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.405730\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.411672\n",
            "resetting env. episode 350.000000, reward total was -19.000000. running mean: -20.397556\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.393580\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.399644\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.405648\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.411591\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.417475\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.423301\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.429068\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.434777\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.440429\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.446025\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.451565\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.457049\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.462479\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.467854\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.473175\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.478443\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.473659\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.468922\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.474233\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.469491\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.474796\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.480048\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.485248\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.490395\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.495491\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.500536\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.505531\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.510476\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.505371\n",
            "resetting env. episode 380.000000, reward total was -19.000000. running mean: -20.490317\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.495414\n",
            "resetting env. episode 382.000000, reward total was -18.000000. running mean: -20.470460\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.475755\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.470998\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.456288\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.461725\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.467108\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.462436\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.467812\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.453134\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.458603\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.444017\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.449576\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.455081\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.450530\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.456025\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.461464\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.466850\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.462181\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.467559\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.472884\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.478155\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.483373\n",
            "resetting env. episode 404.000000, reward total was -19.000000. running mean: -20.468540\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.463854\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.469216\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.474524\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.479778\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.474981\n",
            "resetting env. episode 410.000000, reward total was -18.000000. running mean: -20.450231\n",
            "resetting env. episode 411.000000, reward total was -19.000000. running mean: -20.435728\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.441371\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.446957\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.452488\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.457963\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.453383\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.448850\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.444361\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.449917\n",
            "resetting env. episode 420.000000, reward total was -19.000000. running mean: -20.435418\n",
            "resetting env. episode 421.000000, reward total was -18.000000. running mean: -20.411064\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.416953\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.422784\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.418556\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.424370\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.430127\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.435826\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.441467\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.447053\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.442582\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.448156\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.453675\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.459138\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.464547\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.469901\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.475202\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.470450\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.465746\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.471088\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.476377\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.471613\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.476897\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.472128\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.477407\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.482633\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.477807\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.473029\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.468298\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.463615\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.458979\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.454389\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.459845\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.455247\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.460695\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.466088\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.471427\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.476712\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.481945\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.487126\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.492255\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.487332\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.492459\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.497534\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.502559\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.507533\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.512458\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.507333\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.512260\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.517137\n",
            "resetting env. episode 470.000000, reward total was -19.000000. running mean: -20.501966\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.506946\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.511877\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.506758\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.511691\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.516574\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.511408\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.516294\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.521131\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.525920\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.530660\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.525354\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.530100\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.524799\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -20.509551\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.514456\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.519311\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -20.504118\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.509077\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.513986\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.508846\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.513758\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.518620\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -20.503434\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.508400\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.513316\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.518183\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.523001\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.527771\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -20.512493\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.507368\n",
            "CPU times: user 24min 24s, sys: 10min 45s, total: 35min 10s\n",
            "Wall time: 18min 12s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "NuSP4FE8QTA-",
        "outputId": "bc5bf723-d355-4263-b6c2-65189cfd8128"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -19.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ4AAAFZCAYAAABpOsHqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAHb0lEQVR4nO3dz2scZRzH8WeTjd1m0yTtJtFGMf5uwUMP1qMH8aI3L+Lf4EHEf0HwKuif4EVU8OpRQQTBi3gQRasIxTY1a5om2TSasF6tW3U/k42zSV6v4wM7fBeSNzPPMjONfr9fABITdQ8AHD3CAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4g1qz6wecfOz30bbUTjVKeWTlVpqcOr1P3LXTKdOv0wPpqt1u2d3aGPk5nfq7MzZw58Dy3trfK2vrNAx+H0dtYWSjb588e+DjTqxtl/qcbI5ioPq99/Fujyucqh+OFxwf/Set03+JiWTw7+MewvbMThmO+rCwvH3ieq9dXhWNMbTy0VG489fCBj7Pw9c9HPhxVuVQBYsIBxIQDiAkHEKu8OXrS3NzcLLc2twbWz8y0y9nZ2RomYtTa19ZL+9rghnbv3rmydf+5GiYaX8IxpO76zfLj1asD6yvLy8JxTMz99GtZ/uL7gfXrlx8Rjr9xqQLEhAOICQcQEw4gZnN0SGfa0+X84uLA+uxMu4ZpoF7CMaSlTqcsdTp1jwFjwaUKEBMOICYcQEw4gJjN0SFt9Xp3fSBQu3W6zLSna5gI6iMcQ1pd6/7jvSpPtFdqmAjq41IFiAkHEBMOICYcQMzm6JBOt06Vc3NzA+vTrVYN03AYduemy60HB28ruD3vfqS/E44hLS8tleWlpbrH4BB1n3ygdJ98oO4xjgSXKkBMOICYcAAx4QBix2ZztLezUzaag1/nj7296Di3d38vG5ubB55nZ/f2gY/B4Ti1uXPX96fEx9kY/mXmx02j3+9X+uDbL5yr9kGo2Sj/cBsjPFYdXvv4t0pf4dicccCwjvo/+ziwxwHEhAOIVb5UeebVd0Y5B3CEVN4c7Xa7NkfhiOt0OpW2fFyqADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQq3xb/VcfvDXKOYAaPPfKm5U+55mjcIJVfeaoSxUgJhxAbGyfcj45MVEajcGzqP39/ZE+3h7IjW04Ll28UM602wPrX3373UhemARUN7bhmGo2yz1TU3es9fv9MnGXsxDgTlOtdnns2ZdLY2Ky7O32yvefvF/6+9lbDf/N2IYDqG6qPVsuvfR6ad7TKr311XLls4/K/gjDYXMUiAkHEBMOICYcQMzmKBxDe7d75cqnH5bGZLP80dsc6S8qpQgHHEu/b2+UL99949CO71IFiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcACxsb6tvurrKYHDNbbh+ObKj6U5OTmwvrm9XcM0wF+NbTgEAsaXPQ4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxATDiAmHAAMeEAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLNugeAk26vNVW2zs8PrDd390r7l/XSqGGm/yIcULPe4mz54cWnS2ncmYj2tZvl4nuf1zTVv3OpAsSEA4gJBxATDiBWeXN08YnLo5wDTqz2vbNlb+bRgfXWua2ydGG3lH4NQ/2HRr9fbaq1tbUx/DpAYmFhodKvvZXPOBqNcfx1Gfg/2OMAYsIBxIQDiAkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcAAx4QBiwgHEhAOICQcQEw4gJhxArPJ7VYCTyxkHEBMOICYcQEw4gJhwADHhAGLCAcSEA4gJBxATDiAmHEBMOICYcACxPwHK3d9d+DXnZwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCZR5OV-z-YJ",
        "outputId": "c5f7e264-6125-42c2-c2e7-55d5a6d09c9d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.990297\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.980394\n",
            "resetting env. episode 8.000000, reward total was -19.000000. running mean: -20.960590\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.950984\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.951474\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.951960\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.952440\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.952916\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.953386\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.933853\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.924514\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.925269\n",
            "resetting env. episode 18.000000, reward total was -19.000000. running mean: -20.906016\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.896956\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.897987\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.889007\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.880117\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.881315\n",
            "resetting env. episode 24.000000, reward total was -19.000000. running mean: -20.862502\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.863877\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.865238\n",
            "resetting env. episode 27.000000, reward total was -19.000000. running mean: -20.846586\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.848120\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.849639\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.851143\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.852631\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.854105\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.855564\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.857008\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.858438\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.859854\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.861255\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -20.842643\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.844216\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.835774\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.837416\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.839042\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.830652\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.832345\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.814022\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.815882\n",
            "resetting env. episode 47.000000, reward total was -19.000000. running mean: -20.797723\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.799746\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.791748\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.793831\n",
            "resetting env. episode 51.000000, reward total was -19.000000. running mean: -20.775892\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.778133\n",
            "resetting env. episode 53.000000, reward total was -18.000000. running mean: -20.750352\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.742848\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.745420\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.747966\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.750486\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.752981\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.745451\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.747997\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.740517\n",
            "resetting env. episode 62.000000, reward total was -19.000000. running mean: -20.723112\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -20.705881\n",
            "resetting env. episode 64.000000, reward total was -18.000000. running mean: -20.678822\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.672034\n",
            "resetting env. episode 66.000000, reward total was -19.000000. running mean: -20.655313\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.658760\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.652173\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.655651\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.659094\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.662503\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.665878\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.659220\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -20.642627\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.646201\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.649739\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.643242\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.636809\n",
            "resetting env. episode 79.000000, reward total was -18.000000. running mean: -20.610441\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -20.594337\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.598393\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.592410\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.596485\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.590521\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.594615\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.598669\n",
            "resetting env. episode 87.000000, reward total was -18.000000. running mean: -20.572683\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.576956\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.581186\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.585374\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -20.569521\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.563825\n",
            "resetting env. episode 93.000000, reward total was -19.000000. running mean: -20.548187\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.552705\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.557178\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.561606\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.565990\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.570330\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.564627\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.558981\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -20.543391\n",
            "resetting env. episode 102.000000, reward total was -18.000000. running mean: -20.517957\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.512778\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.507650\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.512573\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.507448\n",
            "resetting env. episode 107.000000, reward total was -19.000000. running mean: -20.492373\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.497449\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.502475\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.507450\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.512376\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.517252\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.522079\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.526859\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.531590\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.536274\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.540911\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.535502\n",
            "resetting env. episode 119.000000, reward total was -19.000000. running mean: -20.520147\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.524946\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.519696\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.524499\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.529254\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.523962\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.528722\n",
            "resetting env. episode 126.000000, reward total was -19.000000. running mean: -20.513435\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.508301\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -20.493218\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.498285\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.503303\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.498269\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.503287\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.508254\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.513171\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.518040\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.512859\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.517731\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.512553\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.517428\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.522254\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.527031\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.531761\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.536443\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.531079\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.535768\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.540410\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.545006\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.549556\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.554060\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.548520\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.553035\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.557504\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.561929\n",
            "resetting env. episode 154.000000, reward total was -19.000000. running mean: -20.546310\n",
            "resetting env. episode 155.000000, reward total was -19.000000. running mean: -20.530847\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.535538\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.530183\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.534881\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.539532\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.544137\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.548696\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.553209\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.547677\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.552200\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.546678\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.551211\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.555699\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.560142\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.554541\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.548995\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.553505\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.557970\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.552390\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.546867\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.551398\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.555884\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.550325\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.554822\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.559274\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.563681\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.568044\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.572364\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.576640\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -20.560874\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.565265\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.569612\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.573916\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.578177\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.582395\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.576571\n",
            "resetting env. episode 191.000000, reward total was -19.000000. running mean: -20.560806\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.565197\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.569545\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.563850\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.568212\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.572529\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.576804\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.581036\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.575226\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.569473\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.563779\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.558141\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.562560\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.556934\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.561365\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.565751\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.570093\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.574392\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.578649\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.582862\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -20.567033\n",
            "resetting env. episode 212.000000, reward total was -19.000000. running mean: -20.551363\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.555849\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.550291\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.544788\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.549340\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.553847\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.548308\n",
            "resetting env. episode 219.000000, reward total was -18.000000. running mean: -20.522825\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.527597\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.532321\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.536998\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.541628\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.536212\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.530849\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.525541\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.530286\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.534983\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.539633\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.534237\n",
            "resetting env. episode 231.000000, reward total was -18.000000. running mean: -20.508894\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.503805\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.508767\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.513680\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.518543\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.523357\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.528124\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.532842\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.537514\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.532139\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.516818\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.511649\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.516533\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.521368\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.526154\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.530892\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.535583\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.540228\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.544825\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.539377\n",
            "resetting env. episode 251.000000, reward total was -19.000000. running mean: -20.523983\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.528743\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.533456\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.528121\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.532840\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.537512\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.542137\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.546715\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.551248\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.555736\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.560178\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.554577\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.549031\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.553540\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.548005\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -20.532525\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.527200\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.531928\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.536608\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.541242\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.545830\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.550372\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.554868\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.559319\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.563726\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.568089\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.572408\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.576684\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.580917\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.585108\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.589257\n",
            "resetting env. episode 282.000000, reward total was -19.000000. running mean: -20.573364\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.577631\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.571854\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.576136\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.580374\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.584571\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.588725\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.592838\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.596909\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.590940\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.595031\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.599080\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.603090\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.607059\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.600988\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.594978\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.599029\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.603038\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.607008\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.610938\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.614828\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.618680\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.622493\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.616268\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.610106\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.614005\n",
            "resetting env. episode 308.000000, reward total was -19.000000. running mean: -20.597865\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.601886\n",
            "resetting env. episode 310.000000, reward total was -18.000000. running mean: -20.575867\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.570108\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.574407\n",
            "resetting env. episode 313.000000, reward total was -19.000000. running mean: -20.558663\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -20.543077\n",
            "resetting env. episode 315.000000, reward total was -18.000000. running mean: -20.517646\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.522469\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.517245\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.502072\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.507052\n",
            "resetting env. episode 320.000000, reward total was -19.000000. running mean: -20.491981\n",
            "resetting env. episode 321.000000, reward total was -18.000000. running mean: -20.467061\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.462391\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.457767\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.453189\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.448657\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.454171\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.459629\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -20.445033\n",
            "resetting env. episode 329.000000, reward total was -17.000000. running mean: -20.410582\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.416476\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.422312\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.428089\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.423808\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.429570\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.435274\n",
            "resetting env. episode 336.000000, reward total was -18.000000. running mean: -20.410921\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.406812\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.412744\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.418616\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.424430\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.430186\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.435884\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.441525\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.437110\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.442739\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.438311\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.443928\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.449489\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.454994\n",
            "resetting env. episode 350.000000, reward total was -19.000000. running mean: -20.440444\n",
            "resetting env. episode 351.000000, reward total was -18.000000. running mean: -20.416040\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.411879\n",
            "resetting env. episode 353.000000, reward total was -18.000000. running mean: -20.387761\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.383883\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.380044\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.376244\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.382481\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.388656\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.384770\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.390922\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.397013\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.393043\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.399112\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.395121\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.401170\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.407158\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.413087\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.418956\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.414766\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.410619\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.416513\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.422347\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.418124\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.413943\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.419803\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.425605\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.431349\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.437036\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.442665\n",
            "resetting env. episode 380.000000, reward total was -19.000000. running mean: -20.428239\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.423956\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.419717\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.425520\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.431264\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.436952\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.432582\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.438256\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.433874\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.439535\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.445140\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.450688\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.456181\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.451620\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.457103\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.462532\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.457907\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.463328\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.468695\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.474008\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.479268\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.484475\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.479630\n",
            "resetting env. episode 403.000000, reward total was -19.000000. running mean: -20.464834\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.470186\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.475484\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.480729\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.485922\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.491062\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.496152\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.491190\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.486278\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.481416\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.466601\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.461935\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.457316\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.462743\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.468115\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.473434\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.478700\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.483913\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.479074\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.484283\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.489440\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.494546\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.499600\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.504604\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.509558\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.504463\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.499418\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.494424\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.489480\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.494585\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -20.479639\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.484843\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.489994\n",
            "resetting env. episode 436.000000, reward total was -19.000000. running mean: -20.475094\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.480343\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.485540\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.480685\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.485878\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.491019\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.486109\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.491248\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.496335\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -20.481372\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.486558\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.491693\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.486776\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.491908\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.496989\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.502019\n",
            "resetting env. episode 452.000000, reward total was -19.000000. running mean: -20.486999\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.492129\n",
            "resetting env. episode 454.000000, reward total was -17.000000. running mean: -20.457207\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.462635\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.458009\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.463429\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.468795\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.474107\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.479366\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -20.464572\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.469926\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.465227\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.470575\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.475869\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.481110\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.486299\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.481436\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.476622\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.481856\n",
            "resetting env. episode 471.000000, reward total was -19.000000. running mean: -20.467037\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.472367\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.477643\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.482867\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.488038\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.483158\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.488326\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.493443\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.498508\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.493523\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.488588\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.493702\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.498765\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.503777\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.498740\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.503752\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -20.488715\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.493828\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.488889\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.484000\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.489160\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.494269\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.499326\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.504333\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -20.489290\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.494397\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.499453\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.504458\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.509414\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.514319\n",
            "CPU times: user 24min 6s, sys: 10min 41s, total: 34min 47s\n",
            "Wall time: 18min 7s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "0a919b00-6335-48d1-bd1e-1353333eb44c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -19.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ4AAAFZCAYAAABpOsHqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAHBklEQVR4nO3dy26cZx3A4ddVShLbOdrOwUSEY0GqYEMRq67Y0B23wQL1KtgiwU0gcQO9AiQ2SIAEEgdRIYWkTuImjp04TkLMpguSQdS/ic1M4udZvvb3zX8k+6f53jkt7O/vD4DirVkPALx+hAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPITkx74A+/fvrAb6t9a2GM96+fHItvH12nrqyujMVTpyfWNzY3x8Pd3QOfZ+X8uXFu+cwrz/Pg4c64e+/+K5+Hw7d1fXU8vHrhlc+zuLE1zn98+xAmmp0PP/p0YZrjpg7HB9+Y/CedpStra2PtwuQfw8Pd3RiO8+P6+vorz3Pjkw3hmFNbX740bn/3K698ntU//OO1D8e0XKoAmXAAmXAAmXAA2dSbo8fN/e3t8WB7Z2L9zPLSuHD27Awm4rAt3bo3lm5Nbmg/unxu7Hzx4gwmml/CcUCb9+6Pv9+4MbF+fX1dON4Q5z6+M9Z/89eJ9U/e+6pwvMSlCpAJB5AJB5AJB5DZHD2gM0uL4+ra2sT62eWlGUwDsyUcB3RpZWVcWlmZ9RgwF1yqAJlwAJlwAJlwAJnN0ZfsPHo0NjY3D/z7S6dOj+WlxSOcCOaPcLzk5u074+btOwf+/evr6+OdpetHOBHMH5cqQCYcQCYcQCYcQGZzFD6zd25xPPjS5NsKHp/3fqSXCQd8ZvPda2Pz3WuzHuO14FIFyIQDyIQDyIQDyN6YzdFHu7tj68Tk3Xn67NmR3u7ekydja3t7Yn137/GR3i7TO7m9+1+/PyWfZ+vgX2b+plnY39+f6sCff3BxugNhxg7zD3fhEM81Cx9+9OlUd+GNecQBB/W6/7PPA3scQCYcQDb1pcr7P/nFYc4BvEam3hzd3Ny0OQqvuZWVlam2fFyqAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwANnUb6v/3a9+dphzvGhhjGuXr4xTJ78w8aN/bmyM3cd7R3fbcIz84Mc/neq4uf3M0e9/59vj7PLyC2v7+/vjt3/807j34MFR3jQcG9N+5qhLFSATDiATDiATDiATDiATDiATDiATDiCb2++Off78+fjX8+cT69O+YA04PHMbjt//+S/jrYXJF7U9efp0BtMA/2luwyEQML/scQCZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcADZiVkPAMfds1Nvj52r5yfWT+w9G0s3742FGcz0eYQDZuzR2tnxtx99b4yFFxOxdOv++NYvfz2jqf43lypAJhxAJhxAJhxANvXm6No77x3mHHBsLV0+O54tf21i/dTFnXHpm3tj7M9gqM+xsL8/3VR3796dw7sDFKurq1M92zv1I46FhXl8dhn4f7DHAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWRTf68KcHx5xAFkwgFkwgFkwgFkwgFkwgFkwgFkwgFkwgFkwgFkwgFkwgFkwgFk/wYVfsQmXdr8tQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "ef7b621e-8ae2-4422-d6c3-650d1ed864a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.980100\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.980299\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.980496\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.980691\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.970884\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.971175\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.971464\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.961749\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.952131\n",
            "resetting env. episode 12.000000, reward total was -19.000000. running mean: -20.932610\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.923284\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.924051\n",
            "resetting env. episode 15.000000, reward total was -18.000000. running mean: -20.894811\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.895863\n",
            "resetting env. episode 17.000000, reward total was -19.000000. running mean: -20.876904\n",
            "resetting env. episode 18.000000, reward total was -19.000000. running mean: -20.858135\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.849554\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.851058\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.842547\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.834122\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.835781\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.837423\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.819049\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.810858\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.812750\n",
            "resetting env. episode 28.000000, reward total was -19.000000. running mean: -20.794622\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.786676\n",
            "resetting env. episode 30.000000, reward total was -19.000000. running mean: -20.768809\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.761121\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -20.743510\n",
            "resetting env. episode 33.000000, reward total was -17.000000. running mean: -20.706075\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.709014\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -20.691924\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -20.675005\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.678255\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.681472\n",
            "resetting env. episode 39.000000, reward total was -18.000000. running mean: -20.654657\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.648111\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.641630\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.635213\n",
            "resetting env. episode 43.000000, reward total was -18.000000. running mean: -20.608861\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.612773\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.616645\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.620478\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.614274\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.618131\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.621950\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.625730\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.629473\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.623178\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.616946\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.610777\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.604669\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.598622\n",
            "resetting env. episode 57.000000, reward total was -19.000000. running mean: -20.582636\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.586810\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.580942\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.585132\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.589281\n",
            "resetting env. episode 62.000000, reward total was -18.000000. running mean: -20.563388\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.567754\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.572077\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -20.556356\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.560792\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.555184\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.559633\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.564036\n",
            "resetting env. episode 70.000000, reward total was -18.000000. running mean: -20.538396\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.543012\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.547582\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.552106\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.556585\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.551019\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.555509\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.549954\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.554454\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.558910\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -20.543321\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.547887\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.552409\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.546884\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.551416\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.555901\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.550342\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.544839\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.549391\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.553897\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.558358\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.552774\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.557246\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.551674\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.556157\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -20.540596\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.545190\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.549738\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.554240\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.558698\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.563111\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.567480\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.571805\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.576087\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.570326\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.574623\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.578877\n",
            "resetting env. episode 107.000000, reward total was -19.000000. running mean: -20.563088\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.557457\n",
            "resetting env. episode 109.000000, reward total was -18.000000. running mean: -20.531883\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.536564\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.541198\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.545786\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.550328\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.554825\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.559277\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.563684\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.568047\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.572367\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.576643\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.570877\n",
            "resetting env. episode 121.000000, reward total was -19.000000. running mean: -20.555168\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.559616\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.564020\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.558380\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.562796\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.557168\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.561596\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.565980\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.570321\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.574617\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.578871\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.573082\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.567352\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.571678\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.575961\n",
            "resetting env. episode 136.000000, reward total was -19.000000. running mean: -20.560202\n",
            "resetting env. episode 137.000000, reward total was -19.000000. running mean: -20.544600\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.549154\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.553662\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.558126\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.562544\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.556919\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.551350\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.555836\n",
            "resetting env. episode 145.000000, reward total was -19.000000. running mean: -20.540278\n",
            "resetting env. episode 146.000000, reward total was -19.000000. running mean: -20.524875\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.529626\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.534330\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.528987\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.523697\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.518460\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.523275\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.528042\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.522762\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.527534\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.532259\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.536937\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.541567\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.546151\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.550690\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.555183\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.559631\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.564035\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.568395\n",
            "resetting env. episode 165.000000, reward total was -19.000000. running mean: -20.552711\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.547184\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.551712\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.546195\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.550733\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.555225\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.549673\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.554176\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.548635\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.543148\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.547717\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.532240\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.536917\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.541548\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.546133\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.540671\n",
            "resetting env. episode 181.000000, reward total was -19.000000. running mean: -20.525264\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.530012\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.524712\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.529465\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -20.514170\n",
            "resetting env. episode 186.000000, reward total was -19.000000. running mean: -20.499028\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.494038\n",
            "resetting env. episode 188.000000, reward total was -19.000000. running mean: -20.479098\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.484307\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.479464\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.474669\n",
            "resetting env. episode 192.000000, reward total was -19.000000. running mean: -20.459922\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.465323\n",
            "resetting env. episode 194.000000, reward total was -18.000000. running mean: -20.440670\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.446263\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.451800\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.457282\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.462710\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.458083\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.463502\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.468867\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.474178\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.479436\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.474642\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.479895\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.485096\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.490246\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.485343\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.490490\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.485585\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.490729\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.495822\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.500863\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.505855\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.510796\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.505688\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.510631\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.515525\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.520370\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.525166\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.519914\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.524715\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.519468\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.524273\n",
            "resetting env. episode 225.000000, reward total was -19.000000. running mean: -20.509031\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -20.493940\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.499001\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.504011\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.498971\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.503981\n",
            "resetting env. episode 231.000000, reward total was -19.000000. running mean: -20.488941\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.494052\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.499111\n",
            "resetting env. episode 234.000000, reward total was -19.000000. running mean: -20.484120\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.489279\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.484386\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -20.469542\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.474847\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.480099\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.485298\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.490445\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.495540\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.500585\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.505579\n",
            "resetting env. episode 245.000000, reward total was -19.000000. running mean: -20.490523\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.485618\n",
            "resetting env. episode 247.000000, reward total was -17.000000. running mean: -20.450762\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.456254\n",
            "resetting env. episode 249.000000, reward total was -19.000000. running mean: -20.441692\n",
            "resetting env. episode 250.000000, reward total was -18.000000. running mean: -20.417275\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.423102\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.428871\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.424582\n",
            "resetting env. episode 254.000000, reward total was -19.000000. running mean: -20.410336\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.416233\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.422071\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.427850\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.423571\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.419336\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.405142\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.411091\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.416980\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.422810\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.428582\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.434296\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.429953\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.435654\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.441297\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.446884\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.452415\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.447891\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.453412\n",
            "resetting env. episode 273.000000, reward total was -19.000000. running mean: -20.438878\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.444489\n",
            "resetting env. episode 275.000000, reward total was -19.000000. running mean: -20.430045\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.425744\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.431487\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.437172\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.432800\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.438472\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.434087\n",
            "resetting env. episode 282.000000, reward total was -18.000000. running mean: -20.409747\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.415649\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.421493\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.427278\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.433005\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.438675\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.444288\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.449845\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.455347\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.460793\n",
            "resetting env. episode 292.000000, reward total was -18.000000. running mean: -20.436185\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.441823\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.437405\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.433031\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.438701\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.444314\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.439871\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.435472\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.441117\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.446706\n",
            "resetting env. episode 302.000000, reward total was -19.000000. running mean: -20.432239\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.437917\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.433538\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.429202\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.434910\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.440561\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.446155\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.451694\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.457177\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.452605\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.458079\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.463498\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.468863\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.474175\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.479433\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.474639\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.479892\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.475093\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.480342\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.485539\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.490684\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.485777\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.490919\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.496010\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.491050\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.486139\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.491278\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.496365\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.501401\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.506387\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -20.491323\n",
            "resetting env. episode 333.000000, reward total was -18.000000. running mean: -20.466410\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.461746\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.457129\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.462557\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.467932\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.463252\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.468620\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.473934\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.469194\n",
            "resetting env. episode 342.000000, reward total was -19.000000. running mean: -20.454502\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.449957\n",
            "resetting env. episode 344.000000, reward total was -16.000000. running mean: -20.405458\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.401403\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.407389\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.393315\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.399382\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.395388\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.401435\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.387420\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.393546\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.399611\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -20.385614\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.391758\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.397841\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.393862\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.399924\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.405924\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.411865\n",
            "resetting env. episode 361.000000, reward total was -18.000000. running mean: -20.387747\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.383869\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.390030\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.396130\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.402169\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.408147\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.404066\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -20.390025\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.376125\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.382363\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.378540\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.374754\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.371007\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.377297\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.383524\n",
            "resetting env. episode 376.000000, reward total was -17.000000. running mean: -20.349689\n",
            "resetting env. episode 377.000000, reward total was -19.000000. running mean: -20.336192\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -20.322830\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.329601\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.336305\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.342942\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.339513\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.336118\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.342757\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.349329\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.355836\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.362277\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.368655\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.364968\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.361318\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.367705\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.374028\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.380288\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.376485\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.382720\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.388893\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.385004\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.391154\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.387243\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.393370\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.379436\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.375642\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.371886\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.378167\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.374385\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.380641\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.386835\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.392966\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.399037\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.405046\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.410996\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.416886\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.402717\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.398690\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.394703\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.400756\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.396748\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.402781\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.408753\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.414666\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.410519\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.416414\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.422250\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.428027\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.433747\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.429409\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.435115\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.440764\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.446357\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.441893\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.447474\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.442999\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.448569\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.454084\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.449543\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.455047\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.450497\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.445992\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.441532\n",
            "resetting env. episode 440.000000, reward total was -19.000000. running mean: -20.427117\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.422846\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.428617\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.414331\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.420188\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -20.405986\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.411926\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.397807\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.403829\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.409790\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.415692\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.421535\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.427320\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.433047\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.428716\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.434429\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.430085\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.435784\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.441426\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.447012\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.452542\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.458016\n",
            "resetting env. episode 462.000000, reward total was -18.000000. running mean: -20.433436\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.439102\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.444711\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.450264\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.445761\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.451304\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.456790\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.452223\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.447700\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.443223\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.448791\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.434303\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.429960\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.425661\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.431404\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.437090\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.442719\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -20.428292\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.434009\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.439669\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.435272\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.440919\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -20.426510\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.432245\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.427923\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.433643\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.439307\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.444914\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.450465\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.455960\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.451401\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -20.436887\n",
            "resetting env. episode 494.000000, reward total was -19.000000. running mean: -20.422518\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.428293\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.414010\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.409869\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.415771\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.421613\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.417397\n",
            "resetting env. episode 501.000000, reward total was -21.000000. running mean: -20.423223\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.428991\n",
            "resetting env. episode 503.000000, reward total was -19.000000. running mean: -20.414701\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -20.410554\n",
            "resetting env. episode 505.000000, reward total was -19.000000. running mean: -20.396448\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.402484\n",
            "resetting env. episode 507.000000, reward total was -21.000000. running mean: -20.408459\n",
            "resetting env. episode 508.000000, reward total was -21.000000. running mean: -20.414374\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.420231\n",
            "resetting env. episode 510.000000, reward total was -21.000000. running mean: -20.426028\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.431768\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.437450\n",
            "resetting env. episode 513.000000, reward total was -19.000000. running mean: -20.423076\n",
            "resetting env. episode 514.000000, reward total was -20.000000. running mean: -20.418845\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.424657\n",
            "resetting env. episode 516.000000, reward total was -20.000000. running mean: -20.420410\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.426206\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.431944\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.437624\n",
            "resetting env. episode 520.000000, reward total was -20.000000. running mean: -20.433248\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.438916\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.444527\n",
            "resetting env. episode 523.000000, reward total was -19.000000. running mean: -20.430081\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -20.425781\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.421523\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -20.427308\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.433034\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.438704\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.444317\n",
            "resetting env. episode 530.000000, reward total was -19.000000. running mean: -20.429874\n",
            "resetting env. episode 531.000000, reward total was -20.000000. running mean: -20.425575\n",
            "resetting env. episode 532.000000, reward total was -20.000000. running mean: -20.421319\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.427106\n",
            "resetting env. episode 534.000000, reward total was -19.000000. running mean: -20.412835\n",
            "resetting env. episode 535.000000, reward total was -21.000000. running mean: -20.418707\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.424520\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.430275\n",
            "resetting env. episode 538.000000, reward total was -20.000000. running mean: -20.425972\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -20.431712\n",
            "resetting env. episode 540.000000, reward total was -20.000000. running mean: -20.427395\n",
            "resetting env. episode 541.000000, reward total was -20.000000. running mean: -20.423121\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.428890\n",
            "resetting env. episode 543.000000, reward total was -19.000000. running mean: -20.414601\n",
            "resetting env. episode 544.000000, reward total was -20.000000. running mean: -20.410455\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -20.416350\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.422187\n",
            "resetting env. episode 547.000000, reward total was -20.000000. running mean: -20.417965\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -20.423785\n",
            "resetting env. episode 549.000000, reward total was -20.000000. running mean: -20.419547\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.425352\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.431098\n",
            "resetting env. episode 552.000000, reward total was -19.000000. running mean: -20.416787\n",
            "resetting env. episode 553.000000, reward total was -21.000000. running mean: -20.422620\n",
            "resetting env. episode 554.000000, reward total was -19.000000. running mean: -20.408393\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.414309\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.410166\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.416065\n",
            "resetting env. episode 558.000000, reward total was -20.000000. running mean: -20.411904\n",
            "resetting env. episode 559.000000, reward total was -20.000000. running mean: -20.407785\n",
            "resetting env. episode 560.000000, reward total was -19.000000. running mean: -20.393707\n",
            "resetting env. episode 561.000000, reward total was -21.000000. running mean: -20.399770\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.405772\n",
            "resetting env. episode 563.000000, reward total was -20.000000. running mean: -20.401715\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.407698\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.413621\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.419484\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.425289\n",
            "resetting env. episode 568.000000, reward total was -21.000000. running mean: -20.431037\n",
            "resetting env. episode 569.000000, reward total was -20.000000. running mean: -20.426726\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.432459\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -20.438134\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -20.433753\n",
            "resetting env. episode 573.000000, reward total was -21.000000. running mean: -20.439416\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.445021\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.450571\n",
            "resetting env. episode 576.000000, reward total was -18.000000. running mean: -20.426065\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -20.431805\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.437487\n",
            "resetting env. episode 579.000000, reward total was -19.000000. running mean: -20.423112\n",
            "resetting env. episode 580.000000, reward total was -21.000000. running mean: -20.428881\n",
            "resetting env. episode 581.000000, reward total was -20.000000. running mean: -20.424592\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.430346\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -20.436043\n",
            "resetting env. episode 584.000000, reward total was -20.000000. running mean: -20.431682\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.437365\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.442992\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.448562\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.454076\n",
            "resetting env. episode 589.000000, reward total was -19.000000. running mean: -20.439535\n",
            "resetting env. episode 590.000000, reward total was -20.000000. running mean: -20.435140\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.440789\n",
            "resetting env. episode 592.000000, reward total was -19.000000. running mean: -20.426381\n",
            "resetting env. episode 593.000000, reward total was -20.000000. running mean: -20.422117\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -20.417896\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.423717\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -20.429480\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.435185\n",
            "resetting env. episode 598.000000, reward total was -19.000000. running mean: -20.420833\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.426625\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.432358\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -20.438035\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -20.443654\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.449218\n",
            "resetting env. episode 604.000000, reward total was -20.000000. running mean: -20.444726\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.450278\n",
            "resetting env. episode 606.000000, reward total was -21.000000. running mean: -20.455776\n",
            "resetting env. episode 607.000000, reward total was -20.000000. running mean: -20.451218\n",
            "resetting env. episode 608.000000, reward total was -19.000000. running mean: -20.436706\n",
            "resetting env. episode 609.000000, reward total was -19.000000. running mean: -20.422339\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.428115\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.433834\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.439496\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -20.445101\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.450650\n",
            "resetting env. episode 615.000000, reward total was -20.000000. running mean: -20.446143\n",
            "resetting env. episode 616.000000, reward total was -20.000000. running mean: -20.441682\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.447265\n",
            "resetting env. episode 618.000000, reward total was -20.000000. running mean: -20.442792\n",
            "resetting env. episode 619.000000, reward total was -21.000000. running mean: -20.448365\n",
            "resetting env. episode 620.000000, reward total was -20.000000. running mean: -20.443881\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -20.439442\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.445048\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.450597\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.456091\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.461530\n",
            "resetting env. episode 626.000000, reward total was -19.000000. running mean: -20.446915\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -20.452446\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.457921\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.463342\n",
            "resetting env. episode 630.000000, reward total was -20.000000. running mean: -20.458709\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.464122\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.469480\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -20.474786\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.480038\n",
            "resetting env. episode 635.000000, reward total was -19.000000. running mean: -20.465237\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.470585\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -20.475879\n",
            "resetting env. episode 638.000000, reward total was -19.000000. running mean: -20.461120\n",
            "resetting env. episode 639.000000, reward total was -20.000000. running mean: -20.456509\n",
            "resetting env. episode 640.000000, reward total was -19.000000. running mean: -20.441944\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.447525\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -20.453049\n",
            "resetting env. episode 643.000000, reward total was -20.000000. running mean: -20.448519\n",
            "resetting env. episode 644.000000, reward total was -20.000000. running mean: -20.444034\n",
            "resetting env. episode 645.000000, reward total was -20.000000. running mean: -20.439593\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -20.445197\n",
            "resetting env. episode 647.000000, reward total was -19.000000. running mean: -20.430745\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.436438\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -20.442074\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.447653\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.453176\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.458645\n",
            "resetting env. episode 653.000000, reward total was -20.000000. running mean: -20.454058\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -20.459518\n",
            "resetting env. episode 655.000000, reward total was -19.000000. running mean: -20.444922\n",
            "resetting env. episode 656.000000, reward total was -21.000000. running mean: -20.450473\n",
            "resetting env. episode 657.000000, reward total was -20.000000. running mean: -20.445968\n",
            "resetting env. episode 658.000000, reward total was -20.000000. running mean: -20.441509\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -20.447094\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -20.442623\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.448197\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.443715\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -20.449277\n",
            "resetting env. episode 664.000000, reward total was -20.000000. running mean: -20.444785\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.450337\n",
            "resetting env. episode 666.000000, reward total was -20.000000. running mean: -20.445833\n",
            "resetting env. episode 667.000000, reward total was -19.000000. running mean: -20.431375\n",
            "resetting env. episode 668.000000, reward total was -20.000000. running mean: -20.427061\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.432791\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.438463\n",
            "resetting env. episode 671.000000, reward total was -20.000000. running mean: -20.434078\n",
            "resetting env. episode 672.000000, reward total was -20.000000. running mean: -20.429737\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -20.435440\n",
            "resetting env. episode 674.000000, reward total was -18.000000. running mean: -20.411086\n",
            "resetting env. episode 675.000000, reward total was -18.000000. running mean: -20.386975\n",
            "resetting env. episode 676.000000, reward total was -21.000000. running mean: -20.393105\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.389174\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.395282\n",
            "resetting env. episode 679.000000, reward total was -21.000000. running mean: -20.401329\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -20.407316\n",
            "resetting env. episode 681.000000, reward total was -19.000000. running mean: -20.393243\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -20.399311\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -20.395317\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -20.391364\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -20.397451\n",
            "resetting env. episode 686.000000, reward total was -19.000000. running mean: -20.383476\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -20.379641\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.385845\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.391986\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.388067\n",
            "resetting env. episode 691.000000, reward total was -20.000000. running mean: -20.384186\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.390344\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.396441\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.402476\n",
            "resetting env. episode 695.000000, reward total was -20.000000. running mean: -20.398451\n",
            "resetting env. episode 696.000000, reward total was -20.000000. running mean: -20.394467\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.400522\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -20.406517\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.412452\n",
            "resetting env. episode 700.000000, reward total was -19.000000. running mean: -20.398327\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.404344\n",
            "resetting env. episode 702.000000, reward total was -20.000000. running mean: -20.400301\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -20.406298\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -20.402235\n",
            "resetting env. episode 705.000000, reward total was -21.000000. running mean: -20.408212\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -20.414130\n",
            "resetting env. episode 707.000000, reward total was -20.000000. running mean: -20.409989\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -20.405889\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.411830\n",
            "resetting env. episode 710.000000, reward total was -20.000000. running mean: -20.407712\n",
            "resetting env. episode 711.000000, reward total was -21.000000. running mean: -20.413635\n",
            "resetting env. episode 712.000000, reward total was -21.000000. running mean: -20.419498\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.425303\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -20.421050\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.426840\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.432571\n",
            "resetting env. episode 717.000000, reward total was -20.000000. running mean: -20.428246\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -20.423963\n",
            "resetting env. episode 719.000000, reward total was -20.000000. running mean: -20.419724\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.425526\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.431271\n",
            "resetting env. episode 722.000000, reward total was -18.000000. running mean: -20.406958\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.412889\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -20.418760\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.424572\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -20.430327\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.436023\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -20.441663\n",
            "resetting env. episode 729.000000, reward total was -20.000000. running mean: -20.437246\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -20.442874\n",
            "resetting env. episode 731.000000, reward total was -21.000000. running mean: -20.448445\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.453961\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.459421\n",
            "resetting env. episode 734.000000, reward total was -21.000000. running mean: -20.464827\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.470179\n",
            "resetting env. episode 736.000000, reward total was -21.000000. running mean: -20.475477\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.480722\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.485915\n",
            "resetting env. episode 739.000000, reward total was -18.000000. running mean: -20.461056\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.466445\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.471781\n",
            "resetting env. episode 742.000000, reward total was -20.000000. running mean: -20.467063\n",
            "resetting env. episode 743.000000, reward total was -21.000000. running mean: -20.472392\n",
            "resetting env. episode 744.000000, reward total was -19.000000. running mean: -20.457668\n",
            "resetting env. episode 745.000000, reward total was -20.000000. running mean: -20.453092\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -20.448561\n",
            "resetting env. episode 747.000000, reward total was -20.000000. running mean: -20.444075\n",
            "resetting env. episode 748.000000, reward total was -20.000000. running mean: -20.439634\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.445238\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.450786\n",
            "resetting env. episode 751.000000, reward total was -19.000000. running mean: -20.436278\n",
            "resetting env. episode 752.000000, reward total was -21.000000. running mean: -20.441915\n",
            "resetting env. episode 753.000000, reward total was -20.000000. running mean: -20.437496\n",
            "resetting env. episode 754.000000, reward total was -20.000000. running mean: -20.433121\n",
            "resetting env. episode 755.000000, reward total was -19.000000. running mean: -20.418790\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -20.424602\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.430356\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.436052\n",
            "resetting env. episode 759.000000, reward total was -19.000000. running mean: -20.421692\n",
            "resetting env. episode 760.000000, reward total was -20.000000. running mean: -20.417475\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.423300\n",
            "resetting env. episode 762.000000, reward total was -17.000000. running mean: -20.389067\n",
            "resetting env. episode 763.000000, reward total was -21.000000. running mean: -20.395176\n",
            "resetting env. episode 764.000000, reward total was -19.000000. running mean: -20.381225\n",
            "resetting env. episode 765.000000, reward total was -20.000000. running mean: -20.377412\n",
            "resetting env. episode 766.000000, reward total was -20.000000. running mean: -20.373638\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -20.379902\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.386103\n",
            "resetting env. episode 769.000000, reward total was -20.000000. running mean: -20.382242\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.388419\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -20.394535\n",
            "resetting env. episode 772.000000, reward total was -19.000000. running mean: -20.380590\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -20.386784\n",
            "resetting env. episode 774.000000, reward total was -19.000000. running mean: -20.372916\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -20.379187\n",
            "resetting env. episode 776.000000, reward total was -21.000000. running mean: -20.385395\n",
            "resetting env. episode 777.000000, reward total was -20.000000. running mean: -20.381541\n",
            "resetting env. episode 778.000000, reward total was -21.000000. running mean: -20.387726\n",
            "resetting env. episode 779.000000, reward total was -21.000000. running mean: -20.393849\n",
            "resetting env. episode 780.000000, reward total was -19.000000. running mean: -20.379910\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -20.386111\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -20.382250\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -20.388427\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -20.394543\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.400598\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -20.406592\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.412526\n",
            "resetting env. episode 788.000000, reward total was -20.000000. running mean: -20.408400\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -20.414316\n",
            "resetting env. episode 790.000000, reward total was -21.000000. running mean: -20.420173\n",
            "resetting env. episode 791.000000, reward total was -19.000000. running mean: -20.405972\n",
            "resetting env. episode 792.000000, reward total was -19.000000. running mean: -20.391912\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.397993\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.404013\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.409973\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.415873\n",
            "resetting env. episode 797.000000, reward total was -20.000000. running mean: -20.411714\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -20.417597\n",
            "resetting env. episode 799.000000, reward total was -18.000000. running mean: -20.393421\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.399487\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -20.405492\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.411437\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.417323\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -20.423150\n",
            "resetting env. episode 805.000000, reward total was -20.000000. running mean: -20.418918\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -20.424729\n",
            "resetting env. episode 807.000000, reward total was -18.000000. running mean: -20.400482\n",
            "resetting env. episode 808.000000, reward total was -19.000000. running mean: -20.386477\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.392612\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.398686\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -20.394699\n",
            "resetting env. episode 812.000000, reward total was -20.000000. running mean: -20.390752\n",
            "resetting env. episode 813.000000, reward total was -20.000000. running mean: -20.386844\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -20.392976\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.399046\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.405056\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.411005\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.416895\n",
            "resetting env. episode 819.000000, reward total was -20.000000. running mean: -20.412726\n",
            "resetting env. episode 820.000000, reward total was -21.000000. running mean: -20.418599\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.424413\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.430169\n",
            "resetting env. episode 823.000000, reward total was -20.000000. running mean: -20.425867\n",
            "resetting env. episode 824.000000, reward total was -20.000000. running mean: -20.421609\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -20.427392\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.433119\n",
            "resetting env. episode 827.000000, reward total was -21.000000. running mean: -20.438787\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.444399\n",
            "resetting env. episode 829.000000, reward total was -20.000000. running mean: -20.439955\n",
            "resetting env. episode 830.000000, reward total was -20.000000. running mean: -20.435556\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -20.441200\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -20.446788\n",
            "resetting env. episode 833.000000, reward total was -20.000000. running mean: -20.442320\n",
            "resetting env. episode 834.000000, reward total was -21.000000. running mean: -20.447897\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.453418\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -20.458884\n",
            "resetting env. episode 837.000000, reward total was -19.000000. running mean: -20.444295\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.449852\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.455354\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -20.460800\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.466192\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.471530\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -20.476815\n",
            "resetting env. episode 844.000000, reward total was -17.000000. running mean: -20.442047\n",
            "resetting env. episode 845.000000, reward total was -20.000000. running mean: -20.437626\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -20.443250\n",
            "resetting env. episode 847.000000, reward total was -20.000000. running mean: -20.438818\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -20.444429\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -20.449985\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.455485\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -20.460930\n",
            "resetting env. episode 852.000000, reward total was -20.000000. running mean: -20.456321\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -20.461758\n",
            "resetting env. episode 854.000000, reward total was -20.000000. running mean: -20.457140\n",
            "resetting env. episode 855.000000, reward total was -19.000000. running mean: -20.442569\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.448143\n",
            "resetting env. episode 857.000000, reward total was -19.000000. running mean: -20.433662\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -20.439325\n",
            "resetting env. episode 859.000000, reward total was -21.000000. running mean: -20.444932\n",
            "resetting env. episode 860.000000, reward total was -20.000000. running mean: -20.440483\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -20.446078\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -20.451617\n",
            "resetting env. episode 863.000000, reward total was -20.000000. running mean: -20.447101\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.452630\n",
            "resetting env. episode 865.000000, reward total was -20.000000. running mean: -20.448104\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.453623\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.459086\n",
            "resetting env. episode 868.000000, reward total was -20.000000. running mean: -20.454495\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.459950\n",
            "resetting env. episode 870.000000, reward total was -21.000000. running mean: -20.465351\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -20.470697\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -20.475990\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.481231\n",
            "resetting env. episode 874.000000, reward total was -17.000000. running mean: -20.446418\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -20.441954\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -20.447535\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.453059\n",
            "resetting env. episode 878.000000, reward total was -19.000000. running mean: -20.438529\n",
            "resetting env. episode 879.000000, reward total was -19.000000. running mean: -20.424143\n",
            "resetting env. episode 880.000000, reward total was -20.000000. running mean: -20.419902\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.425703\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -20.421446\n",
            "resetting env. episode 883.000000, reward total was -21.000000. running mean: -20.427231\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.432959\n",
            "resetting env. episode 885.000000, reward total was -19.000000. running mean: -20.418629\n",
            "resetting env. episode 886.000000, reward total was -20.000000. running mean: -20.414443\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -20.420299\n",
            "resetting env. episode 888.000000, reward total was -20.000000. running mean: -20.416096\n",
            "resetting env. episode 889.000000, reward total was -20.000000. running mean: -20.411935\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.417815\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.423637\n",
            "resetting env. episode 892.000000, reward total was -20.000000. running mean: -20.419401\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -20.425207\n",
            "resetting env. episode 894.000000, reward total was -20.000000. running mean: -20.420955\n",
            "resetting env. episode 895.000000, reward total was -21.000000. running mean: -20.426745\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.432478\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.438153\n",
            "resetting env. episode 898.000000, reward total was -20.000000. running mean: -20.433772\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -20.439434\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.445039\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.450589\n",
            "resetting env. episode 902.000000, reward total was -20.000000. running mean: -20.446083\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.451622\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -20.457106\n",
            "resetting env. episode 905.000000, reward total was -20.000000. running mean: -20.452535\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.458010\n",
            "resetting env. episode 907.000000, reward total was -20.000000. running mean: -20.453430\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.458895\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -20.454306\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -20.459763\n",
            "resetting env. episode 911.000000, reward total was -21.000000. running mean: -20.465166\n",
            "resetting env. episode 912.000000, reward total was -16.000000. running mean: -20.420514\n",
            "resetting env. episode 913.000000, reward total was -18.000000. running mean: -20.396309\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -20.402346\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -20.398322\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.404339\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.410296\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -20.416193\n",
            "resetting env. episode 919.000000, reward total was -21.000000. running mean: -20.422031\n",
            "resetting env. episode 920.000000, reward total was -20.000000. running mean: -20.417811\n",
            "resetting env. episode 921.000000, reward total was -20.000000. running mean: -20.413632\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -20.419496\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -20.425301\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.431048\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -20.436738\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -20.442370\n",
            "resetting env. episode 927.000000, reward total was -19.000000. running mean: -20.427947\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.433667\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -20.439330\n",
            "resetting env. episode 930.000000, reward total was -19.000000. running mean: -20.424937\n",
            "resetting env. episode 931.000000, reward total was -20.000000. running mean: -20.420688\n",
            "resetting env. episode 932.000000, reward total was -20.000000. running mean: -20.416481\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -20.422316\n",
            "resetting env. episode 934.000000, reward total was -21.000000. running mean: -20.428093\n",
            "resetting env. episode 935.000000, reward total was -20.000000. running mean: -20.423812\n",
            "resetting env. episode 936.000000, reward total was -17.000000. running mean: -20.389574\n",
            "resetting env. episode 937.000000, reward total was -21.000000. running mean: -20.395678\n",
            "resetting env. episode 938.000000, reward total was -21.000000. running mean: -20.401721\n",
            "resetting env. episode 939.000000, reward total was -19.000000. running mean: -20.387704\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -20.393827\n",
            "resetting env. episode 941.000000, reward total was -20.000000. running mean: -20.389889\n",
            "resetting env. episode 942.000000, reward total was -21.000000. running mean: -20.395990\n",
            "resetting env. episode 943.000000, reward total was -18.000000. running mean: -20.372030\n",
            "resetting env. episode 944.000000, reward total was -21.000000. running mean: -20.378310\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -20.384527\n",
            "resetting env. episode 946.000000, reward total was -20.000000. running mean: -20.380681\n",
            "resetting env. episode 947.000000, reward total was -21.000000. running mean: -20.386875\n",
            "resetting env. episode 948.000000, reward total was -20.000000. running mean: -20.383006\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -20.389176\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.395284\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.401331\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -20.407318\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.413245\n",
            "resetting env. episode 954.000000, reward total was -20.000000. running mean: -20.409112\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -20.415021\n",
            "resetting env. episode 956.000000, reward total was -20.000000. running mean: -20.410871\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -20.416762\n",
            "resetting env. episode 958.000000, reward total was -19.000000. running mean: -20.402595\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.408569\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -20.414483\n",
            "resetting env. episode 961.000000, reward total was -21.000000. running mean: -20.420338\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -20.426135\n",
            "resetting env. episode 963.000000, reward total was -20.000000. running mean: -20.421873\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -20.427655\n",
            "resetting env. episode 965.000000, reward total was -20.000000. running mean: -20.423378\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.429144\n",
            "resetting env. episode 967.000000, reward total was -21.000000. running mean: -20.434853\n",
            "resetting env. episode 968.000000, reward total was -20.000000. running mean: -20.430504\n",
            "resetting env. episode 969.000000, reward total was -21.000000. running mean: -20.436199\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.441837\n",
            "resetting env. episode 971.000000, reward total was -20.000000. running mean: -20.437419\n",
            "resetting env. episode 972.000000, reward total was -20.000000. running mean: -20.433045\n",
            "resetting env. episode 973.000000, reward total was -19.000000. running mean: -20.418714\n",
            "resetting env. episode 974.000000, reward total was -21.000000. running mean: -20.424527\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.430282\n",
            "resetting env. episode 976.000000, reward total was -20.000000. running mean: -20.425979\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -20.431719\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.437402\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.443028\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -20.448598\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.454112\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.459571\n",
            "resetting env. episode 983.000000, reward total was -19.000000. running mean: -20.444975\n",
            "resetting env. episode 984.000000, reward total was -21.000000. running mean: -20.450525\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.456020\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -20.461460\n",
            "resetting env. episode 987.000000, reward total was -20.000000. running mean: -20.456845\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -20.462277\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -20.467654\n",
            "resetting env. episode 990.000000, reward total was -20.000000. running mean: -20.462977\n",
            "resetting env. episode 991.000000, reward total was -21.000000. running mean: -20.468348\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -20.473664\n",
            "resetting env. episode 993.000000, reward total was -20.000000. running mean: -20.468928\n",
            "resetting env. episode 994.000000, reward total was -20.000000. running mean: -20.464238\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.469596\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -20.474900\n",
            "resetting env. episode 997.000000, reward total was -19.000000. running mean: -20.460151\n",
            "resetting env. episode 998.000000, reward total was -20.000000. running mean: -20.455549\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -20.460994\n",
            "resetting env. episode 1000.000000, reward total was -19.000000. running mean: -20.446384\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -20.451920\n",
            "resetting env. episode 1002.000000, reward total was -21.000000. running mean: -20.457401\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.462827\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -20.468199\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.473517\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.478781\n",
            "resetting env. episode 1007.000000, reward total was -20.000000. running mean: -20.473994\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -20.479254\n",
            "resetting env. episode 1009.000000, reward total was -20.000000. running mean: -20.474461\n",
            "resetting env. episode 1010.000000, reward total was -20.000000. running mean: -20.469717\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.475019\n",
            "resetting env. episode 1012.000000, reward total was -20.000000. running mean: -20.470269\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.475567\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.480811\n",
            "resetting env. episode 1015.000000, reward total was -18.000000. running mean: -20.456003\n",
            "resetting env. episode 1016.000000, reward total was -21.000000. running mean: -20.461443\n",
            "resetting env. episode 1017.000000, reward total was -21.000000. running mean: -20.466828\n",
            "resetting env. episode 1018.000000, reward total was -20.000000. running mean: -20.462160\n",
            "resetting env. episode 1019.000000, reward total was -21.000000. running mean: -20.467538\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.472863\n",
            "resetting env. episode 1021.000000, reward total was -21.000000. running mean: -20.478134\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -20.483353\n",
            "resetting env. episode 1023.000000, reward total was -20.000000. running mean: -20.478520\n",
            "resetting env. episode 1024.000000, reward total was -20.000000. running mean: -20.473734\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -20.478997\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.484207\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -20.489365\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.494471\n",
            "resetting env. episode 1029.000000, reward total was -20.000000. running mean: -20.489527\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -20.484631\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.489785\n",
            "resetting env. episode 1032.000000, reward total was -19.000000. running mean: -20.474887\n",
            "resetting env. episode 1033.000000, reward total was -21.000000. running mean: -20.480138\n",
            "resetting env. episode 1034.000000, reward total was -20.000000. running mean: -20.475337\n",
            "resetting env. episode 1035.000000, reward total was -20.000000. running mean: -20.470584\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -20.475878\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.481119\n",
            "resetting env. episode 1038.000000, reward total was -21.000000. running mean: -20.486308\n",
            "resetting env. episode 1039.000000, reward total was -20.000000. running mean: -20.481445\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.486630\n",
            "resetting env. episode 1041.000000, reward total was -21.000000. running mean: -20.491764\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.496846\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.501878\n",
            "resetting env. episode 1044.000000, reward total was -20.000000. running mean: -20.496859\n",
            "resetting env. episode 1045.000000, reward total was -20.000000. running mean: -20.491890\n",
            "resetting env. episode 1046.000000, reward total was -19.000000. running mean: -20.476972\n",
            "resetting env. episode 1047.000000, reward total was -19.000000. running mean: -20.462202\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -20.467580\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -20.462904\n",
            "resetting env. episode 1050.000000, reward total was -19.000000. running mean: -20.448275\n",
            "resetting env. episode 1051.000000, reward total was -20.000000. running mean: -20.443792\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -20.439354\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -20.444961\n",
            "resetting env. episode 1054.000000, reward total was -20.000000. running mean: -20.440511\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -20.446106\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -20.451645\n",
            "resetting env. episode 1057.000000, reward total was -18.000000. running mean: -20.427129\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -20.432857\n",
            "resetting env. episode 1059.000000, reward total was -20.000000. running mean: -20.428529\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -20.434243\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -20.439901\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.445502\n",
            "resetting env. episode 1063.000000, reward total was -21.000000. running mean: -20.451047\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -20.456536\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.461971\n",
            "resetting env. episode 1066.000000, reward total was -21.000000. running mean: -20.467351\n",
            "resetting env. episode 1067.000000, reward total was -21.000000. running mean: -20.472678\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -20.467951\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -20.473272\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -20.478539\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -20.483753\n",
            "resetting env. episode 1072.000000, reward total was -20.000000. running mean: -20.478916\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -20.484127\n",
            "resetting env. episode 1074.000000, reward total was -20.000000. running mean: -20.479286\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -20.484493\n",
            "resetting env. episode 1076.000000, reward total was -19.000000. running mean: -20.469648\n",
            "resetting env. episode 1077.000000, reward total was -19.000000. running mean: -20.454951\n",
            "resetting env. episode 1078.000000, reward total was -20.000000. running mean: -20.450402\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.455898\n",
            "resetting env. episode 1080.000000, reward total was -21.000000. running mean: -20.461339\n",
            "resetting env. episode 1081.000000, reward total was -20.000000. running mean: -20.456725\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -20.462158\n",
            "resetting env. episode 1083.000000, reward total was -21.000000. running mean: -20.467537\n",
            "resetting env. episode 1084.000000, reward total was -20.000000. running mean: -20.462861\n",
            "resetting env. episode 1085.000000, reward total was -20.000000. running mean: -20.458233\n",
            "resetting env. episode 1086.000000, reward total was -20.000000. running mean: -20.453650\n",
            "resetting env. episode 1087.000000, reward total was -19.000000. running mean: -20.439114\n",
            "resetting env. episode 1088.000000, reward total was -19.000000. running mean: -20.424723\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -20.420475\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.426271\n",
            "resetting env. episode 1091.000000, reward total was -20.000000. running mean: -20.422008\n",
            "resetting env. episode 1092.000000, reward total was -21.000000. running mean: -20.427788\n",
            "resetting env. episode 1093.000000, reward total was -20.000000. running mean: -20.423510\n",
            "resetting env. episode 1094.000000, reward total was -21.000000. running mean: -20.429275\n",
            "resetting env. episode 1095.000000, reward total was -20.000000. running mean: -20.424982\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -20.430732\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -20.436425\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -20.442061\n",
            "resetting env. episode 1099.000000, reward total was -19.000000. running mean: -20.427640\n",
            "resetting env. episode 1100.000000, reward total was -20.000000. running mean: -20.423364\n",
            "resetting env. episode 1101.000000, reward total was -20.000000. running mean: -20.419130\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.424939\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -20.430689\n",
            "resetting env. episode 1104.000000, reward total was -20.000000. running mean: -20.426382\n",
            "resetting env. episode 1105.000000, reward total was -20.000000. running mean: -20.422119\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.427897\n",
            "resetting env. episode 1107.000000, reward total was -19.000000. running mean: -20.413618\n",
            "resetting env. episode 1108.000000, reward total was -20.000000. running mean: -20.409482\n",
            "resetting env. episode 1109.000000, reward total was -19.000000. running mean: -20.395387\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -20.401434\n",
            "resetting env. episode 1111.000000, reward total was -20.000000. running mean: -20.397419\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -20.403445\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -20.409411\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -20.405317\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.411263\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.417151\n",
            "resetting env. episode 1117.000000, reward total was -20.000000. running mean: -20.412979\n",
            "resetting env. episode 1118.000000, reward total was -19.000000. running mean: -20.398849\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -20.404861\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -20.410812\n",
            "resetting env. episode 1121.000000, reward total was -21.000000. running mean: -20.416704\n",
            "resetting env. episode 1122.000000, reward total was -21.000000. running mean: -20.422537\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.428312\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.434029\n",
            "resetting env. episode 1125.000000, reward total was -21.000000. running mean: -20.439688\n",
            "resetting env. episode 1126.000000, reward total was -20.000000. running mean: -20.435291\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.440939\n",
            "resetting env. episode 1128.000000, reward total was -19.000000. running mean: -20.426529\n",
            "resetting env. episode 1129.000000, reward total was -19.000000. running mean: -20.412264\n",
            "resetting env. episode 1130.000000, reward total was -20.000000. running mean: -20.408141\n",
            "resetting env. episode 1131.000000, reward total was -20.000000. running mean: -20.404060\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -20.410019\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -20.415919\n",
            "resetting env. episode 1134.000000, reward total was -21.000000. running mean: -20.421760\n",
            "resetting env. episode 1135.000000, reward total was -21.000000. running mean: -20.427542\n",
            "resetting env. episode 1136.000000, reward total was -21.000000. running mean: -20.433267\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.438934\n",
            "resetting env. episode 1138.000000, reward total was -21.000000. running mean: -20.444545\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -20.450099\n",
            "resetting env. episode 1140.000000, reward total was -19.000000. running mean: -20.435598\n",
            "resetting env. episode 1141.000000, reward total was -19.000000. running mean: -20.421242\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -20.427030\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -20.432760\n",
            "resetting env. episode 1144.000000, reward total was -19.000000. running mean: -20.418432\n",
            "resetting env. episode 1145.000000, reward total was -20.000000. running mean: -20.414248\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -20.420105\n",
            "resetting env. episode 1147.000000, reward total was -21.000000. running mean: -20.425904\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -20.421645\n",
            "resetting env. episode 1149.000000, reward total was -20.000000. running mean: -20.417429\n",
            "resetting env. episode 1150.000000, reward total was -21.000000. running mean: -20.423254\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -20.429022\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -20.434732\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -20.440384\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.445981\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -20.451521\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.457006\n",
            "resetting env. episode 1157.000000, reward total was -19.000000. running mean: -20.442435\n",
            "resetting env. episode 1158.000000, reward total was -20.000000. running mean: -20.438011\n",
            "resetting env. episode 1159.000000, reward total was -21.000000. running mean: -20.443631\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -20.449195\n",
            "resetting env. episode 1161.000000, reward total was -21.000000. running mean: -20.454703\n",
            "resetting env. episode 1162.000000, reward total was -19.000000. running mean: -20.440156\n",
            "resetting env. episode 1163.000000, reward total was -21.000000. running mean: -20.445754\n",
            "resetting env. episode 1164.000000, reward total was -20.000000. running mean: -20.441297\n",
            "resetting env. episode 1165.000000, reward total was -21.000000. running mean: -20.446884\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -20.452415\n",
            "resetting env. episode 1167.000000, reward total was -19.000000. running mean: -20.437891\n",
            "resetting env. episode 1168.000000, reward total was -21.000000. running mean: -20.443512\n",
            "resetting env. episode 1169.000000, reward total was -21.000000. running mean: -20.449077\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.454586\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -20.460040\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -20.455440\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -20.460885\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -20.466276\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -20.471614\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -20.476897\n",
            "resetting env. episode 1177.000000, reward total was -20.000000. running mean: -20.472128\n",
            "resetting env. episode 1178.000000, reward total was -18.000000. running mean: -20.447407\n",
            "resetting env. episode 1179.000000, reward total was -20.000000. running mean: -20.442933\n",
            "resetting env. episode 1180.000000, reward total was -20.000000. running mean: -20.438504\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -20.444119\n",
            "resetting env. episode 1182.000000, reward total was -18.000000. running mean: -20.419678\n",
            "resetting env. episode 1183.000000, reward total was -19.000000. running mean: -20.405481\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -20.411426\n",
            "resetting env. episode 1185.000000, reward total was -20.000000. running mean: -20.407312\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.413239\n",
            "resetting env. episode 1187.000000, reward total was -19.000000. running mean: -20.399106\n",
            "resetting env. episode 1188.000000, reward total was -20.000000. running mean: -20.395115\n",
            "resetting env. episode 1189.000000, reward total was -20.000000. running mean: -20.391164\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -20.397252\n",
            "resetting env. episode 1191.000000, reward total was -21.000000. running mean: -20.403280\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -20.409247\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.415155\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -20.421003\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.426793\n",
            "resetting env. episode 1196.000000, reward total was -19.000000. running mean: -20.412525\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -20.418400\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -20.424216\n",
            "resetting env. episode 1199.000000, reward total was -20.000000. running mean: -20.419974\n",
            "resetting env. episode 1200.000000, reward total was -17.000000. running mean: -20.385774\n",
            "resetting env. episode 1201.000000, reward total was -21.000000. running mean: -20.391916\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.397997\n",
            "resetting env. episode 1203.000000, reward total was -20.000000. running mean: -20.394017\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -20.400077\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -20.406076\n",
            "resetting env. episode 1206.000000, reward total was -21.000000. running mean: -20.412015\n",
            "resetting env. episode 1207.000000, reward total was -21.000000. running mean: -20.417895\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -20.423716\n",
            "resetting env. episode 1209.000000, reward total was -18.000000. running mean: -20.399479\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.405484\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -20.411429\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -20.417315\n",
            "resetting env. episode 1213.000000, reward total was -19.000000. running mean: -20.403142\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -20.399111\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -20.405119\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -20.401068\n",
            "resetting env. episode 1217.000000, reward total was -21.000000. running mean: -20.407058\n",
            "resetting env. episode 1218.000000, reward total was -20.000000. running mean: -20.402987\n",
            "resetting env. episode 1219.000000, reward total was -19.000000. running mean: -20.388957\n",
            "resetting env. episode 1220.000000, reward total was -20.000000. running mean: -20.385068\n",
            "resetting env. episode 1221.000000, reward total was -19.000000. running mean: -20.371217\n",
            "resetting env. episode 1222.000000, reward total was -21.000000. running mean: -20.377505\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -20.383730\n",
            "resetting env. episode 1224.000000, reward total was -20.000000. running mean: -20.379892\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -20.386093\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -20.392233\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -20.398310\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -20.404327\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -20.410284\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -20.416181\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -20.422019\n",
            "resetting env. episode 1232.000000, reward total was -21.000000. running mean: -20.427799\n",
            "resetting env. episode 1233.000000, reward total was -20.000000. running mean: -20.423521\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -20.429286\n",
            "resetting env. episode 1235.000000, reward total was -21.000000. running mean: -20.434993\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -20.440643\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -20.446237\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -20.451774\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -20.457256\n",
            "resetting env. episode 1240.000000, reward total was -20.000000. running mean: -20.452684\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -20.458157\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.463576\n",
            "resetting env. episode 1243.000000, reward total was -20.000000. running mean: -20.458940\n",
            "resetting env. episode 1244.000000, reward total was -21.000000. running mean: -20.464350\n",
            "resetting env. episode 1245.000000, reward total was -20.000000. running mean: -20.459707\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.465110\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -20.470459\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -20.475754\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.480997\n",
            "resetting env. episode 1250.000000, reward total was -21.000000. running mean: -20.486187\n",
            "resetting env. episode 1251.000000, reward total was -21.000000. running mean: -20.491325\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -20.496411\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -20.501447\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -20.506433\n",
            "resetting env. episode 1255.000000, reward total was -19.000000. running mean: -20.491369\n",
            "resetting env. episode 1256.000000, reward total was -20.000000. running mean: -20.486455\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -20.491590\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -20.496674\n",
            "resetting env. episode 1259.000000, reward total was -20.000000. running mean: -20.491708\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -20.486791\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.491923\n",
            "resetting env. episode 1262.000000, reward total was -20.000000. running mean: -20.487003\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -20.492133\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -20.497212\n",
            "resetting env. episode 1265.000000, reward total was -20.000000. running mean: -20.492240\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.497318\n",
            "resetting env. episode 1267.000000, reward total was -20.000000. running mean: -20.492344\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -20.497421\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.502447\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -20.507422\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -20.502348\n",
            "resetting env. episode 1272.000000, reward total was -21.000000. running mean: -20.507325\n",
            "resetting env. episode 1273.000000, reward total was -20.000000. running mean: -20.502251\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -20.507229\n",
            "resetting env. episode 1275.000000, reward total was -19.000000. running mean: -20.492157\n",
            "resetting env. episode 1276.000000, reward total was -20.000000. running mean: -20.487235\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -20.492363\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -20.497439\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -20.502465\n",
            "resetting env. episode 1280.000000, reward total was -21.000000. running mean: -20.507440\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -20.512366\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.517242\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -20.522069\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -20.526849\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -20.531580\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -20.536264\n",
            "resetting env. episode 1287.000000, reward total was -17.000000. running mean: -20.500902\n",
            "resetting env. episode 1288.000000, reward total was -21.000000. running mean: -20.505893\n",
            "resetting env. episode 1289.000000, reward total was -20.000000. running mean: -20.500834\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -20.505826\n",
            "resetting env. episode 1291.000000, reward total was -19.000000. running mean: -20.490767\n",
            "resetting env. episode 1292.000000, reward total was -20.000000. running mean: -20.485860\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -20.491001\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -20.496091\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -20.501130\n",
            "resetting env. episode 1296.000000, reward total was -19.000000. running mean: -20.486119\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -20.491258\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -20.496345\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -20.501382\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -20.506368\n",
            "resetting env. episode 1301.000000, reward total was -20.000000. running mean: -20.501304\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -20.506291\n",
            "resetting env. episode 1303.000000, reward total was -20.000000. running mean: -20.501228\n",
            "resetting env. episode 1304.000000, reward total was -20.000000. running mean: -20.496216\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.501254\n",
            "resetting env. episode 1306.000000, reward total was -21.000000. running mean: -20.506241\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -20.511179\n",
            "resetting env. episode 1308.000000, reward total was -21.000000. running mean: -20.516067\n",
            "resetting env. episode 1309.000000, reward total was -21.000000. running mean: -20.520906\n",
            "resetting env. episode 1310.000000, reward total was -20.000000. running mean: -20.515697\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.520540\n",
            "resetting env. episode 1312.000000, reward total was -20.000000. running mean: -20.515335\n",
            "resetting env. episode 1313.000000, reward total was -21.000000. running mean: -20.520182\n",
            "resetting env. episode 1314.000000, reward total was -20.000000. running mean: -20.514980\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -20.509830\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -20.514732\n",
            "resetting env. episode 1317.000000, reward total was -21.000000. running mean: -20.519584\n",
            "resetting env. episode 1318.000000, reward total was -20.000000. running mean: -20.514388\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -20.519245\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -20.524052\n",
            "resetting env. episode 1321.000000, reward total was -21.000000. running mean: -20.528812\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -20.523523\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -20.528288\n",
            "resetting env. episode 1324.000000, reward total was -20.000000. running mean: -20.523005\n",
            "resetting env. episode 1325.000000, reward total was -18.000000. running mean: -20.497775\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -20.502798\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.507770\n",
            "resetting env. episode 1328.000000, reward total was -20.000000. running mean: -20.502692\n",
            "resetting env. episode 1329.000000, reward total was -19.000000. running mean: -20.487665\n",
            "resetting env. episode 1330.000000, reward total was -21.000000. running mean: -20.492788\n",
            "resetting env. episode 1331.000000, reward total was -21.000000. running mean: -20.497860\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -20.502882\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -20.507853\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.512774\n",
            "resetting env. episode 1335.000000, reward total was -20.000000. running mean: -20.507647\n",
            "resetting env. episode 1336.000000, reward total was -21.000000. running mean: -20.512570\n",
            "resetting env. episode 1337.000000, reward total was -21.000000. running mean: -20.517445\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -20.522270\n",
            "resetting env. episode 1339.000000, reward total was -21.000000. running mean: -20.527047\n",
            "resetting env. episode 1340.000000, reward total was -20.000000. running mean: -20.521777\n",
            "resetting env. episode 1341.000000, reward total was -19.000000. running mean: -20.506559\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -20.501494\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -20.506479\n",
            "resetting env. episode 1344.000000, reward total was -20.000000. running mean: -20.501414\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.506400\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -20.511336\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -20.516222\n",
            "resetting env. episode 1348.000000, reward total was -20.000000. running mean: -20.511060\n",
            "resetting env. episode 1349.000000, reward total was -21.000000. running mean: -20.515950\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -20.520790\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -20.515582\n",
            "resetting env. episode 1352.000000, reward total was -20.000000. running mean: -20.510426\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.515322\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -20.520169\n",
            "resetting env. episode 1355.000000, reward total was -21.000000. running mean: -20.524967\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -20.529717\n",
            "resetting env. episode 1357.000000, reward total was -19.000000. running mean: -20.514420\n",
            "resetting env. episode 1358.000000, reward total was -20.000000. running mean: -20.509276\n",
            "resetting env. episode 1359.000000, reward total was -20.000000. running mean: -20.504183\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -20.509142\n",
            "resetting env. episode 1361.000000, reward total was -20.000000. running mean: -20.504050\n",
            "resetting env. episode 1362.000000, reward total was -20.000000. running mean: -20.499010\n",
            "resetting env. episode 1363.000000, reward total was -21.000000. running mean: -20.504019\n",
            "resetting env. episode 1364.000000, reward total was -20.000000. running mean: -20.498979\n",
            "resetting env. episode 1365.000000, reward total was -19.000000. running mean: -20.483990\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -20.489150\n",
            "resetting env. episode 1367.000000, reward total was -21.000000. running mean: -20.494258\n",
            "resetting env. episode 1368.000000, reward total was -19.000000. running mean: -20.479316\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -20.484522\n",
            "resetting env. episode 1370.000000, reward total was -21.000000. running mean: -20.489677\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.494780\n",
            "resetting env. episode 1372.000000, reward total was -17.000000. running mean: -20.459833\n",
            "resetting env. episode 1373.000000, reward total was -21.000000. running mean: -20.465234\n",
            "resetting env. episode 1374.000000, reward total was -20.000000. running mean: -20.460582\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -20.465976\n",
            "resetting env. episode 1376.000000, reward total was -20.000000. running mean: -20.461316\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -20.466703\n",
            "resetting env. episode 1378.000000, reward total was -20.000000. running mean: -20.462036\n",
            "resetting env. episode 1379.000000, reward total was -20.000000. running mean: -20.457416\n",
            "resetting env. episode 1380.000000, reward total was -21.000000. running mean: -20.462842\n",
            "resetting env. episode 1381.000000, reward total was -20.000000. running mean: -20.458213\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -20.463631\n",
            "resetting env. episode 1383.000000, reward total was -20.000000. running mean: -20.458995\n",
            "resetting env. episode 1384.000000, reward total was -21.000000. running mean: -20.464405\n",
            "resetting env. episode 1385.000000, reward total was -20.000000. running mean: -20.459761\n",
            "resetting env. episode 1386.000000, reward total was -20.000000. running mean: -20.455163\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -20.460612\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.466005\n",
            "resetting env. episode 1389.000000, reward total was -20.000000. running mean: -20.461345\n",
            "resetting env. episode 1390.000000, reward total was -20.000000. running mean: -20.456732\n",
            "resetting env. episode 1391.000000, reward total was -19.000000. running mean: -20.442165\n",
            "resetting env. episode 1392.000000, reward total was -21.000000. running mean: -20.447743\n",
            "resetting env. episode 1393.000000, reward total was -20.000000. running mean: -20.443266\n",
            "resetting env. episode 1394.000000, reward total was -21.000000. running mean: -20.448833\n",
            "resetting env. episode 1395.000000, reward total was -21.000000. running mean: -20.454345\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.459801\n",
            "resetting env. episode 1397.000000, reward total was -20.000000. running mean: -20.455203\n",
            "resetting env. episode 1398.000000, reward total was -21.000000. running mean: -20.460651\n",
            "resetting env. episode 1399.000000, reward total was -18.000000. running mean: -20.436045\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -20.441684\n",
            "resetting env. episode 1401.000000, reward total was -21.000000. running mean: -20.447267\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.452795\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.458267\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -20.463684\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -20.469047\n",
            "resetting env. episode 1406.000000, reward total was -20.000000. running mean: -20.464357\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -20.469713\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -20.465016\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -20.470366\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.475662\n",
            "resetting env. episode 1411.000000, reward total was -20.000000. running mean: -20.470906\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.476196\n",
            "resetting env. episode 1413.000000, reward total was -18.000000. running mean: -20.451434\n",
            "resetting env. episode 1414.000000, reward total was -20.000000. running mean: -20.446920\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.452451\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.457926\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -20.453347\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.458814\n",
            "resetting env. episode 1419.000000, reward total was -20.000000. running mean: -20.454226\n",
            "resetting env. episode 1420.000000, reward total was -18.000000. running mean: -20.429683\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -20.425386\n",
            "resetting env. episode 1422.000000, reward total was -19.000000. running mean: -20.411133\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -20.417021\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -20.422851\n",
            "resetting env. episode 1425.000000, reward total was -20.000000. running mean: -20.418623\n",
            "resetting env. episode 1426.000000, reward total was -20.000000. running mean: -20.414436\n",
            "resetting env. episode 1427.000000, reward total was -19.000000. running mean: -20.400292\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -20.406289\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -20.402226\n",
            "resetting env. episode 1430.000000, reward total was -20.000000. running mean: -20.398204\n",
            "resetting env. episode 1431.000000, reward total was -21.000000. running mean: -20.404222\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -20.410180\n",
            "resetting env. episode 1433.000000, reward total was -20.000000. running mean: -20.406078\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -20.412017\n",
            "resetting env. episode 1435.000000, reward total was -20.000000. running mean: -20.407897\n",
            "resetting env. episode 1436.000000, reward total was -21.000000. running mean: -20.413818\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -20.419680\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.425483\n",
            "resetting env. episode 1439.000000, reward total was -20.000000. running mean: -20.421228\n",
            "resetting env. episode 1440.000000, reward total was -21.000000. running mean: -20.427016\n",
            "resetting env. episode 1441.000000, reward total was -19.000000. running mean: -20.412746\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -20.418618\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.424432\n",
            "resetting env. episode 1444.000000, reward total was -19.000000. running mean: -20.410188\n",
            "resetting env. episode 1445.000000, reward total was -21.000000. running mean: -20.416086\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.421925\n",
            "resetting env. episode 1447.000000, reward total was -20.000000. running mean: -20.417706\n",
            "resetting env. episode 1448.000000, reward total was -19.000000. running mean: -20.403529\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -20.399493\n",
            "resetting env. episode 1450.000000, reward total was -21.000000. running mean: -20.405498\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -20.411443\n",
            "resetting env. episode 1452.000000, reward total was -20.000000. running mean: -20.407329\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -20.413256\n",
            "resetting env. episode 1454.000000, reward total was -20.000000. running mean: -20.409123\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -20.415032\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -20.420882\n",
            "resetting env. episode 1457.000000, reward total was -19.000000. running mean: -20.406673\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -20.412606\n",
            "resetting env. episode 1459.000000, reward total was -21.000000. running mean: -20.418480\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -20.424295\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -20.430052\n",
            "resetting env. episode 1462.000000, reward total was -20.000000. running mean: -20.425752\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -20.431494\n",
            "resetting env. episode 1464.000000, reward total was -20.000000. running mean: -20.427179\n",
            "resetting env. episode 1465.000000, reward total was -18.000000. running mean: -20.402908\n",
            "resetting env. episode 1466.000000, reward total was -20.000000. running mean: -20.398878\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -20.404890\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -20.410841\n",
            "resetting env. episode 1469.000000, reward total was -18.000000. running mean: -20.386732\n",
            "resetting env. episode 1470.000000, reward total was -21.000000. running mean: -20.392865\n",
            "resetting env. episode 1471.000000, reward total was -21.000000. running mean: -20.398936\n",
            "resetting env. episode 1472.000000, reward total was -20.000000. running mean: -20.394947\n",
            "resetting env. episode 1473.000000, reward total was -21.000000. running mean: -20.400998\n",
            "resetting env. episode 1474.000000, reward total was -20.000000. running mean: -20.396988\n",
            "resetting env. episode 1475.000000, reward total was -19.000000. running mean: -20.383018\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -20.389188\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -20.395296\n",
            "resetting env. episode 1478.000000, reward total was -19.000000. running mean: -20.381343\n",
            "resetting env. episode 1479.000000, reward total was -19.000000. running mean: -20.367529\n",
            "resetting env. episode 1480.000000, reward total was -19.000000. running mean: -20.353854\n",
            "resetting env. episode 1481.000000, reward total was -20.000000. running mean: -20.350315\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -20.356812\n",
            "resetting env. episode 1483.000000, reward total was -19.000000. running mean: -20.343244\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -20.349812\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -20.356314\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -20.362750\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.369123\n",
            "resetting env. episode 1488.000000, reward total was -20.000000. running mean: -20.365432\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -20.361777\n",
            "resetting env. episode 1490.000000, reward total was -21.000000. running mean: -20.368160\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -20.374478\n",
            "resetting env. episode 1492.000000, reward total was -20.000000. running mean: -20.370733\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -20.377026\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.383256\n",
            "resetting env. episode 1495.000000, reward total was -20.000000. running mean: -20.379423\n",
            "resetting env. episode 1496.000000, reward total was -20.000000. running mean: -20.375629\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -20.381873\n",
            "resetting env. episode 1498.000000, reward total was -19.000000. running mean: -20.368054\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -20.374373\n",
            "resetting env. episode 1500.000000, reward total was -19.000000. running mean: -20.360630\n",
            "CPU times: user 1h 12min 41s, sys: 32min 15s, total: 1h 44min 57s\n",
            "Wall time: 54min 32s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "CteN7XKMVGqg",
        "outputId": "275bdb20-c48f-4e3b-852d-b5e057b102bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZYA0HgMoO77a"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# pickle.dump(model, open('model.pkl', 'wb'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "pg_from_scratch_(rate_6).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}